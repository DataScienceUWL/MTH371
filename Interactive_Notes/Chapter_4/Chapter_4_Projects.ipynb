{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de1678e",
   "metadata": {},
   "source": [
    "### 4.9 Projects\n",
    "\n",
    "In this section we propose several ideas for projects related to numerical linear algebra. These projects are meant to be open ended, to encourage creative mathematics, to push your coding skills, and to require you to write and communicate your mathematics. Take the time to read Appendix B before you write your final solution.\n",
    "\n",
    "### 4.9.1 The Google Page Rank Algorithm\n",
    "\n",
    "In this project you will discover how the Page Rank algorithm works to give the most relevant information as the top hit on a Google search.\n",
    "\n",
    "Search engines compile large indexes of the dynamic information on the Internet so they are easily searched. This means that when you do a Google search, you are not actually searching the Internet; instead, you are searching the indexes at Google.\n",
    "\n",
    "When you type a query into Google the following two steps take place:\n",
    "\n",
    "1. Query Module: The query module at Google converts your natural language into a language that the search system can understand and consults the various indexes at Google in order to answer the query. This is done to find the list of relevant pages.\n",
    "2. Ranking Module: The ranking module takes the set of relevant pages and ranks them. The outcome of the ranking is an ordered list of web pages such that the pages near the top of the list are most likely to be what you desire from your search. This ranking is the same as assigning a popularity score to each web site and then listing the relevant sites by this score.\n",
    "\n",
    "This section focuses on the Linear Algebra behind the Ranking Module developed by the founders of Google: Sergey Brin and Larry Page. Their algorithm is called the Page Rank algorithm, and you use it every single time you use Google's search engine.\n",
    "\n",
    "In simple terms: A webpage is important if it is pointed to by other important pages.\n",
    "\n",
    "The Internet can be viewed as a directed graph (look up this term here on Wikipedia) where the nodes are the web pages and the edges are the hyperlinks between the pages. The hyperlinks into a page are called in links, and the ones pointing out of a page are called out links. In essence, a hyperlink from my page to yours is my endorsement of your page. Thus, a page with more recommendations must be more important than a page with a few links. However, the status of the recommendation is also important.\n",
    "\n",
    "Let us now translate this into mathematics. To help understand this we first consider the small web of six pages shown in Figure 4.3 (a graph of the router level of the internet can be found here). The links between the pages are shown\n",
    "by arrows. An arrow pointing into a node is an in link and an arrow pointing out of a node is an out link. In Figure 4.3, node 3 has three out links (to nodes 1,2 , and 5) and 1 in link (from node 1 ).\n",
    "![](https://cdn.mathpix.com/cropped/2025_04_20_985fa04feb24e6557d79g-57.jpg?height=347&width=264&top_left_y=580&top_left_x=833)\n",
    "\n",
    "Figure 4.3: Example web graph.\n",
    "We will first define some notation in the Page Rank algorithm:\n",
    "\n",
    "- $\\left|P_{i}\\right|$ is the number of out links from page $P_{i}$\n",
    "- $H$ is the hyperlink matrix defined as\n",
    "\n",
    "$$\n",
    "H_{i j}=\\left\\{\\begin{array}{cl}\n",
    "\\frac{1}{\\left|P_{j}\\right|}, & \\text { if there is a link from node } j \\text { to node } i \\\\\n",
    "0, & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where the \" $i$ \" and \" $j$ \" are the row and column indices respectively.\n",
    "\n",
    "- $\\boldsymbol{x}$ is a vector that contains all of the Page Ranks for the individual pages.\n",
    "\n",
    "The Page Rank algorithm works as follows:\n",
    "\n",
    "1. Initialize the page ranks to all be equal. This means that our initial assumption is that all pages are of equal rank. In the case of Figure 4.3 we would take $\\boldsymbol{x}_{0}$ to be\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{0}=\\left(\\begin{array}{l}\n",
    "1 / 6 \\\\\n",
    "1 / 6 \\\\\n",
    "1 / 6 \\\\\n",
    "1 / 6 \\\\\n",
    "1 / 6 \\\\\n",
    "1 / 6\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "2. Build the hyperlink matrix.\n",
    "\n",
    "As an example we'll consider node 3 in Figure 4.3. There are three out links from node 3 (to nodes 1,2 , and 5). Hence $H_{13}=1 / 3, H_{23}=1 / 3$, and $H_{53}=1 / 3$ and the partially complete hyperlink matrix is\n",
    "\n",
    "$$\n",
    "H=\\left(\\begin{array}{cccccc}\n",
    "- & - & 1 / 3 & - & - & - \\\\\n",
    "- & - & 1 / 3 & - & - & - \\\\\n",
    "- & - & 0 & - & - & - \\\\\n",
    "- & - & 0 & - & - & - \\\\\n",
    "- & - & 1 / 3 & - & - & - \\\\\n",
    "- & - & 0 & - & - & -\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "3. The difference equation $\\boldsymbol{x}_{n+1}=H \\boldsymbol{x}_{n}$ is used to iteratively refine the estimates of the page ranks. You can view the iterations as a person visiting a page and then following a link at random, then following a random link on the next page, and the next, and the next, etc. Hence we see that the iterations evolve exactly as expected for a difference equation.\n",
    "\n",
    "| Iteration | New Page Rank Estimation |\n",
    "| :--- | :--- |\n",
    "| 0 | $\\boldsymbol{x}_{0}$ |\n",
    "| 1 | $\\boldsymbol{x}_{1}=H \\boldsymbol{x}_{0}$ |\n",
    "| 2 | $\\boldsymbol{x}_{2}=H \\boldsymbol{x}_{1}=H^{2} \\boldsymbol{x}_{0}$ |\n",
    "| 3 | $\\boldsymbol{x}_{3}=H \\boldsymbol{x}_{2}=H^{3} \\boldsymbol{x}_{0}$ |\n",
    "| 4 | $\\boldsymbol{x}_{4}=H \\boldsymbol{x}_{3}=H^{4} \\boldsymbol{x}_{0}$ |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| $k$ | $\\boldsymbol{x}_{k}=H^{k} \\boldsymbol{x}_{0}$ |\n",
    "\n",
    "4. When a steady state is reached we sort the resulting vector $\\boldsymbol{x}_{k}$ to give the page rank. The node (web page) with the highest rank will be the top search result, the second highest rank will be the second search result, and so on.\n",
    "\n",
    "It doesn't take much to see that this process can be very time consuming. Think about your typical web search with hundreds of thousands of hits; that makes a square matrix $H$ that has a size of hundreds of thousands of entries by hundreds of thousands of entries! The matrix multiplications alone would take many minutes (or possibly many hours) for every search! ... but Brin and Page were pretty smart dudes!!\n",
    "\n",
    "We now state a few theorems and definitions that will help us simplify the iterative Page Rank process.\n",
    "\n",
    "Theorem 4.7. If $A$ is an $n \\times n$ matrix with $n$ linearly independent eigenvectors $\\boldsymbol{v}_{1}, \\boldsymbol{v}_{2}, \\boldsymbol{v}_{3}, \\ldots, \\boldsymbol{v}_{n}$ and associated eigenvalues $\\lambda_{1}, \\lambda_{2}, \\lambda_{3}, \\ldots, \\lambda_{n}$ then for any initial vector $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ we can write $A^{k} \\boldsymbol{x}$ as\n",
    "\n",
    "$$\n",
    "A^{k} \\boldsymbol{x}=c_{1} \\lambda_{1}^{k} \\boldsymbol{v}_{1}+c_{2} \\lambda_{2}^{k} \\boldsymbol{v}_{2}+c_{3} \\lambda_{3}^{k} \\boldsymbol{v}_{3}+\\cdots c_{n} \\lambda_{n}^{k} \\boldsymbol{v}_{n}\n",
    "$$\n",
    "\n",
    "where $c_{1}, c_{2}, c_{3}, \\ldots, c_{n}$ are the constants found by expressing $\\boldsymbol{x}$ as a linear combination of the eigenvectors.\n",
    "Note: We can assume that the eigenvalues are ordered such that $\\left|\\lambda_{1}\\right|>\\left|\\lambda_{2}\\right| \\geq$ $\\left|\\lambda_{3}\\right| \\geq \\cdots \\geq\\left|\\lambda_{n}\\right|$.\n",
    "\n",
    "Exercise 4.94. Prove the preceding theorem.\n",
    "\n",
    "A probability vector is a vector with entries on the interval $[0,1]$ that add up to 1 .\n",
    "\n",
    "A stochastic matrix is a square matrix whose columns are probability vectors.\n",
    "\n",
    "Theorem 4.8. If $A$ is a stochastic $n \\times n$ matrix then $A$ will have $n$ linearly independent eigenvectors. Furthermore, the largest eigenvalue of a stochastic matrix will be $\\lambda_{1}=1$ and the smallest eigenvalue will always be nonnegative: $0 \\leq\\left|\\lambda_{n}\\right|<1$.\n",
    "Some of the following tasks will ask you to prove a statement or a theorem. This means to clearly write all of the logical and mathematical reasons why the statement is true. Your proof should be absolutely crystal clear to anyone with a similar mathematical background ...if you are in doubt then have a peer from a different group read your proof to you .\n",
    "\n",
    "Exercise 4.95. Finish writing the hyperlink matrix $H$ from Figure 4.3.\n",
    "\n",
    "Exercise 4.96. Write code to implement the iterative process defined previously. Make a plot that shows how the rank evolves over the iterations.\n",
    "\n",
    "Exercise 4.97. What must be true about a collection of $n$ pages such that an $n \\times n$ hyperlink matrix $H$ is a stochastic matrix.\n",
    "\n",
    "Exercise 4.98. The statement of the next theorem is incomplete, but the proof is given to you. Fill in the blank in the statement of the theorem and provide a few sentences supporting your answer.\n",
    "\n",
    "Theorem 4.9. If $A$ is an $n \\times n$ stochastic matrix and $\\boldsymbol{x}_{0}$ is some initial vector for the difference equation $\\boldsymbol{x}_{n+1}=A \\boldsymbol{x}_{n}$, then the steady state vector is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{e q u i l i b}=\\lim _{k \\rightarrow \\infty} A^{k} \\boldsymbol{x}_{0}=\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "First note that $A$ is an $n \\times n$ stochastic matrix so from Theorem 4.8 we know that there are $n$ linearly independent eigenvectors. We can then substitute the eigenvalues from Theorem 4.8 in Theorem 4.7. Noting that if $0<\\lambda_{j}<1$ we have $\\lim _{k \\rightarrow \\infty} \\lambda_{j}^{k}=0$ the result follows immediately.\n",
    "\n",
    "Exercise 4.99. Discuss how Theorem 4.9 greatly simplifies the PageRank iterative process described previously. In other words: there is no reason to iterate at all. Instead, just find ... what?\n",
    "\n",
    "Exercise 4.100. Now use the previous two problems to find the resulting PageRank vector from the web in Figure 4.3? Be sure to rank the pages in order of importance. Compare your answer to the one that you got in problem 2.\n",
    "![](https://cdn.mathpix.com/cropped/2025_04_20_985fa04feb24e6557d79g-60.jpg?height=250&width=258&top_left_y=870&top_left_x=1023)\n",
    "\n",
    "Figure 4.4: A second example web graph.\n",
    "Exercise 4.101. Consider the web in Figure 4.4.\n",
    "\n",
    "1. Write the $H$ matrix and find the initial state $\\boldsymbol{x}_{0}$,\n",
    "2. Find steady state PageRank vector using the two different methods described: one using the iterative difference equation and the other using Theorem 4.9 and the dominant eigenvector.\n",
    "3. Rank the pages in order of importance.\n",
    "\n",
    "Exercise 4.102. One thing that we didn't consider in this version of the Google Page Rank algorithm is the random behavior of humans. One, admittedly slightly naive, modification that we can make to the present algorithm is to assume that the person surfing the web will randomly jump to any other page in the web at any time. For example, if someone is on page 1 in Figure 4.4 then they could randomly jump to any page $2-8$. They also have links to pages 2,3 , and 7 . That is a total of 10 possible next steps for the web surfer. There is a $2 / 10$ chance of heading to page 2 . One of those is following the link from page 1 to page 2 and the other is a random jump to page 2 without following the link. Similarly, there is a $2 / 10$ chance of heading to page $3,2 / 10$ chance of heading to page 7 , and a $1 / 10$ chance of randomly heading to any other page.\n",
    "\n",
    "Implement this new algorithm, called the random surfer algorithm, on the web in Figure 4.4. Compare your ranking to the non-random surfer results from the previous problem.\n",
    "\n",
    "### 4.9.2 Alternative Methods To Solving $A \\boldsymbol{x}=\\boldsymbol{b}$\n",
    "\n",
    "Throughout most of the linear algebra chapter we have studied ways to solve systems of equations of the form $A \\boldsymbol{x}=\\boldsymbol{b}$ where $A$ is a square $n \\times n$ matrix, $\\boldsymbol{x} \\in \\mathbb{R}^{n}$, and $\\boldsymbol{b} \\in \\mathbb{R}^{n}$. We have reviewed by-hand row reduction and learned new techniques such as the $L U$ decomposition and the $Q R$ decomposition - all of which are great in their own right and all of which have their shortcomings.\n",
    "\n",
    "Both $L U$ and $Q R$ are great solution techniques and they generally work very very well. However (no surprise), we can build algorithms that will usually be faster!\n",
    "\n",
    "In the following new algorithms we want to solve the linear system of equations\n",
    "\n",
    "$$\n",
    "A \\boldsymbol{x}=\\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "but in each we will do so iteratively by applying an algorithm over and over until the algorithm converges to an approximation of the solution vector $\\boldsymbol{x}$. Convergence here means that $\\|A \\boldsymbol{x}-\\boldsymbol{b}\\|$ is less than some pre-determined tolerance.\n",
    "Method 1: Start by \"factoring' ${ }^{\\prime}{ }^{7}$ the matrix $A$ into $A=L+U$ where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix. Take note that this time we will not force the diagonal entries of $L$ to be 1 like we did in the classical $L U$ factorization. The $U$ in the factorization $A=L+U$ is an upper triangular matrix where the entries on the main diagonal are exactly 0 .\n",
    "\n",
    "Specifically,\n",
    "\n",
    "$$\n",
    "A=L+U=\\left(\\begin{array}{ccccc}\n",
    "a_{00} & 0 & 0 & \\cdots & 0 \\\\\n",
    "a_{10} & a_{11} & 0 & \\cdots & 0 \\\\\n",
    "a_{20} & a_{21} & a_{22} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n 0} & a_{n 1} & a_{n 2} & \\cdots & a_{n-1, n-1}\n",
    "\\end{array}\\right)+\\left(\\begin{array}{ccccc}\n",
    "0 & a_{01} & a_{02} & \\cdots & a_{0, n-1} \\\\\n",
    "0 & 0 & a_{12} & \\cdots & a_{1, n-1} \\\\\n",
    "0 & 0 & 0 & a_{23} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "As an example,\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 \\\\\n",
    "8 & 9 & 1\n",
    "\\end{array}\\right)=\\left(\\begin{array}{lll}\n",
    "2 & 0 & 0 \\\\\n",
    "5 & 6 & 0 \\\\\n",
    "8 & 9 & 1\n",
    "\\end{array}\\right)+\\left(\\begin{array}{lll}\n",
    "0 & 3 & 4 \\\\\n",
    "0 & 0 & 7 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "After factoring the system of equations can be rewritten as\n",
    "\n",
    "$$\n",
    "A \\boldsymbol{x}=\\boldsymbol{b} \\Longrightarrow(L+U) x=\\boldsymbol{b} \\Longrightarrow L \\boldsymbol{x}+U \\boldsymbol{x}=\\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "[^6]Moving the term $U \\boldsymbol{x}$ to the right-hand side gives $L \\boldsymbol{x}=b-U \\boldsymbol{x}$, and if we solve for the unknown $\\boldsymbol{x}$ we get $\\boldsymbol{x}=L^{-1}(\\boldsymbol{b}-U \\boldsymbol{x})$.\n",
    "Of course we would never (ever!) actually compute the inverse of $L$, and consequently we have to do something else in place of the matrix inverse. Stop and think here for a moment. We've run into this problem earlier in this chapter and you have some code that you will need to modify for this job (but take very careful note that the $L$ matrix here does not quite have the same structure as the $L$ matrix we used in the past). Moreover, notice that we have the unknown $\\boldsymbol{x}$ on both sides of the equation. Initially this may seem like nonsense, but if we treat this as an iterative scheme by first making a guess about $x$ and then iteratively find better approximations of solutions via the difference equation\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{k+1}=L^{-1}\\left(b-U \\boldsymbol{x}_{k}\\right)\n",
    "$$\n",
    "\n",
    "we may, under moderate conditions on $A$, quickly be able to approximate the solution to $\\boldsymbol{A} \\boldsymbol{x}=\\boldsymbol{b}$. The subscripts in the iterative scheme represet the iteration number. Hence,\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\boldsymbol{x}_{1}=L^{-1}\\left(b-U \\boldsymbol{x}_{0}\\right) \\\\\n",
    "\\boldsymbol{x}_{2}=L^{-1}\\left(b-U \\boldsymbol{x}_{1}\\right) \\\\\n",
    "\\vdots\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "What we need to pay attention to is that the method is not guaranteed to converge to the actual solution to the equation $A \\boldsymbol{x}=\\boldsymbol{b}$ unless some conditions on $A$ are met, and you will need to experiemnt with the algorithm to come up with a conjecture about the appropriate conditions.\n",
    "\n",
    "Method 2: Start by factoring the matrix $A$ into $A=L+D+U$ where $L$ is strictly lower triangular (0's on the main diagonal and in the entire upper triangle), $D$ is a diagonal matrix, and $U$ is a strictly upper triangular matrix ( 0 's on the main diagonal and in the entire lower triangle). In this new factorization, the diagonal matrix $D$ simply contains the entries from the main diagonal of $A$. The $L$ matrix is the lower triangle of $A$, and the $U$ matrix is the upper triangle of $A$.\n",
    "\n",
    "Considering the system of equations $A \\boldsymbol{x}=\\boldsymbol{b}$ we get\n",
    "\n",
    "$$\n",
    "(L+D+U) \\boldsymbol{x}=\\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "and after simplifying, rearranging, and solving for $\\boldsymbol{x}$ we get $\\boldsymbol{x}=D^{-1}(b-L \\boldsymbol{x}-$ $U \\boldsymbol{x})$. A moment's relection should reveal that the inverse of $D$ is really easy to find (no heavy-duty linear algebra necessary) if some mild conditions on the diagonal entries of $A$ are met. Like before there is an $\\boldsymbol{x}$ on both sides of the equation, but if we again make the algorithm iterative we can get successive approximations of the solution with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{k+1}=D^{-1}\\left(b-L \\boldsymbol{x}_{k}-U \\boldsymbol{x}_{k}\\right)\n",
    "$$\n",
    "\n",
    "## Your Tasks:\n",
    "\n",
    "## Your Tasks\n",
    "\n",
    "1. Pick a small (larger than $3 \\times 3$ ) matrix and an appropriate right-hand side $\\boldsymbol{b}$ and work each of the algorithms by hand. You do not need to write this step up in the final product, but this exercise will help you locate where things may go wrong in the algorithms and what conditions we might need on $A$ in order to get convergent sequences of approximate solutions.\n",
    "2. Build Python functions that accept a square matrix $A$ and complete the factorizations $A=L+U$ and $A=L+D+U$.\n",
    "3. Build functions to implement the two methods and then demonstrate that the methods work on a handful of carefully chosen test examples. As part of these functions you need to build a way to deal with the matrix inversions as well as build a stopping rule for the iterative schemes. Hint: You should use a while loop with a proper logical condition. Think carefully about what we're finding at each iteration and what we can use to check our accuracy at each iteration. It would also be wise to write your code in such a way that it checks to see if the sequence of approximations is diverging.\n",
    "4. Discuss where each method might fail and then demonstrate the possible failures with several carefully chosen examples. Stick to small examples and work these out by hand to clearly show the failure.\n",
    "5. Iterative methods such as these will produce a sequence of approximations, but there is no guranatee that either method will actually produce a convergent sequence. Experiment with several examples and propose a condition on the matrix $A$ which will likely result in a convergent sequence. Demonstrate that the methods fail if your condition is violated and that the methods converge if your condition is met. Take care that it is tempting to think that your code is broken if it doesn't converge. The more likely scenario is that the problem that you have chosen to solve will result in a non-convergent sequence of iterations, and you need to think and experiment carefully when choosing the example problems to solve. One such convergence criterion has something to do with the diagonal entries of $A$ relative to the other entries, but that doesn't mean that you shouldn't explore other features of the matrices as well (I-gen can't give you any more hints than that). This task is not asking for a proof; just a conjecture and convincing numerical evidence that the conjecture holds. The actual proofs are beyond the scope of this project and this course.\n",
    "6. Devise a way to demonstrate how the time to solve a large linear system $A \\boldsymbol{x}=\\boldsymbol{b}$ compares between our two new methods, the $L U$ algorithm, and the $Q R$ algorithm that we built earlier in the chapter. Conclude this demonstration with apropriate plots and ample discussion.\n",
    "\n",
    "You need to do this project without the help of your old buddy Google. All code must be originally yours or be modified from code that we built in class. You can ask Google how Python works with matrices and the like, but searching directly for the algorithms (which are actually well-known, well-studied, and named algorithms) is not allowed.\n",
    "\n",
    "Finally, solving systems of equations with the |np.linalg.solve() command can only be done to verify or check your answer(s).\n",
    "\n",
    "\n",
    "[^0]:    ${ }^{1}$ You should also note that $\\|\\boldsymbol{u}\\|=\\sqrt{\\boldsymbol{u} \\cdot \\boldsymbol{u}}$ is not the only definition of distance. More generally, if you let $\\langle\\boldsymbol{u}, \\boldsymbol{v}\\rangle$ be an inner product for $\\boldsymbol{u}$ and $\\boldsymbol{v}$ in some vector space $\\mathcal{V}$ then $\\|\\boldsymbol{u}\\|=\\sqrt{\\langle\\boldsymbol{u}, \\boldsymbol{u}\\rangle}$. In most cases in this text we will be using the dot product as our prefered inner product so we won't have to worry much about this particular natural extension of the definition of the length of a vector.\n",
    "\n",
    "[^1]:    ${ }^{2}$ You might have thought that naive multiplication was a much more natural way to do matrix multiplication when you first saw it. Hopefully now you see the power in the definition of matrix multiplication that we actually use. If not, then I give you this moment to ponder that (a) matrix multiplication is just a bunch of dot products, and (b) dot products can be seen as projections. Hence, matrix multiplication is really just a projection of the rows of $A$ onto the columns of $B$. This has much more rich geometric flavor than naive multiplication.\n",
    "\n",
    "[^2]:    ${ }^{3}$ Take careful note here. We have actually just built a special case of the $L U$ decomposition. Remember that in row reduction you are allowed to swap the order of the rows, but in our $L U$ algorithm we don't have any row swaps. The version of $L U$ with row swaps is called $L U$ with partial pivoting. We won't built the full partial pivoting algorithm in this text but feel free to look it up. The wikipedia page is a decent place to start. What you'll find is that there are indeed many different versions of the $L U$ decomposition.\n",
    "\n",
    "[^3]:    ${ }^{4}$ Numerical Linear Algebra is a huge field and there is way more to say ... but alas, this is an introductory course in numerical methods so we can't do everything. Sigh.\n",
    "\n",
    "[^4]:    ${ }^{5}$ To build a matrix with specific eigenvalues it may be helpful to recall the matrix factorization $A=P D P^{-1}$ where the columns of $P$ are the eigenvectors of $A$ and the diagonal entries of $D$ are the eigenvalues. If you choose $P$ and $D$ then you can build $A$ with your specific eigen-structure. If you are looking for complex eigenvalues then remember that the eigenvectors may well be complex too.\n",
    "\n",
    "[^5]:    ${ }^{6}$ Actually, the determinant computation uses LU with partial pivoting which we did not cover here in the text. What we are looking at in this exercise is a smaller subcase of what happens when you have a matrix $A$ that does not require any row swaps in the row reduction process.\n",
    "\n",
    "[^6]:    ${ }^{7}$ Technically speaking we should not call this a \"factorization\" since we have not split the matrix $A$ into a product of two matrices. Instead we should call it a \"partition\" since in number theory we call the process of breaking an integer into the sum of two integers is called a \"partition.\" Even so, we will still use the word factorization here for simpllicity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
