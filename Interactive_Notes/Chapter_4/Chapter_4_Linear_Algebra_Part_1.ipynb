{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143f94f4",
   "metadata": {},
   "source": [
    "## Chapter 4\n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "### 4.1 Intro to Numerical Linear Algebra\n",
    "\n",
    "You cannot learn too much linear algebra.\n",
    "\n",
    "- Every mathematician\n",
    "\n",
    "The preceding comment says it all - linear algebra is the most important of all of the mathematical tools that you can learn as a practitioner of the mathematical sciences. The theorems, proofs, conjectures, and big ideas in almost every other mathematical field find their roots in linear algebra. Our goal in this chapter is to explore numerical algorithms for the primary questions of linear algebra:\n",
    "\n",
    "- solving systems of equations,\n",
    "- approximating solutions to over-determined systems of equations, and\n",
    "- finding eigenvalue-eigenvector pairs for a matrix.\n",
    "\n",
    "To see an introductory video to this chapter go to https://youtu.be/S190SQBoNg.\n",
    "\n",
    "Take careful note, that in our current digital age numerical linear algebra and its fast algorithms are behind the scenes for wide varieties of computing applications. Applications of numerical linear algebra include:\n",
    "\n",
    "- determining the most important web page in a Google search,\n",
    "- determine the forces on a car during a crash,\n",
    "- modeling realistic 3D environments in video games,\n",
    "- digital image processing,\n",
    "- building neural networks and AI algorithms,\n",
    "- and many many more.\n",
    "\n",
    "What's more, researchers have found provably optimal ways to perform most of the typical tasks of linear algebra so most scientific software works very well and very quickly with linear algebra. For example, we have already seen in Chapter 3 that programming numerical differentiation and numerical integration schemes can be done in Python with the use of vectors instead of loops. We want to use vectors specifically so that we can use the fast implementations of numerical linear algebra in the background in Python.\n",
    "\n",
    "Lastly, a comment on notation. Throughout this chapter we will use the following notation conventions.\n",
    "\n",
    "- A bold mathematical symbol such as $\\boldsymbol{x}$ or $\\boldsymbol{u}$ will represent a vector.\n",
    "- If $\\boldsymbol{u}$ is a vector then $u_{j}$ will be the $j^{\\text {th }}$ entry of the vector.\n",
    "- Vectors will typically be written vertically with parenthesis as delimiters such as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "- Two bold symbols separated by a centered dot such as $\\boldsymbol{u} \\cdot \\boldsymbol{v}$ will represent the dot product of two vectors.\n",
    "- A capital mathematical symbol such as $A$ or $X$ will represent a matrix\n",
    "- If $A$ is a matrix then $A_{i j}$ will be the element in the $i^{t h}$ row and $j^{t h}$ column of the matrix.\n",
    "- A matrix will typically be written with parenthesis as delimiters such as\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & \\pi\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "- The juxtaposition of a capital symbol and a bold symbol such as $A \\boldsymbol{x}$ will represent matrix-vector multiplication.\n",
    "- A lower case or Greek mathematical symbol such as $x, c$, or $\\lambda$ will represent a scalar.\n",
    "- The scalar field of real numbers is given as $\\mathbb{R}$ and the scalar field of complex numbers is given as $\\mathbb{C}$.\n",
    "- The symbol $\\mathbb{R}^{n}$ represents the collection of $n$-dimensional vectors where the elements are drawn from the real numbers.\n",
    "- The symbol $\\mathbb{C}^{n}$ represents the collection of $n$-dimensional vectors where the elements are drawn from the complex numbers.\n",
    "\n",
    "It is an important part of learning to read and write linear algebra to give special attention to the symbolic language so you can communicate your work easily and efficiently.\n",
    "\n",
    "### 4.2 Vectors and Matrices in Python\n",
    "\n",
    "We first need to understand how Python's numpy library builds and stores vectors and matrices. The following exercises will give you some experience building and working with these data structures and will point out some common pitfalls that mathematicians fall into when using Python for linear algebra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a967a12",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Example 4.1. (numpy Arrays) \n",
    "\n",
    "In Python you can build a list using square brackets such as $[1,2,3]$. This is called a \"Python list\" and is NOT a vector in the way that we think about it mathematically. It is simply an ordered collection of objects. To build mathematical vectors in Python we need to use numpy arrays with np.array (). For example, the vector\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "would be built with the following code.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "u = np.array([1,2,3])\n",
    "print(u)\n",
    "```\n",
    "\n",
    "Notice that Python defines the vector $u$ as a matrix without a second dimension.\n",
    "You can see that in the following code.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "u= np.array([1,2,3])\n",
    "print(\"The length of the u vector is \\n\",len(u))\n",
    "print(\"The shape of the u vector is \\n\",u.shape)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35d039",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Example 4.2. (numpy Matrices) \n",
    "\n",
    "In numpy, a matrix is a list of lists. For example, the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "is defined using np.matrix() where each row is an individual list, and the matrix is a collection of these lists.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "A = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(A)\n",
    "```\n",
    "\n",
    "Moreover, we can extract the shape, the number of rows, and the number of columns of $A$ using the A. shape command. To be a bit more clear on this one\n",
    "we'll use the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "A = np.matrix([[1,2,3],[4,5,6]])\n",
    "print(\"The shape of the A matrix is \\n\",A.shape)\n",
    "print(\"Number of rows in A is \\n\",A.shape[0])\n",
    "print(\"Number of columns in A is \\n\",A.shape[1])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96512aac",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Example 4.3. (Row and Column Vectors in Python) \n",
    "\n",
    "You can more specifically build row or column vectors in Python using the np.matrix() command and then only specifying one row or column. For example, if you want the vectors\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{v}=\\left(\\begin{array}{lll}\n",
    "4 & 5 & 6\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "then we would use the following Python code.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "u = np.matrix([[1],[2],[3]])\n",
    "print(\"The column vector u is \\n\",u)\n",
    "v = np.matrix([[1,2,3]])\n",
    "print(\"The row vector v is \\n\",v)\n",
    "```\n",
    "\n",
    "Alternatively, if you want to define a column vector you can define a row vector (since there are far fewer brackets to keep track of) and then transpose the matrix to turn it into a column.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "u = np.matrix([[1,2,3]])\n",
    "u = u.transpose()\n",
    "print(\"The column vector u is \\n\",u)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a39ff",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Example 4.4. (Matrix Indexing) \n",
    "\n",
    "Python indexes all arrays, vectors, lists, and matrices starting from index 0 . Let's get used to this fact.\n",
    "\n",
    "Consider the matrix $A$ defined in the previous problem. Mathematically we know that the entry in row 1 column 1 is a 1 , the entry in row 1 column 2 is a 2 , and so on. However, with Python we need to shift the way that we enumerate the rows and columns of a matrix. Hence we would say that the entry in row 0 column 0 is a 1 , the entry in row 0 column 1 is a 2 , and so on.\n",
    "\n",
    "Mathematically we can view all Python matrices as follows. If $A$ is an $n \\times n$\n",
    "matrix then\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ccccc}\n",
    "A_{0,0} & A_{0,1} & A_{0,2} & \\cdots & A_{0, n-1} \\\\\n",
    "A_{1,0} & A_{1,1} & A_{1,2} & \\cdots & A_{1, n-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & A_{n-1,2} & \\cdots & A_{n-1, n-1}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Similarly, we can view all vectors as follows. If $\\boldsymbol{u}$ is an $n \\times 1$ vector then\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}=\\left(\\begin{array}{c}\n",
    "u_{0} \\\\\n",
    "u_{1} \\\\\n",
    "\\vdots \\\\\n",
    "u_{n-1}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The following code should help to illustrate this indexing convention.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "A = np.matrix([[1,2,3],[4,5,6],[7, 8,9]])\n",
    "print(\"Entry in row O column O is\",A[0,0])\n",
    "print(\"Entry in row 0 column 1 is\",A[0,1])\n",
    "print(\"Entry in the bottom right corner\",A[2,2])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fbaca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.1. \n",
    "\n",
    "Build your own matrix in Python and practice choosing individual entries from the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150af920",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Example 4.5. (Matrix Slicing) \n",
    "\n",
    "The last thing that we need to be familiar with is slicing a matrix. The term \"slicing\" generally refers to pulling out individual rows, columns, entries, or blocks from a list, array, or matrix in Python. Examine the code below to see how to slice parts out of a numpy matrix.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "A = np.matrix([[1,2,3],[4,5,6],[7, 8,9]])\n",
    "print(A)\n",
    "print(\"The first column of A is \\n\",A[:,0])\n",
    "print(\"The second row of A is \\n\",A[1,:])\n",
    "print(\"The top left 2x2 sub matrix of A is \\n\",A[:-1,:-1])\n",
    "print(\"The bottom right 2x2 sub matrix of A is \\n\",A[1:,1:])\n",
    "u = np.array([1,2,3,4,5,6])\n",
    "print(\"The first 3 entries of the vector u are \\n\",u[:3])\n",
    "print(\"The last entry of the vector u is \\n\",u[-1])\n",
    "print(\"The last two entries of the vector u are \\n\",u[-2:])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3629f7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.2. \n",
    "\n",
    "Define the matrix $A$ and the vector $u$ in Python. Then perform all of the tasks below.\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{cccc}\n",
    "1 & 3 & 5 & 7 \\\\\n",
    "2 & 4 & 6 & 8 \\\\\n",
    "-3 & -2 & -1 & 0\n",
    "\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{u}=\\left(\\begin{array}{c}\n",
    "10 \\\\\n",
    "20 \\\\\n",
    "30\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "1. Print the matrix $A$, the vector $\\boldsymbol{u}$, the shape of $A$, and the shape of $\\boldsymbol{u}$.\n",
    "2. Print the first column of $A$.\n",
    "3. Print the first two rows of $A$.\n",
    "4. Print the first two entries of $\\boldsymbol{u}$.\n",
    "5. Print the last two entries of $\\boldsymbol{u}$.\n",
    "6. Print the bottom left $2 \\times 2$ submatrix of $A$.\n",
    "7. Print the middle two elements of the middle row of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b6ef4",
   "metadata": {},
   "source": [
    "### 4.3 Matrix and Vector Operations\n",
    "\n",
    "Now let's start doing some numerical linear algebra. We start our discussion with the basics: the dot product and matrix multiplication. The numerical routines in Python's numpy packages are designed to do these tasks in very efficient ways but it is a good coding exercise to build your own dot product and matrix multiplication routines just to further cement the way that Python deals with these data structures and to remind you of the mathematical algorithms. What you will find in numerical linear algebra is that the indexing and the housekeeping in the codes is the hardest part. So why don't we start \"easy.\"\n",
    "\n",
    "\n",
    "### 4.3.1 The Dot Product\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a9ec2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.3. \n",
    "\n",
    "This problem is meant to jog your memory about dot products, how to compute them, and what you might use them for. If your linear algebra is a bit rusty then read ahead a bit and then come back to this problem.\n",
    "\n",
    "Consider two vectors $\\boldsymbol{u}$ and $\\boldsymbol{v}$ defined as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}=\\binom{1}{2} \\quad \\text { and } \\quad \\boldsymbol{v}=\\binom{3}{4}\n",
    "$$\n",
    "\n",
    "1. Draw a picture showing both $\\boldsymbol{u}$ and $\\boldsymbol{v}$.\n",
    "2. What is $\\boldsymbol{u} \\cdot \\boldsymbol{v}$ ?\n",
    "3. What is $\\|\\boldsymbol{u}\\|$ ?\n",
    "4. What is $\\|\\boldsymbol{v}\\|$ ?\n",
    "5. What is the angle between $\\boldsymbol{u}$ and $\\boldsymbol{v}$ ?\n",
    "6. Give two reasons why we know that $\\boldsymbol{u}$ is not perpendicular to $\\boldsymbol{v}$.\n",
    "7. What is the scalar projection of $\\boldsymbol{u}$ onto $\\boldsymbol{v}$ ? Draw this scalar projections on your picture from part 1.\n",
    "8. What is the scalar projection of $\\boldsymbol{v}$ onto $\\boldsymbol{u}$ ? Draw this scalar projections on your picture from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065ae51",
   "metadata": {},
   "source": [
    "Now let's get the formal definitions of the dot product on the table.\n",
    "Definition 4.1. (\"The Dot Product) The dot product of two vectors $\\boldsymbol{u}, \\boldsymbol{v} \\in \\mathbb{R}^{n}$ is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u} \\cdot \\boldsymbol{v}=\\sum_{j=1}^{n} u_{j} v_{j}\n",
    "$$\n",
    "\n",
    "Without summation notation the dot product of two vectors is,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u} \\cdot \\boldsymbol{v}=u_{1} v_{1}+u_{2} v_{2}+\\cdots+u_{n} v_{n}\n",
    "$$\n",
    "\n",
    "Alternatively, you may also recall that the dot product of two vectors is given geometrically as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u} \\cdot \\boldsymbol{v}=\\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\| \\cos \\theta\n",
    "$$\n",
    "\n",
    "where $\\|\\boldsymbol{u}\\|$ and $\\|\\boldsymbol{v}\\|$ are the magnitudes (or lengths) of $\\boldsymbol{u}$ and $\\boldsymbol{v}$ respectively, and $\\theta$ is the angle between the two vectors. In physical applications the dot product is often used to find the angle between two vectors (e.g. between two forces). Hence, the last form of the dot product is often rewritten as\n",
    "\n",
    "$$\n",
    "\\theta=\\cos ^{-1}\\left(\\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\|}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Definition 4.2. (Magnitude of a Vector) \n",
    "\n",
    "The magnitude of a vector $\\boldsymbol{u} \\in \\mathbb{R}^{n}$ is defined as\n",
    "\n",
    "$$\n",
    "\\|\\boldsymbol{u}\\|=\\sqrt{\\boldsymbol{u} \\cdot \\boldsymbol{u}}\n",
    "$$\n",
    "\n",
    "You should note that in two dimensions this collapses to the Pythagorean Theorem, and in higher dimensions this is just a natural extension of the Pythagorean Theorm. ${ }^{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af472f6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.4. \n",
    "\n",
    "Verify that $\\sqrt{\\boldsymbol{u} \\cdot \\boldsymbol{u}}$ indeed gives the Pythagorean Theorem for $\\boldsymbol{u} \\in \\mathbb{R}^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7618b8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.5. \n",
    "\n",
    "Our task now is to write a Python function that accepts two vectors (defined as numpy arrays) and returns the dot product. Write this code without the use any loops.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "def myDotProduct(u,v):\n",
    "    return # the dot product formula uses a product inside a sum.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9699ec",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.6. \n",
    "\n",
    "Test your `myDotProduct()` function on several dot products to make sure that it works. Example code to find the dot product between\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{v}=\\left(\\begin{array}{l}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "is given below. Test your code on other vectors. Then implement an error catch into your code to catch the case where the two input vectors are not the same size. You will want to use the `len()` command to find the length of the vectors.\n",
    "\n",
    "```\n",
    "u = np.array([1,2,3])\n",
    "v = np.array([4,5,6])\n",
    "myDotProduct(u,v)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39f600",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.7. \n",
    "\n",
    "Try sending Python lists instead of numpy arrays into your myDotProduct function. What happens? Why does it happen? What is the cautionary tale here? Modify your `myDotProduct()` function one more time so that it starts by converting the input vectors into numpy arrays.\n",
    "\n",
    "```\n",
    "u = [1,2,3]\n",
    "v = [4,5,6]\n",
    "myDotProduct(u,v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9715e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.8. \n",
    "\n",
    "The numpy library in Python has a built-in command for doing the dot product: `np.dot()`. Test the `np.dot()` command and be sure that it does the same thing as your `myDotProduct()` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e0faa",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 4.3.2 Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8b4a2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.9. \n",
    "\n",
    "Next we will blow the dust off of your matrix multiplication skills. Verify that the product of $A$ and $B$ is indeed what we show below. Work out all of the details by hand.\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "A=\\left(\\begin{array}{ll}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right) \\quad B=\\left(\\begin{array}{ccc}\n",
    "7 & 8 & 9 \\\\\n",
    "10 & 11 & 12\n",
    "\\end{array}\\right) \\\\\n",
    "A B=\\left(\\begin{array}{ccc}\n",
    "27 & 30 & 33 \\\\\n",
    "61 & 68 & 75 \\\\\n",
    "95 & 106 & 117\n",
    "\\end{array}\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50059e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now that you've practiced the algorithm for matrix multiplication we can formalize the definition and then turn the algorithm into a Python function.\n",
    "\n",
    "### Definition 4.3. (Matrix Multiplication) \n",
    "\n",
    "If $A$ and $B$ are matrices with $A \\in \\mathbb{R}^{n \\times p}$ and $B \\in \\mathbb{R}^{p \\times m}$ then the product $A B$ is defined as\n",
    "\n",
    "$$\n",
    "(A B)_{i j}=\\sum_{k=1}^{p} A_{i k} B_{k j}\n",
    "$$\n",
    "\n",
    "A moment's reflection reveals that each entry in the matrix product is actually a dot product,\n",
    "(Entry in row $i$ column $j$ of $A B)=($ Row $i$ of matrix $A) \\cdot($ Column $j$ of matrix $B)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf90fd6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.10. \n",
    "\n",
    "The definition of matrix multiplication above contains the cryptic phrase a moment's reflection reveals that each entry in the matrix product is actually a dot product. Let's go back to the matrices $A$ and $B$ defined above and re-evaluate the matrix multiplication algorithm to make sure that you see each entry as the end result of a dot product.\n",
    "\n",
    "We want to find the product of matrices $A$ and $B$ using dot products.\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ll}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right) \\quad B=\\left(\\begin{array}{ccc}\n",
    "7 & 8 & 9 \\\\\n",
    "10 & 11 & 12\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "1. Why will the product $A B$ clear be a $3 \\times 3$ matrix?\n",
    "2. When we do matrix multiplication we take the product of a row from the first matrix times a column from the second matrix ... at least that's how many people think of it when they perform the operation by hand.\n",
    "   \n",
    "- The rows of $A$ can be written as the vectors\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\boldsymbol{a}_{0}=\\left(\\begin{array}{ll}\n",
    "1 & 2\n",
    "\\end{array}\\right) \\\\\n",
    "\\boldsymbol{a}_{1}=\\left(\\begin{array}{ll}\n",
    "\\square & \\square\n",
    "\\end{array}\\right) \\\\\n",
    "\\boldsymbol{a}_{2}=\\left(\\begin{array}{ll}\n",
    "\\square & \\square\n",
    "\\end{array}\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "- The columns of $B$ can be written as the vectors\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\boldsymbol{b}_{0}=\\binom{7}{10} \\\\\n",
    "\\boldsymbol{b}_{1}=(\\square) \\\\\n",
    "\\boldsymbol{b}_{2}=(\\square)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "- Now let's write each entry in the product $A B$ as a dot product.\n",
    "\n",
    "$$\n",
    "A B=\\left(\\begin{array}{ccc}\n",
    "a_{0} \\cdot b_{0} & - \\\\\n",
    "-\\square & - & - \\\\\n",
    "- & - & - \\\\\n",
    "- & -\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "- Verify that you get\n",
    "\n",
    "$$\n",
    "A B=\\left(\\begin{array}{ccc}\n",
    "27 & 30 & 33 \\\\\n",
    "61 & 68 & 75 \\\\\n",
    "95 & 106 & 117\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "when you perform all of the dot products from the previous part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13215c4a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.11. \n",
    "\n",
    "The observation that matrix multiplication is just a bunch of dot products is what makes the code for doing matrix multiplication very fast and very streamlined. We want to write a Python function that accepts two numpy matrices and returns the product of the two matrices. Inside the code we will leverage the `np.dot()` command to do the appropriate dot products.\n",
    "\n",
    "Partial code is given below. Fill in all of the details and give ample comments showing what each line does.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "def myMatrixMult(A,B):\n",
    "    # Get the shapes of the matrices A and B.\n",
    "    # Then write an if statement that catches size mismatches\n",
    "    # in the matrices. Next build a zeros matrix that is the\n",
    "    # correct size for the product of }A\\mathrm{ and }B\\mathrm{ .\n",
    "    AB = ???\n",
    "    # AB is a zeros matix that will be filled with the values\n",
    "    # from the product\n",
    "    #\n",
    "    # Next we do a double for-loop that loops through all of\n",
    "    # the indices of the product\n",
    "    for i in range(n): # loop over the rows of AB\n",
    "        for j in range(m): # loop over the columns of AB\n",
    "            # use the np.dot() command to take the dot product\n",
    "            AB[i,j] = ???\n",
    "    return AB\n",
    "```\n",
    "\n",
    "Use the following test code to determine if you actually get the correct matrix product out of your code.\n",
    "\n",
    "```\n",
    "A = np.matrix([[1,2],[3,4],[5,6]])\n",
    "B = np.matrix([[7, 8,9],[10,11, 12]])\n",
    "AB = myMatrixMult(A,B)\n",
    "print(AB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb56362",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.12. \n",
    "\n",
    "Try your `myMatrixMult()` function on several other matrix multiplication problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a036b6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.13. \n",
    "\n",
    "Build in an error catch so that your `myMatrixMult()` function catches when the input matrices do not have compatible sizes for multiplication. Write your code so that it returns an appropriate error message in this special case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40fdd5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now that you've been through the exercise of building a matrix multiplication function we will admit that using it inside larger coding problems would be a bit cumbersome (and perhaps annoying). It would be nice to just type $*$ and have Python just know that you mean to do matrix multiplication. The trouble is that there are many different versions of multiplication and any programming language needs to be told explicitly which type they're dealing with. This is where numpy and `np.matrix()` come in quite handy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c78c73",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.14. (Matrix Multiplication with Python) \n",
    "\n",
    "Python will handle matrix multiplication easily so long as the matrices are defined as numpy matrices with `np.matrix()`. For example, with the matrices $A$ and $B$ from above if you can just type `A*B` in Python and you will get the correct result. Pretty nice!! Let's take another moment to notice, though, that regular Python arrays do not behave in the same way. What happens if you run the following Python code?\n",
    "\n",
    "```\n",
    "A = [[1,2],[3,4],[5,6]] # a Python list of lists\n",
    "B = [[7,8,9],[10,11,12]] # a Python list of lists\n",
    "A*B\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854be44",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Example 4.6. (Element-by-Element Multiplication) \n",
    "\n",
    "Sometimes it is convenient to do naive multiplication of matrices when you code. That is, if you have two matrices that are the same size, \"naive multiplication\" would just line up the matrices on top of each other and multiply the corresponding entries. ${ }^{2}$ In Python the tool to do this is np.multiply(). The code below demonstrates this tool with the matrices\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ll}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right) \\quad \\text { and } \\quad B=\\left(\\begin{array}{cc}\n",
    "7 & 8 \\\\\n",
    "9 & 10 \\\\\n",
    "11 & 12\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "(Note that the product $A B$ does not make sense under the mathematical definition of matrix multiplication, but it does make sense in terms of element-by-element (\"naive\") multiplication.)\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "A=[[1,2],[3,4],[5,6]]\n",
    "B=[[7,8],[9,10],[11,12]]\n",
    "np.multiply (A,B)\n",
    "```\n",
    "\n",
    "The key takeaways for doing matrix multiplication in Python are as follows:\n",
    "\n",
    "- If you are doing linear algebra in Python then you should define vectors with `np.array()` and matrices with `np.matrix()`.\n",
    "- If your matrices are defined with `np.matrix()` then $*$ does regular matrix multiplication and `np.multiply()` does element-by-element multiplication.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2bf0b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 4.4 The LU Factorization\n",
    "\n",
    "One of the many classic problems of linear algebra is to solve the linear system $A \\boldsymbol{x}=\\boldsymbol{b}$ where $A$ is a matrix of coefficients and $\\boldsymbol{b}$ is a vector of right-hand sides. You likely recall your go-to technique for solving systems was row reduction (or Gaussian Elimination or RREF). Furthermore, you likely recall from your linear algebra class that you rarely actually did row reduction by hand, and instead you relied on a computer to do most of the computations for you. Just what was the computer doing, exactly? Do you think that it was actually following the same algorithm that you did by hand?\n",
    "\n",
    "\n",
    "### 4.4.1 A Recap of Row Reduction\n",
    "\n",
    "Let's blow the dust off your row reduction skills before we look at something better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316afe2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.15. \n",
    "\n",
    "Solve the following system of equations by hand.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{0}+2 x_{1}+3 x_{2} & =1 \\\\\n",
    "4 x_{0}+5 x_{1}+6 x_{2} & =0 \\\\\n",
    "7 x_{0}+8 x_{1} & =2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that the system of equations can also be written in the matrix form\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 0\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "If you need a nudge to get started then jump ahead to the next problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc784",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.16. \n",
    "\n",
    "We want to solve the system of equations\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 0\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "#### Row Reduction Process:\n",
    "\n",
    "**Note:** Throughout this discussion we use Python-type indexing so the rows and columns are enumerated starting at 0 . That is to say, we will talk about row 0 , row 1 , and row 2 of a matrix instead of rows 1, 2, and 3.\n",
    "\n",
    "1. Augment the coefficient matrix and the vector on the right-hand side to get\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll|l}\n",
    "1 & 2 & 3 & 1 \\\\\n",
    "4 & 5 & 6 & 0 \\\\\n",
    "7 & 8 & 0 & 2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "2. The goal of row reduction is to perform elementary row operations until our augmented matrix gets to (or at least gets as close as possible to)\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll|l}\n",
    "1 & 0 & 0 & \\star \\\\\n",
    "0 & 1 & 0 & \\star \\\\\n",
    "0 & 0 & 1 & \\star\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The allowed elementary row operations are:\n",
    "\n",
    "* We are allowed to scale any row.\n",
    "\n",
    "* We can add two rows.\n",
    "\n",
    "* We can interchange two rows.\n",
    "\n",
    "3. We are going to start with column 0 . We already have the \"1\" in the top left corner so we can use it to eliminate all of the other values in the first column of the matrix.\n",
    "\n",
    "* For example, if we multiply the $0^{t h}$ row by -4 and add it to the first row we get\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc|c}\n",
    "1 & 2 & 3 & 1 \\\\\n",
    "0 & -3 & -6 & -4 \\\\\n",
    "7 & 8 & 0 & 2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "* Multiply row 0 by a scalar and add it to row 2 . Your end result should be\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc|c}\n",
    "1 & 2 & 3 & 1 \\\\\n",
    "0 & -3 & -6 & -4 \\\\\n",
    "0 & -6 & -21 & -5\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "What did you multiply by? Why?\n",
    "\n",
    "4. Now we should deal with column 1.\n",
    "\n",
    "* We want to get a 1 in row 1 column 1. We can do this by scaling row 1. What did you scale by? Why? Your end result should be\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc|c}\n",
    "1 & 2 & 3 & 1 \\\\\n",
    "0 & 1 & 2 & \\frac{4}{3} \\\\\n",
    "0 & -6 & -21 & -5\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "* Now scale row 1 by something and add it to row 0 so that the entry in row 0 column 1 becomes a 0 .\n",
    "\n",
    "* Next scale row 1 by something and add it to row 2 so that the entry in row 2 column 1 becomes a 0 .\n",
    "\n",
    "* At this point you should have the augmented system\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc|c}\n",
    "1 & 0 & -1 & -\\frac{5}{3} \\\\\n",
    "0 & 1 & 2 & \\frac{4}{3} \\\\\n",
    "0 & 0 & -9 & 3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "5. Finally we need to work with column 2.\n",
    "\n",
    "* Make the value in row 2 column 2 a 1 by scaling row 2 . What did you scale by? Why?\n",
    "\n",
    "* Scale row 2 by something and add it to row 1 so that the entry in row 1 column 2 becomes a 0 . What did you scale by? Why?\n",
    "\n",
    "* Scale row 2 by something and add it to row 0 so that the entry in row 0 column 2 becomes a 0 . What did you scale by? Why?\n",
    "\n",
    "* By the time you've made it this far you should have the system\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc|c}\n",
    "1 & 0 & 0 & -2 \\\\\n",
    "0 & 1 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & -\\frac{1}{3}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and you should be able to read off the solution to the system.\n",
    "\n",
    "6. You should verify your answer in two different ways:\n",
    "\n",
    "* If you substitute your values into the original system then all of the equal signs should be true. Verify this.\n",
    "\n",
    "* If you substitute your values into the matrix equation and perform the matrix-vector multiplication on the left-hand side of the equation you should get the right-hand side of the equation. Verify this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8a5e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Exercise 4.17. \n",
    "\n",
    "Summarize the process for doing Gaussian Elimination to solve a square system of linear equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b597505",
   "metadata": {},
   "source": [
    "### 4.4.2 The LU Decomposition\n",
    "\n",
    "You may have used the `rref()` command either on a calculator in other software to perform row reduction in the past. You will be surprised to learn that there is no `rref()` command in Python's numpy library! That's because there are far more efficient and stable ways to solve a linear system on a computer. There is an `rref` command in Python's sympy (symbolic Python) library, but given that it works with symbolic algebra it is quite slow.\n",
    "\n",
    "In solving systems of equations we are interested in equations of the form $A \\boldsymbol{x}=\\boldsymbol{b}$. Notice that the $\\boldsymbol{b}$ vector is just along for the ride, so to speak, in the row reduction process since none of the values in $\\boldsymbol{b}$ actually cause you to make different decisions in the row reduction algorithm. Hence, we only really need to focus on the matrix $A$. Furthermore, let's change our awfully restrictive view of always seeking a matrix of the form\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cccc|c}\n",
    "1 & 0 & \\cdots & 0 & \\star \\\\\n",
    "0 & 1 & \\cdots & 0 & \\star \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1 & \\star\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and instead say:\n",
    "\n",
    "What if we just row reduce until the system is simple enough to solve by hand?\n",
    "That's what the next several exercises are going to lead you to. Our goal here is to develop an algorithm that is fast to implement on a computer and simultaneously performs the same basic operations as row reduction for solving systems of linear equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feed58",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.18. \n",
    "\n",
    "Let $A$ be defined as\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "1. The first step in row reducing $A$ would be to multiply row 0 by -4 and add it to row 1. Do this operation by hand so that you know what the result is supposed to be. Check out the following amazing observation. Define the matrix $L_{1}$ as follows:\n",
    "\n",
    "$$\n",
    "L_{1}=\\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0 \\\\\n",
    "-4 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Now multiply $L_{1}$ and $A$.\n",
    "\n",
    "$$\n",
    "L_{1} A=\\left(\\begin{array}{lll}\n",
    "- & - & - \\\\\n",
    "- & - & - \\\\\n",
    "- & - & -\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "What just happened?!\n",
    "\n",
    "2. Let's do it again. The next step in the row reduction of your result from part (b) would be to multiply row 0 by -7 and add to row 2. Again, do this by hand so you know what the result should be. Then define the matrix $L_{2}$ as\n",
    "\n",
    "$$\n",
    "L_{2}=\\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "-7 & 0 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and find the product $L_{2}\\left(L_{1} A\\right)$.\n",
    "\n",
    "$$\n",
    "L_{2}\\left(L_{1} A\\right)=\\left(\\begin{array}{lll}\n",
    "- & - & - \\\\\n",
    "- & - & - \\\\\n",
    "- & - & -\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Pure insanity!!\n",
    "\n",
    "3. Now let's say that you want to make the entry in row 2 column 1 into a 0 by scaling row 1 by something and then adding to row 2 . Determine what\n",
    "the scalar would be and then determine which matrix, call it $L_{3}$, would do the trick so that $L_{3}\\left(L_{2} L_{1} A\\right)$ would be the next row reduced step.\n",
    "\n",
    "$$\n",
    "L_{3} =\\left(\\begin{array}{lll}\n",
    "1 & - & - \\\\\n",
    "- & 1 & - \\\\\n",
    "- & - & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{3}\\left(L_{2} L_{1} A\\right)  =\\left(\\begin{array}{lll}\n",
    "- & - & - \\\\\n",
    "- & - & - \\\\\n",
    "- & - & -\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b73fff",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.19. \n",
    "\n",
    "Apply the same idea from the previous problem to do the first three steps of row reduction to the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ccc}\n",
    "2 & 6 & 9 \\\\\n",
    "-6 & 8 & 1 \\\\\n",
    "2 & 2 & 10\n",
    "\\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eedb54",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.20. \n",
    "\n",
    "Now let's make a few observations about the two previous problems.\n",
    "\n",
    "1. What will multiplying $A$ by a matrix of the form\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "c & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "do?\n",
    "\n",
    "2. What will multiplying $A$ by a matrix of the form\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "c & 0 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "do?\n",
    "\n",
    "3. What will multiplying $A$ by a matrix of the form\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & c & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "do?\n",
    "\n",
    "4. More generally: If you wanted to multiply row $j$ of an $n \\times n$ matrix by $c$ and add it to row $k$, that is the same as multiplying by what matrix?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d588a6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.21. \n",
    "\n",
    "After doing all of the matrix products, $L_{3} L_{2} L_{1} A$, the resulting matrix will have zeros in the entire lower triangle. That is, all of the nonzero entries of the resulting matrix will be on the main diagonal or above. We call this matrix $U$, for upper triangular. Hence, we have formed a matrix\n",
    "\n",
    "$$\n",
    "L_{3} L_{2} L_{1} A=U\n",
    "$$\n",
    "\n",
    "and if we want to solve for $A$ we would get\n",
    "\n",
    "$$\n",
    "A=(\\square)^{-1}(\\square)^{-1}(\\square)^{-1} U\n",
    "$$\n",
    "\n",
    "(Take care that everything is in the right order in your answer.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5068c19",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.22. \n",
    "\n",
    "It would be nice, now, if the inverses of the $L$ matrices were easy to find. Use np.linalg.inv() to directly compute the inverse of $L_{1}, L_{2}$, and $L_{3}$ for each of the example matrices. Then complete the statement: If $L_{k}$ is an identity matrix with some nonzero $c$ in row $i$ and column $j$ then $L_{k}^{-1}$ is what matrix?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085030f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.23. \n",
    "\n",
    "We started this discussion with $A$ as\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and we defined\n",
    "\n",
    "$$\n",
    "L_{1}=\\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0 \\\\\n",
    "-4 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right), \\quad L_{2}=\\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "-7 & 0 & 1\n",
    "\\end{array}\\right), \\quad \\text { and } \\quad L_{3}=\\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & -2 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Based on your answer to the previous exercises we know that\n",
    "\n",
    "$$\n",
    "A=L_{1}^{-1} L_{2}^{-1} L_{3}^{-1} U\n",
    "$$\n",
    "\n",
    "Explicitly write down the matrices $L_{1}^{-1}, L_{2}^{-1}$, and $L_{3}^{-1}$.\n",
    "Now explicitly find the product $L_{1}^{-1} L_{2}^{-1} L_{3}^{-1}$ and call this product $L$. Verify that $L$ itself is also a lower triangular matrix with ones on the main diagonal. Moreover, take note of exactly the form of the matrix. The answer should be super surprising to you!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b72f9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Throughout all of the preceding exercises, our final result is that we have factored the matrix $A$ into the product of a lower triangular matrix and an upper triangular matrix. Stop and think about that for a minute ... we just factored a matrix!\n",
    "\n",
    "Let's return now to our discussion of solving the system of equations $A \\boldsymbol{x}=\\boldsymbol{b}$. If $A$ can be factored into $A=L U$ then the system of equations can be rewritten as $L U \\boldsymbol{x}=\\boldsymbol{b}$. As we will see in the next subsection, solving systems of equations with triangular matrices is super fast and relatively simple! Hence, we have partially achieved our modified goal of reducing the row reduction into some simpler case. ${ }^{3}$\n",
    "\n",
    "It remains to implement the $L U$ decomposition (also called the $L U$ factorization) in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### Definition 4.4. (The LU Factorization) \n",
    "\n",
    "The following Python function takes a square matrix $A$ and outputs the matrices $L$ and $U$ such that $A=L U$. The entire code is given to you. It will be up to you in the next exercise to pick apart every step of the function.\n",
    "\n",
    "```\n",
    "def myLU(A):\n",
    "    n = A.shape[0] # get the dimension of the matrix A\n",
    "    L = np.matrix( np.identity(n) ) # Build the identity part L\n",
    "    U = np.copy(A) # start the U matrix as a copy of A\n",
    "    for j in range(0,n-1):\n",
    "        for i in range(j+1,n):\n",
    "            mult = A[i,j] / A[j,j]\n",
    "            U[i, j+1:n] = U[i, j+1:n] - mult * U[j,j+1:n]\n",
    "            L[i,j] = mult\n",
    "            U[i,j] = 0 # why are we doing this?\n",
    "    return L,U\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165d963",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.24. \n",
    "\n",
    "Go to Definition 4.4 and go through every iteration of every loop by hand starting with the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Give details of what happens at every step of the algorithm. I'll get you started.\n",
    "\n",
    "- $\\mathrm{n}=3$, `L` starts as an identity matrix of the correct size, and `U` starts as a copy of A.\n",
    "- Start the outer loop: $\\mathrm{j}=0$ : $(\\mathrm{j}$ is the counter for the column)\n",
    "    - Start the inner loop: $i=1$ : ( $i$ is the counter for the row)\n",
    "        - `mult = A[1,0] / \\A[0,0]` so `mult =4/ 1`.\n",
    "        - `A[1, 1:3] = A[1, 1:3]- 4 * A[0,1:3].` Translated, this states that columns 1 and 2 of matrix $A$ took their original value minus 4 times the corresponding values in row 0.\n",
    "        - `U[1, 1:3] = A[1, 1:3].` Now we replace the locations in $U$ with the updated information from our first step of row reduction.\n",
    "        - `L[1,0]=4.` We now fill the $L$ matrix with the proper value.\n",
    "        - `U[1,0]=0.` Finally, we zero out the lower triangle piece of the $U$ matrix which we've now taken care of.\n",
    "    - i=2:\n",
    "        - ... keep going from here ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5a88f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.25. \n",
    "\n",
    "Apply your new `myLU` code to other square matrices and verify that indeed $A$ is the product of the resulting $L$ and $U$ matrices. You can produce a random matrix with `np.random.randn(m,n)` where `n` is the number of rows and columns of the matrix. For example, `np.random.randn(10,10)` will produce a random $10 \\times 10$ matrix with entries chosen from the normal distribution with center 0 and standard deviation 1. Random matrices are just as good as any other when testing your algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15512a0a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 4.4.3 Solving Triangular Systems\n",
    "\n",
    "We now know that row reduction is just a collection of sneaky matrix multiplications. In the previous exercises we saw that we can often turn our system of equations $A \\boldsymbol{x}=\\boldsymbol{b}$ into the system $L U \\boldsymbol{x}=\\boldsymbol{b}$ where $L$ us lower triangular (with ones on the main diagonal) and $U$ is upper triangular. But why was this important?\n",
    "\n",
    "Well, if $L U \\boldsymbol{x}=\\boldsymbol{b}$ then we can rewrite our system of equations as two systems:\n",
    "\n",
    "$$\n",
    "\\text{An upper triangular system: } U \\boldsymbol{x}=\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text { A lower triangular system: } L \\boldsymbol{y}=\\boldsymbol{b} \\text {. }\n",
    "$$\n",
    "\n",
    "In the following exercises we will devise algorithms for solving triangular systems. After we know how to work with triangular systems we'll put all of the pieces together and show how to leverage the $L U$ decomposition and the solution techniques for triangular systems to quickly and efficiently solve linear systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96093ba9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.26. \n",
    "\n",
    "Outline a fast algorithm (without formal row reduction) for\n",
    "solving the lower triangular system\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "4 & 1 & 0 \\\\\n",
    "7 & 2 & 1\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "y_{0} \\\\\n",
    "y_{1} \\\\\n",
    "y_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right) .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091e63e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.27. \n",
    "\n",
    "As a convention we will always write our lower triangular matrices with ones on the main diagonal. Generalize your steps from the previous exercise so that you have an algorithm for solving any lower triangular system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc432538",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    " The most natural algorithm that most people devise here is called **forward substitution**.\n",
    "\n",
    "### Definition 4.5. (The Forward Substutition Algorithm (`lsolve`)) \n",
    "\n",
    "The general statement of the Forward Substitution Algorithm is:\n",
    "\n",
    "*Solve $L \\boldsymbol{y}=\\boldsymbol{b}$ for $\\boldsymbol{y}$, where the matrix $L$ is assumed to be lower triangular with ones on the main diagonal.*\n",
    "\n",
    "The code below gives a full implementation of the Forward Substitution algorithm (also called the lsolve algorithm).\n",
    "\n",
    "```\n",
    "def lsolve(L, b):\n",
    "    L = np.matrix(L) # make sure L is the correct data type\n",
    "    n = b.size # what does this do?\n",
    "    y = np.matrix( np.zeros( (n,1)) ) # what does this do?\n",
    "    for i in range(n):\n",
    "        # start the loop by assigning y to the value on the right\n",
    "        y[i] = b[i]\n",
    "        for j in range(i): # now adjust y\n",
    "            y[i] = y[i] - L[i,j] * y[j]\n",
    "    return(y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf12f36",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.28. \n",
    "\n",
    "Work with your partner(s) to apply the `lsolve()` code to the lower triangular system\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "4 & 1 & 0 \\\\\n",
    "7 & 2 & 1\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "y_{0} \\\\\n",
    "y_{1} \\\\\n",
    "y_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "by hand. It is incredibly important to impelement numerical linear algebra routines by hand a few times so that you truly understand how everything is being tracked and calculated.\n",
    "\n",
    "I'll get you started.\n",
    "\n",
    "- Start: `i=0`:\n",
    "    - `y[0]=1` since `b[0]=1`.\n",
    "    - The next `for` loop does not start since `range(0)` has no elements (stop and think about why this is).\n",
    "- Next step in the loop: `i=1`:\n",
    "    - `y[1]` is initialized as 0 since `b[0]=11.\n",
    "    - Now we enter the inner loop at `j=0`:\n",
    "        - What does `y[1]` become when `j=0`?\n",
    "    - Does `j` increment to anything larger?\n",
    "- Finally we increment `i` to `i=2`:\n",
    "    - What does `y[2]` get initialized to?\n",
    "    - Enter the inner loop at `j=0`:\n",
    "        - What does `y[2]` become when `j=0`?\n",
    "    - Increment the inner loop to  `j=1`:\n",
    "        - What does `y[2]` become when `j=1`?\n",
    "- Stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4536a2b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.29. \n",
    "\n",
    "Copy the code from Definition 4.5 into a Python function but in your code write a comment on every line stating what it is doing. Write a test script that creates a lower triangular matrix of the correct form and a right-hand side $\\boldsymbol{b}$ and solve for $\\mathbf{y}$. Test your code by giving it a large lower triangular system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf2beb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now that we have a method for solving lower triangular systems, let's build a similar method for solving upper triangular systems. The merging of lower and upper triangular systems will play an important role in solving systems of equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013607",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.30. \n",
    "\n",
    "Outline a fast algorithm (without formal row reduction) for solving the upper triangular system\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "1 & 2 & 3 \\\\\n",
    "0 & -3 & -6 \\\\\n",
    "0 & 0 & -9\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "1 \\\\\n",
    "-4 \\\\\n",
    "3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The most natural algorithm that most people devise here is called **backward substitution**. Notice that in our upper triangular matrix we do not have a diagonal containing all ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304e63d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.31. \n",
    "\n",
    "Generalize your backward substitution algorithm from the previous problem so that it could be applied to any upper triangular system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96269f86",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Definition 4.6. (Backward Substitution Algorithm) \n",
    "\n",
    "The following code solves the problem $U \\boldsymbol{x}=\\boldsymbol{y}$ using backward substitution. The matrix $U$ is assumed to be upper triangular. You'll notice that most of this code is incomplete. It is your job to complete this code, and the next exercise should help.\n",
    "\n",
    "```\n",
    "def usolve(U, y):\n",
    "    U = np.matrix(U)\n",
    "    n = y.size\n",
    "    x = np.matrix( np.zeros( (n,1)))\n",
    "    for i in range( ??? ): # what should we be looping over?\n",
    "        x[i] = y[i] / ??? # what should we be dividing by?\n",
    "        for j in range( ??? ): # what should we be looping over:\n",
    "            x[i] = x[i] - U[i,j] * x[j] / ??? # complete this line\n",
    "            # ... what does the previous line do?\n",
    "    return(x)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2e834",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.32. \n",
    "\n",
    "Now we will work through the backward substitution algorithm to help fill in the blanks in the code. Consider the upper triangular system\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "1 & 2 & 3 \\\\\n",
    "0 & -3 & -6 \\\\\n",
    "0 & 0 & -9\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "1 \\\\\n",
    "-4 \\\\\n",
    "3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Work the code from Definition 4.6 to solve the system. Keep track of all of the indices as you work through the code. You may want to work this problem in conjunction with the previous two problems to unpack all of the parts of the backward substitution algorithm.\n",
    "\n",
    "I'll get you started.\n",
    "\n",
    "- In your backward substitution algorithm you should have started with the last row, therefore the outer loop starts at `n-1` and reads backward to `0`. (Why are we starting at `n-1` and not `n`?)\n",
    "- Outer loop: `i=2`:\n",
    "    - We want to solve the equation $-9 x_{2}=3$ so the clear solution is to divide by -9 . In code this means that `x[2]=y[2]/U[2,2]`.\n",
    "    - There is nothing else to do for row 3 of the matrix, so we should not enter the inner loop. How can we keep from entering the inner loop?\n",
    "- Outer loop: `i=1`:\n",
    "    - Now we are solving the algebraic equation $-3 x_{1}-6 x_{2}=-4$. If we follow the high school algebra we see that $x_{1}=\\frac{-4-(-6) x_{2}}{-3}$ but this can be rearranged to $x_{1}=\\frac{-4}{-3}-\\frac{-6 x_{2}}{-3}$. So we can initialize $x_{1}$ with $x_{1}=\\frac{-4}{-3}$. In code, this means that we initialize with $\\mathrm{x}[1]=\\mathrm{y}[1] / \\mathrm{U}[1,1]$.\n",
    "    - Now we need to enter the inner loop at `j=2` : (why are we entering the loop at `j=2`?)\n",
    "        -To complete the algebra we need to take our initialized value of `x[1]` and subtract off $\\frac{-6 x_{2}}{-3}$. In code this is `x[1] = x[1] - U[1,2] * x[2] / U[1,1]`\n",
    "    - There is nothing else to do so the inner loop should end.\n",
    "- Outer loop: $i=0$ :\n",
    "    - Finally, we are solving the algebraic equation $x_{0}+2 x_{1}+3 x_{2}=1$ for $x_{0}$. The clear and obvious solution is $x_{0}=\\frac{1-2 x_{1}-3 x_{2}}{1}$ (why am I explicitly showing the division by 1 here?).\n",
    "    - Initialize $x_{0}$ at `x[0] =  ???`\n",
    "    - Enter the inner loop at $j=2$ :\n",
    "        * Adjust the value of `x[0]` by subtracting off $\\frac{3 x_{2}}{1}$. In code we have `x[0] = x[0] - ??? * ??? / ???`\n",
    "    - Increment `j` to  `j=1`:\n",
    "        - Adjust the value of `x[0]` by subtracting off $\\frac{2 x_{1}}{1}$. In code we have `x[0] = x[0] - ??? * ??? / ???`    \n",
    "- Stop.\n",
    "- You should now have a solution to the equation $U \\boldsymbol{x}=\\boldsymbol{y}$. Substitute your solution in and verify that your solution is correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5073a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.33. \n",
    "\n",
    "Copy the code from Definition 4.6 into a Python function but in your code write a comment on every line stating what it is doing. Write a test script that creates an upper triangular matrix of the correct form and a right-hand side $\\boldsymbol{y}$ and solve for $\\boldsymbol{x}$. Your code needs to work on systems of arbitrarily large size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1ef90",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 4.4.4 Solving Systems with LU\n",
    "\n",
    "We are finally ready for the punch line of this whole $L U$ and triangular systems business!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277656b9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.34. \n",
    "\n",
    "If we want to solve $A \\boldsymbol{x}=\\boldsymbol{b}$ then\n",
    "a. If we can, write the system of equations as $L U \\boldsymbol{x}=\\boldsymbol{b}$.\n",
    "b. Solve $L \\boldsymbol{y}=\\boldsymbol{b}$ for $\\boldsymbol{y}$ using forward substitution.\n",
    "c. Solve $U \\boldsymbol{x}=\\boldsymbol{y}$ for $\\boldsymbol{x}$ using backward substitution.\n",
    "\n",
    "Pick a matrix $A$ and a right-hand side $\\boldsymbol{b}$ and solve the system using this process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddc6ed",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.35. \n",
    "\n",
    "Try the process again on the $3 \\times 3$ system of equations\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "3 & 6 & 8 \\\\\n",
    "2 & 7 & -1 \\\\\n",
    "5 & 2 & 2\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "-13 \\\\\n",
    "4 \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "That is: Find matrices $L$ and $U$ such that $A \\boldsymbol{x}=\\boldsymbol{b}$ can be written as $L U \\boldsymbol{x}=\\boldsymbol{b}$. Then do two triangular solves to determine $\\boldsymbol{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e87ff0",
   "metadata": {},
   "source": [
    "Let's take stock of what we have done so far.\n",
    "\n",
    "- Solving lower triangular systems is super fast and easy!\n",
    "- Solving upper triangular systems is super fast and easy (so long as we never divide by zero).\n",
    "- It is often possible to rewrite the matrix $A$ as the product of a lower triangular matrix $L$ and an upper triangular matrix $U$ so $A=L U$.\n",
    "- Now we can re-frame the equation $A \\boldsymbol{x}=\\boldsymbol{b}$ as $L U \\boldsymbol{x}=\\boldsymbol{b}$.\n",
    "- Substitute $\\boldsymbol{y}=U \\boldsymbol{x}$ so the system becomes $L \\boldsymbol{y}=\\boldsymbol{b}$. Solve for $\\boldsymbol{y}$ with forward substitution.\n",
    "- Now solve $U \\boldsymbol{x}=\\boldsymbol{y}$ using backward substitution.\n",
    "\n",
    "We have successfully take row reduction and turned into some fast matrix multiplications and then two very quick triangular solves. Ultimately this will be a faster algorithm for solving a system of linear equations.\n",
    "\n",
    "---\n",
    "\n",
    "### Definition 4.7. (Solving Linear Systems with the LU Decomposition) \n",
    "\n",
    "Let $A$ be a square matrix in $\\mathbb{R}^{n \\times n}$ and let $\\boldsymbol{x}, \\boldsymbol{b} \\in \\mathbb{R}^{n}$. To solve the problem $A \\boldsymbol{x}=\\boldsymbol{b}$,\n",
    "\n",
    "1. Factor $A$ into lower and upper triangular matrices $A=L U$. `L, U = myLU(A)`\n",
    "\n",
    "2. The system can now be written as $L U \\boldsymbol{x}=\\boldsymbol{b}$. Substitute $U \\boldsymbol{x}=\\boldsymbol{y}$ and solve the system $L \\boldsymbol{y}=\\boldsymbol{b}$ with forward substitution. `y = lsolve(L, b)`\n",
    "\n",
    "3. Finally, solve the system $U \\boldsymbol{x}=\\boldsymbol{y}$ with backward substitution. `x = usolve(U,y)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765f1be",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.36. \n",
    "\n",
    "Test your `lsolve`, `usolve`, and `myLU` functions on a linear system for which you know the answer. Then test your problem on a system that you don't know the solution to. As a way to compare your solutions you should:\n",
    "\n",
    "- Find Python's solution using np.linalg.solve() and compare your answer to that one using np.linalg.norm() to give the error between the two.\n",
    "- Time your code using the time library as follows\n",
    "- use the code starttime = time.time() before you start the main computation\n",
    "- use the code endtime $=$ time.time() after the main computation\n",
    "- then calculate the total elapsed time with totaltime = endtime starttime\n",
    "- Compare the timing of your $L U$ solve against np.linalg.solve() and against the RREF algorithm in the sympy library.\n",
    "\n",
    "```\n",
    "A = # Define your matrix\n",
    "b = # Defind your right-hand side vector\n",
    "# build a symbolic augmented matrix\n",
    "import sympy as sp\n",
    "Ab = sp.Matrix(np.c_[A,b])\n",
    "# note that np.c_[A,b] does a column concatenation of A with b\n",
    "t0 = time.time()\n",
    "Abrref = # row reduce the symbolic augmented matrix\n",
    "t1 = time.time()\n",
    "RREFTime = t1-t0\n",
    "t0=time.time()\n",
    "exact = # use np.linalg.solve() to solve the linear system\n",
    "t1=time.time()\n",
    "exactTime = t1-t0\n",
    "t0 = time.time()\n",
    "L, U = # get L and U from your myLU\n",
    "y = # use forward substitution to get y\n",
    "x = # use bacckward substituation to get x\n",
    "t1 = time.time()\n",
    "LUTime = t1-t0\n",
    "print(\"Time for symbolic RREF:\\t\\t\\t\",RREFTime)\n",
    "print(\"Time for np.linalg.solve() solution:\\t\",exactTime)\n",
    "print(\"Time for LU solution:\\t\\t\\t\",LUTime)\n",
    "err = np.linalg.norm(x-exact)\n",
    "print(\"Error between LU and np.linalg.solve():\",err)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11bef7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.37. \n",
    "\n",
    "The $L U$ decomposition is not perfect. Discuss where the\n",
    "algorithm will fail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0dde54",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.38. \n",
    "\n",
    "What happens when you try to solve the system of equations\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "7 \\\\\n",
    "9 \\\\\n",
    "-3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "with the $L U$ decomposition algorithm? Discuss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93f85d",
   "metadata": {},
   "source": [
    "### 4.5 The QR Factorization\n",
    "\n",
    "In this section we will try to find an improvement on the $L U$ factorization scheme from the previous section. What we'll do here is leverage the geometry of the column space of the $A$ matrix instead of leveraging the row reduction process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d689fe",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.39. \n",
    "\n",
    "We want to solve the system of equations\n",
    " \n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "1 / 3 & 2 / 3 & 2 / 3 \\\\\n",
    "2 / 3 & 1 / 3 & -2 / 3 \\\\\n",
    "-2 / 3 & 2 / 3 & -1 / 3\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "6 \\\\\n",
    "12 \\\\\n",
    "-9\n",
    "\\end{array}\\right) .\n",
    "$$\n",
    "\n",
    "1. We could do row reduction by hand ... yuck ... don't do this.\n",
    "\n",
    "2. We could apply our new-found skills with the $L U$ decomposition to solve the system, so go ahead and do that with your Python code.\n",
    "\n",
    "3. What do you get if you compute the product $A^{T} A$ ?\n",
    "    - Why do you get what you get? In other words, what was special about $A$ that gave such an nice result?\n",
    "    - What does this mean about the matrices $A$ and $A^{T}$ ?\n",
    "\n",
    "4. Now let's leverage what we found in part (c) to solve the system of equations $A \\boldsymbol{x}=\\boldsymbol{b}$ much faster. Multiply both sides of the matrix equation by $A^{T}$, and now you should be able to just read off the solution. This seems amazing!!\n",
    "\n",
    "5. What was it about this particular problem that made part (d) so elegant and easy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c492f",
   "metadata": {},
   "source": [
    "Theorem 4.1. (Orthonomal Matrices) The previous exercise tells us something amazing: If $A$ is an orthonormal matrix where the columns are mutually orthogonal and every column is a unit vector, then $A^{T}=A^{-1}$ and to solve the system of equation $A \\boldsymbol{x}=\\boldsymbol{b}$ we simply need to multiply both sides of the equation by $A^{T}$. Hence, the solution to $A \\boldsymbol{x}=\\boldsymbol{b}$ is just $\\boldsymbol{x}=A^{T} \\boldsymbol{b}$ in this special case.\n",
    "\n",
    "Theorem 4.1 begs an obvious question: Is there a way to turn any matrix A into an orthogonal matrix so that we can solve $A \\boldsymbol{x}=\\boldsymbol{b}$ in this same very efficient and fast way?\n",
    "\n",
    "The answer: Yes. Kind of.\n",
    "In essence, if we can factor our coefficient matrix into an orthonormal matrix and some other nicely formatted matrix (like a triangular matrix, perhaps) then the job of solving the linear system of equations comes down to matrix multiplication and a quick triangular solve - both of which are extremely extremely fast!\n",
    "\n",
    "What we will study in this section is a new matrix factorization called the $Q R$ factorization who's goal is to convert the matrix $A$ into a product of two matrices,\n",
    "$Q$ and $R$, where $Q$ is orthonormal and $R$ is upper triangular.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d146ed",
   "metadata": {},
   "source": [
    "Exercise 4.40. Let's say that we have a matrix $A$ and we know that it can be factored into $A=Q R$ where $Q$ is an orthonormal matrix and $R$ is an upper triangular matrix. How would we then leverage this factorization to solve the system of equation $A \\boldsymbol{x}=\\boldsymbol{b}$ for $\\boldsymbol{x}$ ?\n",
    "\n",
    "Before proceeding to the algorithm for the $Q R$ factorization let's pause for a moment and review scalar and vector projections from Linear Algebra. In Figure 4.1 we see a graphical depiction of the vector $\\boldsymbol{u}$ projected onto vector $\\boldsymbol{v}$. Notice that the projection is indeed the perpendicular projection as this is what seems natural geometrically.\n",
    "\n",
    "The vector projection of $\\boldsymbol{u}$ onto $\\boldsymbol{v}$ is the vector $c \\boldsymbol{v}$. That is, the vector projection of $\\boldsymbol{u}$ onto $\\boldsymbol{v}$ is a scalar multiple of the vector $\\boldsymbol{v}$. The value of the scalar $c$ is called the scalar projection of $\\boldsymbol{u}$ onto $\\boldsymbol{v}$.\n",
    "![](https://cdn.mathpix.com/cropped/2025_04_20_985fa04feb24e6557d79g-30.jpg?height=310&width=313&top_left_y=1198&top_left_x=993)\n",
    "\n",
    "Figure 4.1: Projection of one vector onto another.\n",
    "We can arrive at a formula for the scalar projection rather easily is we consider that the vector $\\boldsymbol{w}$ in Figure 4.1 must be perpendicular to $c \\boldsymbol{v}$. Hence\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} \\cdot(c \\boldsymbol{v})=0\n",
    "$$\n",
    "\n",
    "From vector geometry we also know that $\\boldsymbol{w}=\\boldsymbol{u}-c \\boldsymbol{v}$. Therefore\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{u}-c \\boldsymbol{v}) \\cdot(c \\boldsymbol{v})=0\n",
    "$$\n",
    "\n",
    "If we distribute we can see that\n",
    "\n",
    "$$\n",
    "c \\boldsymbol{u} \\cdot \\boldsymbol{v}-c^{2} \\boldsymbol{v} \\cdot \\boldsymbol{v}=0\n",
    "$$\n",
    "\n",
    "and therefore either $c=0$, which is only true if $\\boldsymbol{u} \\perp \\boldsymbol{v}$, or\n",
    "\n",
    "$$\n",
    "c=\\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\boldsymbol{v} \\cdot \\boldsymbol{v}}=\\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{v}\\|^{2}}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "- the scalar projection of $\\boldsymbol{u}$ onto $\\boldsymbol{v}$ is\n",
    "\n",
    "$$\n",
    "c=\\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{v}\\|^{2}}\n",
    "$$\n",
    "\n",
    "- the vector projection of $\\boldsymbol{u}$ onto $\\boldsymbol{v}$ is\n",
    "\n",
    "$$\n",
    "c \\boldsymbol{v}=\\left(\\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{v}\\|^{2}}\\right) \\boldsymbol{v}\n",
    "$$\n",
    "\n",
    "Another problem related to scalar and vector projections is to take a basis for the column space of a matrix and transform that basis into an orthogonal (or orthonormal) basis. Indeed, in Figure 4.1 if we have the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{cc}\n",
    "\\mid & \\mid \\\\\n",
    "\\boldsymbol{u} & \\boldsymbol{v} \\\\\n",
    "\\mid & \\mid\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "it should be clear from the picture that the columns of this matrix are not perpendicular. However, if we take the vector $\\boldsymbol{v}$ and the vector $\\boldsymbol{w}$ we do arrive at two orthogonal vector that form a basis for the same space. Moreover, if we normalize these vectors (by dividing by their respective lengths) then we can easily transform the original basis for the column space of $A$ into an orthonormal basis. This process is called the Gramm-Schmidt process, and you may have encountered it in your Linear Algebra class.\n",
    "\n",
    "Now we return to our goal of finding a way to factor a matrix $A$ into an orthonormal matrix $Q$ and an upper triangular matrix $R$. The algorithm that we are about to build depends greatly on the ideas of scalar and vector projections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a93f0",
   "metadata": {},
   "source": [
    "Exercise 4.41. We want to build a $Q R$ factorization of the matrix $A$ in the matrix equation $A \\boldsymbol{x}=\\boldsymbol{b}$ so that we can leverage the fact that solving the equation $Q R \\boldsymbol{x}=\\boldsymbol{b}$ is easy. Consider the matrix $A$ defined as\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ll}\n",
    "3 & 1 \\\\\n",
    "4 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Notice that the columns of $A$ are NOT othonormal (they are not unit vectors and they are not perpendicular to each other).\n",
    "a. Draw a picture of the two column vectors of $A$ in $\\mathbb{R}^{2}$. We'll use this picture to build geometric intuition for the rest of the $Q R$ factorization process.\n",
    "b. Define $\\boldsymbol{a}_{0}$ as the first column of $A$ and $\\boldsymbol{a}_{1}$ as the second column of $A$. That is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{a}_{0}=\\binom{3}{4} \\quad \\text { and } \\quad \\boldsymbol{a}_{1}=\\binom{1}{1}\n",
    "$$\n",
    "\n",
    "Turn $\\boldsymbol{a}_{0}$ into a unit vector and call this unit vector $\\boldsymbol{q}_{0}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_{0}=\\frac{a_{0}}{\\left\\|a_{0}\\right\\|}=(\\square)\n",
    "$$\n",
    "\n",
    "This vector $\\boldsymbol{q}_{0}$ will be the first column of the $2 \\times 2$ matrix $Q$. Why is this a nice place to start building the $Q$ matrix (think about the desired structure of $Q$ )?\n",
    "c. In your picture of $\\boldsymbol{a}_{0}$ and $\\boldsymbol{a}_{1}$ mark where $\\boldsymbol{q}_{0}$ is. Then draw the orthogonal projection from $\\boldsymbol{a}_{1}$ onto $\\boldsymbol{q}_{0}$. In your picture you should now see a right triangle with $\\boldsymbol{a}_{1}$ on the hypotenuse, the projection of $\\boldsymbol{a}_{1}$ onto $\\boldsymbol{q}_{0}$ on one leg, and the second leg is the vector difference of the hypotenuse and the first leg. Simplify the projection formula for leg 1 and write the formula for leg 2.\n",
    "\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\text { hypotenuse }=\\boldsymbol{a}_{1} \\\\\n",
    "\\operatorname{leg} 1=\\left(\\frac{\\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{0}}{\\boldsymbol{q}_{0} \\cdot \\boldsymbol{q}_{0}}\\right) \\boldsymbol{q}_{0}= \\\\\n",
    "\\operatorname{leg} 2= \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "d. Compute the vector for leg 2 and then normalize it to turn it into a unit vector. Call this vector $\\boldsymbol{q}_{1}$ and put it in the second column of $Q$.\n",
    "e. Verify that the columns of $Q$ are now orthogonal and are both unit vectors.\n",
    "f . The matrix $R$ is supposed to complete the matrix factorization $A=Q R$. We have built $Q$ as an orthonormal matrix. How can we use this fact to solve for the matrix $R$ ?\n",
    "g. You should now have an orthonormal matrix $Q$ and an upper triangular matrix $R$. Verify that $A=Q R$.\n",
    "h. An alternate way to build the $R$ matrix is to observe that\n",
    "\n",
    "$$\n",
    "R=\\left(\\begin{array}{cc}\n",
    "\\boldsymbol{a}_{0} \\cdot \\boldsymbol{q}_{0} & \\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{0} \\\\\n",
    "0 & \\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{1}\n",
    "\\end{array}\\right) .\n",
    "$$\n",
    "\n",
    "Show that this is indeed true for the matrix $A$ from this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6663931",
   "metadata": {},
   "source": [
    "Exercise 4.42. Keeping track of all of the arithmetic in the $Q R$ factorization process is quite challenging, so let's leverage Python to do some of the work for us. The following block of code walks through the previous exercise without any looping (that way we can see every step transparently). Some of the code is missing so you'll need to fill it in.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "# Define the matrix $A$\n",
    "A = np.matrix([[3,1],[4,1]])\n",
    "n = A.shape[0]\n",
    "# Build the vectors a0 and a1\n",
    "a0 = A[??? , ???] # ... write code to get column O from A\n",
    "a1 = A[??? , ???] # ... write code to get column 1 from A\n",
    "# Set up storage for Q\n",
    "Q = np.matrix( np.zeros( (n,n) ) )\n",
    "```\n",
    "\n",
    "```\n",
    "# build the vector q0 by normalizing aO\n",
    "q0 = a0 / np.linalg.norm(a0)\n",
    "# Put q0 as the first column of Q\n",
    "Q[:,0] = q0\n",
    "# Calculate the lengths of the two legs of the triangle\n",
    "leg1 = # write code to get the vector for leg 1 of the triangle\n",
    "leg2 = # write code to get the vector for leg 2 of the triangle\n",
    "# normalize leg2 and call it q1\n",
    "q1 = # write code to normalize leg2\n",
    "Q[:,1] = q1 # What does this line do?\n",
    "R = # ... build the R matrix out of A and Q\n",
    "print(\"The Q matrix is \\n\",Q,\"\\n\")\n",
    "print(\"The R matrix is \\n\",R,\"\\n\")\n",
    "print(\"The A matrix is \\n\",A,\"\\n\")\n",
    "print(\"The product QR is\\n\",Q*R)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0842d0",
   "metadata": {},
   "source": [
    "Exercise 4.43. You should notice that the code in the previous exercise does not depend on the specific matrix $A$ that we used? Put in a different $2 \\times 2$ matrix and verify that the process still works. That is, verify that $Q$ is orthonormal, $R$ is upper triangular, and $A=Q R$. Be sure, however, that your matrix $A$ is full rank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1000df",
   "metadata": {},
   "source": [
    "Exercise 4.44. Draw two generic vectors in $\\mathbb{R}^{2}$ and demonstrate the process outlined in the previous problem to build the vectors for the $Q$ matrix starting from your generic vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cb1ab",
   "metadata": {},
   "source": [
    "Exercise 4.45. Now we'll extend the process from the previous exercises to three dimensions. This time we will seek a matrix $Q$ that has three othonormal vectors starting from the three original columns of a $3 \\times 3$ matrix $A$. Perform each of the following steps by hand on the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "In the end you should end up with an orthonormal matrix $Q$ and an uper triangular matrix $R$.\n",
    "\n",
    "- Step 1: Pick column $\\boldsymbol{a}_{0}$ from the matrix $A$ and normalize it. Call this new vector $\\boldsymbol{q}_{0}$ and make that the first column of the matrix $Q$.\n",
    "- Step 2: Project column $\\boldsymbol{a}_{1}$ of $A$ onto $\\boldsymbol{q}_{0}$. This forms a right triangle with $\\boldsymbol{a}_{1}$ as the hypotenuse, the projection of $\\boldsymbol{a}_{1}$ onto $\\boldsymbol{q}_{0}$ as one of the legs, and\n",
    "the vector difference between these two as the second leg. Notice that the second leg of the newly formed right triangle is perpendicular to $\\boldsymbol{q}_{0}$ by design. If we normalize this vector then we have the second column of $Q$, $\\boldsymbol{q}_{1}$.\n",
    "- Step 3: Now we need a vector that is perpendicular to both $\\boldsymbol{q}_{0}$ AND $\\boldsymbol{q}_{1}$. To achieve this we are going to project column $\\boldsymbol{a}_{2}$ from $A$ onto the plane formed by $\\boldsymbol{q}_{0}$ and $\\boldsymbol{q}_{1}$. We'll do this in two steps:\n",
    "- Step 3a: We first project $\\boldsymbol{a}_{2}$ down onto both $\\boldsymbol{q}_{0}$ and $\\boldsymbol{q}_{1}$.\n",
    "- Step 3b: The vector that is perpendicular to both $\\boldsymbol{q}_{0}$ and $\\boldsymbol{q}_{1}$ will be the difference between $\\boldsymbol{a}_{2}$ the projection of $\\boldsymbol{a}_{2}$ onto $\\boldsymbol{q}_{0}$ and the projection of $\\boldsymbol{a}_{2}$ onto $\\boldsymbol{q}_{1}$. That is, we form the vector $\\boldsymbol{w}=\\boldsymbol{a}_{2}-\\left(\\boldsymbol{a}_{2}\\right.$. $\\left.\\boldsymbol{q}_{0}\\right) \\boldsymbol{q}_{0}-\\left(\\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{1}\\right) \\boldsymbol{q}_{1}$. Normalizing this vector will give us $\\boldsymbol{q}_{2}$. (Stop now and prove that $\\boldsymbol{q}_{2}$ is indeed perpendicular to both $\\boldsymbol{q}_{1}$ and $\\boldsymbol{q}_{0}$.)\n",
    "\n",
    "The result should be the matrix $Q$ which contains orthonormal columns. To build the matrix $R$ we simply recall that $A=Q R$ and $Q^{-1}=Q^{T}$ so $R=Q^{T} A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff21f3b",
   "metadata": {},
   "source": [
    "Exercise 4.46. Repeat the previous exercise but write code for each step so that Python can handle all of the computations. Again use the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6b984",
   "metadata": {},
   "source": [
    "Example 4.7. (QR for $n=3$ ) For the sake of clarity let's now write down the full $Q R$ factorization for a $3 \\times 3$ matrix.\n",
    "\n",
    "If the columns of $A$ are $\\boldsymbol{a}_{0}, \\boldsymbol{a}_{1}$, and $\\boldsymbol{a}_{2}$ then\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\boldsymbol{q}_{0}=\\frac{\\boldsymbol{a}_{0}}{\\left\\|\\boldsymbol{a}_{0}\\right\\|} \\\\\n",
    "\\boldsymbol{q}_{1}=\\frac{\\boldsymbol{a}_{1}-\\left(\\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{0}\\right) \\boldsymbol{q}_{0}}{\\left\\|\\boldsymbol{a}_{1}-\\left(\\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{0}\\right) \\boldsymbol{q}_{0}\\right\\|} \\\\\n",
    "\\boldsymbol{q}_{2}=\\frac{\\boldsymbol{a}_{2}-\\left(\\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{0}\\right) \\boldsymbol{q}_{0}-\\left(\\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{1}\\right) \\boldsymbol{q}_{1}}{\\left\\|\\boldsymbol{a}_{2}-\\left(\\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{0}\\right) \\boldsymbol{q}_{0}-\\left(\\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{1}\\right) \\boldsymbol{q}_{1}\\right\\|}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "R=\\left(\\begin{array}{ccc}\n",
    "\\boldsymbol{a}_{0} \\cdot \\boldsymbol{q}_{0} & \\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{0} & \\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{0} \\\\\n",
    "0 & \\boldsymbol{a}_{1} \\cdot \\boldsymbol{q}_{1} & \\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{1} \\\\\n",
    "0 & 0 & \\boldsymbol{a}_{2} \\cdot \\boldsymbol{q}_{2}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6b372",
   "metadata": {},
   "source": [
    "Exercise 4.47. (The QR Factorization) Now we're ready to build general code for the $Q R$ factorization. The following Python function definition is\n",
    "partially complete. Fill in the missing pieces of code and then test your code on square matrices of many different sizes. The easiest way to check if you have an error is to find the normed difference between $A$ and $Q R$ with np.linalg.norm (A - $\\mathrm{Q} * \\mathrm{R}$ ).\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "def myQR(A):\n",
    "    n = A.shape[0]\n",
    "    Q = np.matrix( np.zeros( (n,n) ) )\n",
    "    for j in range( ??? ): # The outer loop goes over the columns\n",
    "        q = A[:,j]\n",
    "        # The next loop is meant to do all of the projections.\n",
    "        # When do you start the inner loop and how far do you go?\n",
    "        # Hint: You don't need to enter this loop the first time\n",
    "        for i in range( ??? ):\n",
    "            length_of_leg = np.sum(A[:,j].T * Q[:,i])\n",
    "            q = q - ??? * ??? # This is where we do projections\n",
    "        Q[:,j] = q / np.linalg.norm(q)\n",
    "    R = # finally build the R matrix\n",
    "    return Q, R\n",
    "# Test Code\n",
    "A = np.matrix( ... )\n",
    "# or you can build A with use np.random.randn()\n",
    "# Often time random matrices are good test cases\n",
    "Q, R = myQR(A)\n",
    "error = np.linalg.norm(A - Q*R)\n",
    "print(error)\n",
    "```\n",
    "\n",
    "We now have a robust algorithm for doing $Q R$ factorization of square matrices we can finally return to solving systems of equations.\n",
    "Theorem 4.2. (Solving Systems with $Q R$ ) Remember that we want to solve $A \\boldsymbol{x}=\\boldsymbol{b}$ and since $A=Q R$ we can rewrite it with $Q R \\boldsymbol{x}=\\boldsymbol{b}$. Since we know that $Q$ is orthonormal by design we can multiply both sides of the equation by $Q^{T}$ to get $R \\boldsymbol{x}=Q^{T} \\boldsymbol{b}$. Finally, since $R$ is upper triangular we can use our usolve code from the previous section to solve the resulting triangular system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914332e",
   "metadata": {},
   "source": [
    "Exercise 4.48. Solve the system of equations\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 0\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "by first computing the $Q R$ factorization of $A$ and then solving the resulting upper triangular system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2188f",
   "metadata": {},
   "source": [
    "Exercise 4.49. Write code that builds a random $n \\times n$ matrix and a random $n \\times 1$ vector. Solve the equation $A \\boldsymbol{x}=\\boldsymbol{b}$ using the $Q R$ factorization and compare the answer to what we find from np.linalg.solve(). Do this many times for various values of $n$ and create a plot with $n$ on the horizontal axis and the normed error between Python's answer and your answer from the $Q R$ algorithm on the vertical axis. It would be wise to use a plt.semilogy() plot. To find the normed difference you should use np.linalg. norm(). What do you notice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df1f2a7",
   "metadata": {},
   "source": [
    "### 4.6 Over Determined Systems and Curve Fitting\n",
    "\n",
    "Exercise 4.50. In Exercise 3.81 we considered finding the quadratic function $f(x)=a x^{2}+b x+c$ that best fits the points\n",
    "\n",
    "$$\n",
    "(0,1.07),(1,3.9),(2,14.8),(3,26.8)\n",
    "$$\n",
    "\n",
    "Back in Exercise 3.81 and the subsequent problems we approached this problem using an optimization tool in Python. You might be surprised to learn that there is a way to do this same optimization with linear algebra!!\n",
    "\n",
    "We don't know the values of $a, b$, or $c$ but we do have four different $(x, y)$ ordered pairs. Hence, we have four equations:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "1.07=a(0)^{2}+b(0)+c \\\\\n",
    "3.9=a(1)^{2}+b(1)+c \\\\\n",
    "14.8=a(2)^{2}+b(2)+c \\\\\n",
    "26.8=a(3)^{2}+b(3)+c\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "There are four equations and only three unknowns. This is what is called an over determined systems - when there are more equations than unknowns. Let's play with this problem.\n",
    "a. First turn the system of equations into a matrix equation.\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "0 & 0 & 1 \\\\\n",
    "- & - & - \\\\\n",
    "- & - & -\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "a \\\\\n",
    "b \\\\\n",
    "c\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "1.07 \\\\\n",
    "3.9 \\\\\n",
    "14.8 \\\\\n",
    "26.8\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "b. None of our techniques for solving systems will likely work here since it is highly unlikely that the vector on the right-hand side of the equation is in the column space of the coefficient matrix. Discuss this.\n",
    "c. One solution to the unfortunate fact from part (b) is that we can project the vector on the right-hand side into the subspace spanned by the columns of the coefficient matrix. Think of this as casting the shadow of the righthand vector down onto the space spanned by the columns. If we do this projection we will be able to solve the equation for the values of $a, b$, and $c$ that will create the projection exactly - and hence be as close as we can get to the actual right-hand side. Draw a picture of what we've said here.\n",
    "d. Now we need to project the right-hand side, call it $\\boldsymbol{b}$, onto the column space of the the coefficient matrix $A$. Recall the following facts:\n",
    "\n",
    "- Projections are dot products\n",
    "- Matrix multiplication is nothing but a bunch of dot products.\n",
    "- The projections of $\\boldsymbol{b}$ onto the columns of $A$ are the dot products of $\\boldsymbol{b}$ with each of the columns of $A$.\n",
    "- What matrix can we multiply both sides of the equation $\\boldsymbol{A x}=\\boldsymbol{b}$ by in order for the right-hand side to become the projection that we want? (Now do the projection in Python)\n",
    "e. If you have done part (d) correctly then you should now have a square system (i.e. the matrix on the left-hand side should now be square). Solve this system for $a, b$, and $c$. Compare your answers to what you found way back in Exercise 3.81.\n",
    "\n",
    "Theorem 4.3. (Solving Overdetermined Systems) If $\\boldsymbol{A x}=\\boldsymbol{b}$ is an overdetermined system (i.e. A has more rows than columns) then we first multiply both sides of the equation by $A^{T}$ (why do we do this?) and then solve the square system of equations $\\left(A^{T} A\\right) \\boldsymbol{x}=A^{T} \\boldsymbol{b}$ using a system solving like $L U$ or $Q R$. The answer to this new system is interpreted as the vector $\\boldsymbol{x}$ which solves exactly for the projection of $\\boldsymbol{b}$ onto the column space of $A$.\n",
    "The equation $\\left(A^{T} A\\right) \\boldsymbol{x}=A^{T} \\boldsymbol{b}$ is called the normal equations and arises often in Statistics and Machine Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bb57b",
   "metadata": {},
   "source": [
    "Exercise 4.51. Fit a linear function to the following data. Solve for the slope and intercept using the technique outlined in Theorem 4.3. Make a plot of the points along with your best fit curve.\n",
    "\n",
    "| $x$ | $y$ |\n",
    "| :--- | :--- |\n",
    "| 0 | 4.6 |\n",
    "| 1 | 11 |\n",
    "| 2 | 12 |\n",
    "| 3 | 19.1 |\n",
    "| 4 | 18.8 |\n",
    "| 5 | 39.5 |\n",
    "| 6 | 31.1 |\n",
    "| 7 | 43.4 |\n",
    "| 8 | 40.3 |\n",
    "| 9 | 41.5 |\n",
    "| 10 | 41.6 |\n",
    "\n",
    "Code to download the data directly is given below.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "URL1 = 'https://raw.githubusercontent.com/NumericalMethodsSullivan'\n",
    "URL2 = '/NumericalMethodsSullivan.github.io/master/data/'\n",
    "URL = URL1+URL2\n",
    "data = np.array( pd.read_csv(URL+'Exercise4_51.csv') )\n",
    "```\n",
    "\n",
    "```\n",
    "# Exercise4_51.csv\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c64fb4",
   "metadata": {},
   "source": [
    "Exercise 4.52. Fit a quadratic function to the following data using the technique outlined in Theorem 4.3. Make a plot of the points along with your best fit curve.\n",
    "\n",
    "| $x$ | $y$ |\n",
    "| :--- | :--- |\n",
    "| 0 | -6.8 |\n",
    "| 1 | 11.8 |\n",
    "| 2 | 50.6 |\n",
    "| 3 | 94 |\n",
    "| 4 | 224.3 |\n",
    "| 5 | 301.7 |\n",
    "| 6 | 499.2 |\n",
    "| 7 | 454.7 |\n",
    "| 8 | 578.5 |\n",
    "| 9 | 1102 |\n",
    "| 10 | 1203.2 |\n",
    "\n",
    "Code to download the data directly is given below.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "URL1 = 'https://raw.githubusercontent.com/NumericalMethodsSullivan'\n",
    "URL2 = '/NumericalMethodsSullivan.github.io/master/data/'\n",
    "URL = URL1+URL2\n",
    "data = np.array( pd.read_csv(URL+'Exercise4_52.csv') )\n",
    "# Exercise4_52.csv\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b3e05",
   "metadata": {},
   "source": [
    "Exercise 4.53. The Statistical technique of curve fitting is often called \"linear regression.\" This even holds when we are fitting quadratic functions, cubic functions, etc to the data ... we still call that linear regression! Why?\n",
    "\n",
    "This section of the text on solving over determined systems is just a bit of a teaser for a bit of higher-level statistics, data science, and machine learning. The normal equations and solving systems via projections is the starting point of many modern machine learning algorithms. For more information on this sort of problem look into taking some statistics, data science, and/or machine learning courses. You'll love it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c498a",
   "metadata": {},
   "source": [
    "### 4.7 The Eigenvalue-Eigenvector Problem\n",
    "\n",
    "We finally turn our attention to the last major topic in numerical linear algebra in this course. ${ }^{4}$\n",
    "\n",
    "Definition 4.8. (The Eigenvalue Problem) Recall that the eigenvectors, $\\boldsymbol{x}$, and the eigenvalues, $\\lambda$ of a square matrix satisfy the equation $A \\boldsymbol{x}=\\lambda \\boldsymbol{x}$. Geometrically, the eign-problem is the task of finding the special vectors $\\boldsymbol{x}$ such that multiplication by the matrix $A$ only produces a scalar multiple of $\\boldsymbol{x}$.\n",
    "\n",
    "Thinking about matrix multiplication, the geometric notion of the eigenvalue problem is rather peculiar since matrix-vector multiplication usually results in a scaling and a rotation of the vector $\\boldsymbol{x}$. Therefore, in some sense the eigenvectors are the only special vectors which avoid geometric rotation under matrix multiplication. For a graphical exploration of this idea see:\n",
    "https://www.geogebra.org/m/JP2XZpzV.\n",
    "\n",
    "Theorem 4.4. Recall that to solve the eigen-problem for a square matrix $A$ we complete the following steps:\n",
    "a. First rearrange the definition of the eigenvalue-eigenvector pair to\n",
    "\n",
    "$$\n",
    "(A \\boldsymbol{x}-\\lambda \\boldsymbol{x})=\\mathbf{0}\n",
    "$$\n",
    "\n",
    "b. Next, factor the $\\boldsymbol{x}$ on the right to get\n",
    "\n",
    "$$\n",
    "(A-\\lambda I) \\boldsymbol{x}=\\mathbf{0}\n",
    "$$\n",
    "\n",
    "c. Now observe that since $\\boldsymbol{x} \\neq 0$ the matrix $A-\\lambda I$ must NOT have an inverse. Therefore,\n",
    "\n",
    "$$\n",
    "\\operatorname{det}(A-\\lambda I)=0\n",
    "$$\n",
    "\n",
    "d. Solve the equation $\\operatorname{det}(A-\\lambda I)=0$ for all of the values of $\\lambda$.\n",
    "$e$. For each $\\lambda$, find a solution to the equation $(A-\\lambda I) \\boldsymbol{x}=\\mathbf{0}$. Note that there will be infinitely many solutions so you will need to make wise choices for the free variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c30e1",
   "metadata": {},
   "source": [
    "Exercise 4.54. Find the eigenvalues and eigenvectors of\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ll}\n",
    "1 & 2 \\\\\n",
    "4 & 3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f36c7",
   "metadata": {},
   "source": [
    "Exercise 4.55. In the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{lll}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "one of the eigenvalues is $\\lambda_{1}=0$.\n",
    "a. What does that tell us about the matrix $A$ ?\n",
    "b. What is the eigenvector $\\boldsymbol{v}_{1}$ associated with $\\lambda_{1}=0$ ?\n",
    "c. What is the null space of the matrix $A$ ?\n",
    "\n",
    "OK. Now that you recall some of the basics let's play with a little limit problem. The following exercises are going to work us toward the power method for finding certain eigen-structure of a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0c027",
   "metadata": {},
   "source": [
    "Exercise 4.56. Consider the matrix\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ccc}\n",
    "8 & 5 & -6 \\\\\n",
    "-12 & -9 & 12 \\\\\n",
    "-3 & -3 & 5\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "This matrix has the following eigen-structure:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\boldsymbol{v}_{1}=\\left(\\begin{array}{c}\n",
    "1 \\\\\n",
    "-1 \\\\\n",
    "0\n",
    "\\end{array}\\right) \\quad \\text { with } \\quad \\lambda_{1}=3 \\\\\n",
    "\\boldsymbol{v}_{2}=\\left(\\begin{array}{l}\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right) \\quad \\text { with } \\quad \\lambda_{2}=2 \\\\\n",
    "\\boldsymbol{v}_{3}=\\left(\\begin{array}{c}\n",
    "-1 \\\\\n",
    "3 \\\\\n",
    "1\n",
    "\\end{array}\\right) \\quad \\text { with } \\quad \\lambda_{3}=-1\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "If we have\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}=-2 \\boldsymbol{v}_{1}+1 \\boldsymbol{v}_{2}-3 \\boldsymbol{v}_{3}=\\left(\\begin{array}{c}\n",
    "3 \\\\\n",
    "-7 \\\\\n",
    "-1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "then we want to do a bit of an experiment. What happens when we iteratively multiply $\\boldsymbol{x}$ by $A$ but at the same time divide by the largest eigenvalue. Let's see:\n",
    "\n",
    "- What is $A^{1} \\boldsymbol{x} / 3^{1}$ ?\n",
    "- What is $A^{2} \\boldsymbol{x} / 3^{2}$ ?\n",
    "- What is $A^{3} \\boldsymbol{x} / 3^{3}$ ?\n",
    "- What is $A^{4} \\boldsymbol{x} / 3^{4}$ ?\n",
    "- ...\n",
    "\n",
    "It might be nice now to go to some Python code to do the computations (if you haven't already). Use your code to conjecture about the following limit.\n",
    "\n",
    "$$\n",
    "\\lim _{k \\rightarrow \\infty} \\frac{A^{k} \\boldsymbol{x}}{\\lambda_{\\max }^{k}}=? ? ?\n",
    "$$\n",
    "\n",
    "In this limit we are really interested in the direction of the resulting vector, not the magnitude. Therefore, in the code below you will see that we normalize the resulting vector so that it is a unit vector.\n",
    "\n",
    "Note: be careful, computers don't do infinity, so for powers that are too large you won't get any results.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "A = np.matrix([[8,5,-6],[-12,-9,12],[-3,-3,5]])\n",
    "x = np.matrix([[3],[-7],[-1]])\n",
    "eigval_max = 3\n",
    "k = 4\n",
    "result = A**k * x / eigval_max**k\n",
    "print(result / np.linalg.norm(result) )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71cdd5",
   "metadata": {},
   "source": [
    "Exercise 4.57. If a matrix $A$ has eigenvectors $\\boldsymbol{v}_{1}, \\boldsymbol{v}_{2}, \\boldsymbol{v}_{3}, \\cdots, \\boldsymbol{v}_{n}$ with eigenvalues $\\lambda_{1}, \\lambda_{2}, \\lambda_{3}, \\ldots, \\lambda_{n}$ and $\\boldsymbol{x}$ is in the column space of $A$ then what will we get, approximately, if we evaluate $A^{k} \\boldsymbol{x} / \\max _{j}\\left(\\lambda_{j}\\right)^{k}$ for very large values of $k$ ?\n",
    "\n",
    "Discuss your conjecture with your peers. Then try to verify it with several numerical examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324bb8c",
   "metadata": {},
   "source": [
    "Exercise 4.58. Explain your result from the previous exercise geometrically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ff93a",
   "metadata": {},
   "source": [
    "Exercise 4.59. The algorithm that we've been toying with will find the dominant eigenvector of a matrix fairly quickly. Why might you be only interested in the dominant eigenvector of a matrix? Discuss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c72715",
   "metadata": {},
   "source": [
    "Exercise 4.60. In this problem we will formally prove the conjecture that you just made. This conjecture will lead us to the power method for finding the dominant eigenvector and eigenvalue of a matrix.\n",
    "a. Assume that $A$ has $n$ linearly independent eigenvectors $\\boldsymbol{v}_{1}, \\boldsymbol{v}_{2}, \\ldots, \\boldsymbol{v}_{n}$ and\n",
    "choose $\\boldsymbol{x}=\\sum_{j=1}^{n} c_{j} \\boldsymbol{v}_{j}$. You have proved in the past that\n",
    "\n",
    "$$\n",
    "A^{k} \\boldsymbol{x}=c_{1} \\lambda_{1}^{k} \\boldsymbol{v}_{1}+c_{2} \\lambda_{2}^{k} \\boldsymbol{v}_{2}+\\cdots c_{n} \\lambda_{n}^{k} \\boldsymbol{v}_{n} .\n",
    "$$\n",
    "\n",
    "Stop and sketch out the details of this proof now.\n",
    "b. If we factor $\\lambda_{1}^{k}$ out of the right-hand side we get\n",
    "\n",
    "$$\n",
    "A^{k} \\boldsymbol{x}=\\lambda_{1}^{k}\\left(c_{1} ? ? ?+c_{2}\\left(\\frac{? ? ?}{? ? ?}\\right)^{k} \\boldsymbol{v}_{2}+c_{3}\\left(\\frac{? ? ?}{? ? ?}\\right)^{k} \\boldsymbol{v}_{3}+\\cdots+c_{n}\\left(\\frac{? ? ?}{? ? ?}\\right)^{k} \\boldsymbol{v}_{n}\\right)\n",
    "$$\n",
    "\n",
    "(fill in the question marks)\n",
    "c. If $\\left|\\lambda_{1}\\right|>\\left|\\lambda_{2}\\right| \\geq\\left|\\lambda_{3}\\right| \\geq \\cdots \\geq\\left|\\lambda_{n}\\right|$ then what happens to each of the $\\left(\\lambda_{j} / \\lambda_{1}\\right)^{k}$ terms as $k \\rightarrow \\infty$ ?\n",
    "d. Using your answer to part (c), what is $\\lim _{k \\rightarrow \\infty} A^{k} \\boldsymbol{x} / \\lambda_{1}^{k}$ ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c97b4",
   "metadata": {},
   "source": [
    "Theorem 4.5. (The Power Method) The following algorithm, called the power method will quickly find the eigenvalue of largest absolute value for a square matrix $A \\in \\mathbb{R}^{n \\times n}$ as well as the associated (normalized) eigenvector. We are assuming that there are $n$ linearly independent eigenvectors of $A$.\n",
    "\n",
    "Step \\#1: Given a nonzero vector $\\boldsymbol{x}$, set $\\boldsymbol{v}^{(1)}=\\boldsymbol{x} /\\|\\boldsymbol{x}\\|$. (Here the superscript indicates the iteration number) Note that the initial vector $\\boldsymbol{x}$ is pretty irrelevant to the process so it can just be a random vector of the correct size..\n",
    "\n",
    "Step \\#2: For $k=2,3, \\ldots$\n",
    "Step \\#2a: Compute $\\tilde{\\boldsymbol{v}}^{(k)}=A \\boldsymbol{v}^{(k-1)}$ (this gives a non-normalized version of the next estimate of the dominant eigenvector.)\n",
    "Step \\#2b: Set $\\lambda^{(k)}=\\tilde{\\boldsymbol{v}}^{(k)} \\cdot \\boldsymbol{v}^{(k-1)}$. (this gives an approximation of the eigenvalue since if $\\boldsymbol{v}^{(k-1)}$ was the actual eigenvector we would have $\\lambda=A \\boldsymbol{v}^{(k-1)} \\cdot \\boldsymbol{v}^{(k-1)}$. Stop now and explain this.)\n",
    "Step \\#2c: Normalize $\\tilde{\\boldsymbol{v}}^{(k)}$ by computing $\\boldsymbol{v}^{(k)}=\\tilde{\\boldsymbol{v}}^{(k)} /\\left\\|\\tilde{\\boldsymbol{v}}^{(k)}\\right\\|$. (This guarantees that you will be sending a unit vector into the next iteration of the loop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa960e57",
   "metadata": {},
   "source": [
    "Exercise 4.61. Go through Theorem 4.5 carefully and describe what we need to do in each step and why we're doing it. Then complete all of the missing pieces of the following Python function.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "def myPower(A, tol = 1e-8):\n",
    "    n = A.shape[0]\n",
    "```\n",
    "\n",
    "```\n",
    "x = np.matrix( np.random.randn(n,1) )\n",
    "x = # turn x into a unit vector\n",
    "# we don't actually need to keep track of the old iterates\n",
    "L = 1 # initialize the dominant eigenvalue\n",
    "counter = 0 # keep track of how many steps we've taken\n",
    "# You can build a stopping rule from the definition\n",
    "# Ax = lambda x ...\n",
    "while (???) > tol and counter < 10000:\n",
    "    x = A*x # update the dominant eigenvector\n",
    "    x = ??? # normalize\n",
    "    L = ??? # approximate the eignevalue\n",
    "    counter += 1 # increment the counter\n",
    "return x, L\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf3b33",
   "metadata": {},
   "source": [
    "Exercise 4.62. Test your myPower() function on several matrices where you know the eigenstructure. Then try the myPower() function on larger random matrices. You can check that it is working using np.linalg.eig() (be sure to normalize the vectors in the same way so you can compare them.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff2007",
   "metadata": {},
   "source": [
    "Exercise 4.63. In the Power Method iteration you may end up getting a different sign on your eigenvector as compared to np.linalg.eig(). Why might this happen? Generate a few examples so you can see this. You can avoid this issue if you use a while loop in your Power Method code and the logical check takes advantage of the fact that we are trying to solve the equation $\\boldsymbol{A x}=\\lambda \\boldsymbol{x}$. Hint: $A \\boldsymbol{x}=\\lambda \\boldsymbol{x}$ is equivalent to $A \\boldsymbol{x}-\\lambda \\boldsymbol{x}=\\mathbf{0}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693e589",
   "metadata": {},
   "source": [
    "Exercise 4.64. What happens in the power method iterations when $\\lambda_{1}$ is complex. The maximum eigenvalue can certainly be complex if $\\left|\\lambda_{1}\\right|$ (the modulus of the complex number) is larger than all of the other eigenvalues. It may be helpful to build a matrix specifically with complex eigenvalues. ${ }^{5}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9707e",
   "metadata": {},
   "source": [
    "Exercise 4.65. (onvergence Rate of the Power Method) The proof that the power method will work hinges on the fact that $\\left|\\lambda_{1}\\right|>\\left|\\lambda_{2}\\right| \\geq\\left|\\lambda_{3}\\right| \\geq \\cdots \\geq\\left|\\lambda_{n}\\right|$.\n",
    "\n",
    "[^4]In Exercise 4.60 we proved that the limit\n",
    "\n",
    "$$\n",
    "\\lim _{k \\rightarrow \\infty} \\frac{A^{k} \\boldsymbol{x}}{\\lambda_{1}^{k}}\n",
    "$$\n",
    "\n",
    "converges to the dominant eigenvector, but how fast is the convergence? What does the speed of the convergence depend on?\n",
    "\n",
    "Take note that since we're assuming that the eigenvalues are ordered, the ratio $\\lambda_{2} / \\lambda_{1}$ will be larger than $\\lambda_{j} / \\lambda_{1}$ for all $j>2$. Hence, the speed at which the power method converges depends mostly on the ratio $\\lambda_{2} / \\lambda_{1}$. Let's build a numerical experiment to see how sensitive the power method is to this ratio.\n",
    "\n",
    "Build a $4 \\times 4$ matrix $A$ with dominant eigenvalue $\\lambda_{1}=1$ and all other eigenvalues less than 1 in absolute value. Then choose several values of $\\lambda_{2}$ and build an experiment to determine the number of iterations that it takes for the power method to converge to within a pre-determined tolerance to the dominant eigenvector. In the end you should produce a plot with the ratio $\\lambda_{2} / \\lambda_{1}$ on the horizontal axis and the number of iterations to converge to a fixed tolerance on the vertical axis. Discuss what you see in your plot.\n",
    "\n",
    "Hint: To build a matrix with specific eigen-structure use the matrix factorization $A=P D P^{-1}$ where the columns of $P$ contain the eigenvectors of $A$ and the diagonal of $D$ contains the eigenvalues. In this case the $P$ matrix can be random but you need to control the $D$ matrix. Moreover, remember that $\\lambda_{3}$ and $\\lambda_{4}$ should be smaller than $\\lambda_{2}$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
