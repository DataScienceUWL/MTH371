{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d67718",
   "metadata": {},
   "source": [
    "### 4.8 Exercises\n",
    "\n",
    "### 4.8.1 Algorithm Summaries\n",
    "\n",
    "Exercise 4.66. Explain in clear language how to efficiently solve an upper triangular system of linear equations.\n",
    "\n",
    "Exercise 4.67. Explain in clear language how to efficiently solve a lower triangular system of linear equations.\n",
    "\n",
    "Exercise 4.68. Explain in clear language how to solve the equation $A \\boldsymbol{x}=\\boldsymbol{b}$ using an $L U$ decomposition.\n",
    "\n",
    "Exercise 4.69. Explain in clear language how to solve an overdetermined system of linear equations (more equations than unknowns) numerically.\n",
    "\n",
    "Exercise 4.70. Explain in clear language the algorithm for finding the columns of the $Q$ matrix in the $Q R$ factorization. Give all of the mathematical details.\n",
    "\n",
    "Exercise 4.71. Explain in clear language how to find the upper triangular matrix $R$ in the $Q R$ factorization. Give all of the mathematical details.\n",
    "\n",
    "Exercise 4.72. Explain in clear language how to solve the equation $A \\boldsymbol{x}=\\boldsymbol{b}$ using a $Q R$ decomposition.\n",
    "\n",
    "Exercise 4.73. Explain in clear language how the power method works to find the dominant eigenvalue and eigenvector of a square matrix. Give all of the mathematical details.\n",
    "\n",
    "### 4.8.2 Applying What You've Learned\n",
    "\n",
    "Exercise 4.74. As mentioned much earlier in this chapter, there is an rref () command in Python, but it is in the sympy library instead of the numpy library it is implemented as a symbolic computation instead of a numerical computation. OK. So what? In this problem we want to compare the time to solve a system of equations $A \\boldsymbol{x}=\\boldsymbol{b}$ with each of the following techniques:\n",
    "\n",
    "- row reduction of an augmented matrix $\\left(\\begin{array}{l|l}A & \\mid\\end{array}\\right)$ with sympy,\n",
    "- our implementation of the $L U$ decomposition,\n",
    "- our implementation of the $Q R$ decomposition, and\n",
    "- the numpy.linalg.solve() command.\n",
    "\n",
    "To time code in Python first import the time library. Then use start = time.time() at the start of your code and stop = time.time() and the end of your code. The difference between stop and start is the elapsed computation time.\n",
    "\n",
    "Make observations about how the algorithms perform for different sized matrices. You can use random matrices and vectors for $A$ and $\\boldsymbol{b}$. The end result should be a plot showing how the average computation time for each algorithm behaves as a function of the size of the coefficient matrix.\n",
    "\n",
    "The code below will compute the reduced row echelon form of a matrix (RREF). Implement the code so that you know how it works.\n",
    "\n",
    "```\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "# in this problem it will be easiest to start with numpy matrices\n",
    "A = np.matrix([[1, 0, 1], [2, 3, 5], [-1, -3, -3]])\n",
    "b = np.matrix([[3], [7], [3]])\n",
    "Augmented = np.c_[A,b] # augment b onto the right hand side of A\n",
    "Msymbolic = sp.Matrix(Augmented)\n",
    "MsymbolicRREF = Msymbolic.rref()\n",
    "print(MsymbolicRREF)\n",
    "```\n",
    "\n",
    "To time code you can use code like the following.\n",
    "\n",
    "```\n",
    "import time\n",
    "start = time.time()\n",
    "# some code that you want to time\n",
    "stop = time.time()\n",
    "total_time = stop - start\n",
    "print(\"Total computation time=\",total_time)\n",
    "```\n",
    "\n",
    "Exercise 4.75. Imagine that we have a 1 meter long thin metal rod that has been heated to $100^{\\circ}$ on the left-hand side and cooled to $0^{\\circ}$ on the right-hand side. We want to know the temperature every 10 cm from left to right on the rod.\n",
    "a. First we break the rod into equal 10 cm increments as shown. See Figure 4.2. How many unknowns are there in this picture?\n",
    "b. The temperature at each point along the rod is the average of the temperatures at the adjacent points. For example, if we let $T_{1}$ be the temperature\n",
    "at point $x_{1}$ then\n",
    "\n",
    "$$\n",
    "T_{1}=\\frac{T_{0}+T_{2}}{2}\n",
    "$$\n",
    "\n",
    "Write a system of equations for each of the unknown temperatures.\n",
    "c. Solve the system for the temperature at each unknown node using either $L U$ or $Q R$ decomposition.\n",
    "![](https://cdn.mathpix.com/cropped/2025_04_20_985fa04feb24e6557d79g-48.jpg?height=71&width=714&top_left_y=748&top_left_x=798)\n",
    "\n",
    "Figure 4.2: A rod to be heated broken into 10 equal-length segments.\n",
    "\n",
    "Exercise 4.76. Write code to solve the following systems of equations via both LU and QR decompositions. If the algorithm fails then be sure to explain exactly why.\n",
    "a.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x+2 y+3 z & =4 \\\\\n",
    "2 x+4 y+3 z & =5 \\\\\n",
    "x+y & =4\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "b.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "2 y+3 z & =4 \\\\\n",
    "2 x+3 z & =5 \\\\\n",
    "y & =4\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "c.\n",
    "\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "2 y+3 z=4 \\\\\n",
    "2 x+4 y+3 z=5 \\\\\n",
    "x+y=4\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Exercise 4.77. Give a specific example of a nonzero matrix which will NOT have an $L U$ decomposition. Give specific reasons why $L U$ will fail on your matrix.\n",
    "\n",
    "Exercise 4.78. Give a specific example of a nonzero matrix which will NOT have an $Q R$ decomposition. Give specific reasons why $Q R$ will fail on your matrix.\n",
    "\n",
    "Exercise 4.79. Have you ever wondered how scientific software computes a determinant? The formula that you learned for calculating determinants by hand is horribly cumbersome and computationally intractible for large matrices.\n",
    "\n",
    "This problem is meant to give you glimpse of what is actually going on under the hood. ${ }^{6}$\n",
    "\n",
    "If $A$ has an $L U$ decomposition then $A=L U$. Use properties that you know about determinants to come up with a simple way to find the determinant for matrices that have an $L U$ decomposition. Show all of your work in developing your formula.\n",
    "\n",
    "Once you have your formula for calculating $\\operatorname{det}(A)$, write a Python function that accepts a matrix, produces the $L U$ decomposition, and returns the determinant of $A$. Check your work against Python's np.linalg. $\\operatorname{det}$ () function.\n",
    "\n",
    "Exercise 4.80. For this problem we are going to run a numerical experiment to see how the process of solving the equation $A \\boldsymbol{x}=\\boldsymbol{b}$ using the $L U$ factorization performs on random coefficient matrices $A$ and random right-hand sides $\\boldsymbol{b}$. We will compare against Python's algorithm for solving linear systems.\n",
    "\n",
    "We will do the following:\n",
    "Create a loop that does the following:\n",
    "a. Loop over the size of the matrix $n$.\n",
    "b. Build a random matrix $A$ of size $n \\times n$. You can do this with the code A $=n p \\cdot \\operatorname{matrix}(n p \\cdot r a n d o m \\cdot \\operatorname{randn}(n, n))$\n",
    "c. Build a random vector $\\boldsymbol{b}$ in $\\mathbb{R}^{n}$. You can do this with the code $\\mathrm{b}=$ np.matrix ( np.random.randn(n,1) )\n",
    "d. Find Python's answer to the problem $A \\boldsymbol{x}=\\boldsymbol{b}=0$ using the command exact = np.linalg.solve(A,b)\n",
    "e. Write code that uses your three $L U$ functions (myLU, lsolve, usolve) to find a solution to the equation $A \\boldsymbol{x}=\\boldsymbol{b}$.\n",
    "f. Find the error between your answer and the exact answer using the code np.linalg.norm(x - exact)\n",
    "g. Make a plot (plt. $\\operatorname{semilogy}())$ that shows how the error behaves as the size of the problem changes. You should run this for matrices of larger and larger size but be warned that the loop will run for quite a long time if you go above $300 \\times 300$ matrices. Just be patient.\n",
    "\n",
    "Conclusions: What do you notice in your final plot. What does this tell you about the behavior of our $L U$ decomposition code?\n",
    "\n",
    "Exercise 4.81. Repeat Exercise 4.80 for the $Q R$ decomposition. Your final plot should show both the behavior of $Q R$ and of $L U$ throughout the experiement. What do you notice?\n",
    "\n",
    "[^5]Exercise 4.82. Find a least squares solution to the equation $\\boldsymbol{A x}=\\boldsymbol{b}$ in two different ways with\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{ccc}\n",
    "1 & 3 & 5 \\\\\n",
    "4 & -2 & 6 \\\\\n",
    "4 & 7 & 8 \\\\\n",
    "3 & 7 & 19\n",
    "\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{b}=\\left(\\begin{array}{c}\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "-2 \\\\\n",
    "8\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Exercise 4.83. Let $A$ be defined as\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{cc}\n",
    "10^{-20} & 1 \\\\\n",
    "1 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and let $\\boldsymbol{b}$ be the vector\n",
    "\n",
    "$$\n",
    "\\boldsymbol{b}=\\binom{2}{3}\n",
    "$$\n",
    "\n",
    "Notice that $A$ has a tiny, but nonzero, value in the first entry.\n",
    "a. Solve the linear system $A \\boldsymbol{x}=\\boldsymbol{b}$ by hand.\n",
    "b. Use your myLU, lsolve, and usolve functions to solve this problem using the LU decomposition method.\n",
    "c. Compare your answers to parts (a) and (b). What went wrong?\n",
    "\n",
    "Exercise 4.84. (Hilbert Matrices) A Hilbert Matrix is a matrix of the form $H_{i j}=1 /(i+j+1)$ where both $i$ and $j$ both start indexed at 0 . For example, a $4 \\times 4$ Hilbert Matrix is\n",
    "\n",
    "$$\n",
    "H=\\left(\\begin{array}{cccc}\n",
    "1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} \\\\\n",
    "\\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} \\\\\n",
    "\\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} \\\\\n",
    "\\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "This type of matrix is often used to test numerical linear algebra algorithms since it is known to have some odd behaviors ... which you'll see in a moment.\n",
    "a. Write code to build a $n \\times n$ Hilbert Matrix and call this matrix $H$. Test your code for various values of $n$ to be sure that it is building the correct matrices.\n",
    "b. Build a vector of ones called $\\boldsymbol{b}$ with code $\\mathrm{b}=\\mathrm{np}$. ones ( $(\\mathrm{n}, 1)$ ). We will use $\\boldsymbol{b}$ as the right hand side of the system of equations $H \\boldsymbol{x}=\\boldsymbol{b}$.\n",
    "c. Solve the system of equations $H \\boldsymbol{x}=\\boldsymbol{b}$ using any technique you like from this chapter.\n",
    "d. Now let's say that you change the first entry of $\\boldsymbol{b}$ by just a little bit, say $10^{-15}$. If we were to now solve the equation $H \\boldsymbol{x}_{\\text {new }}=\\boldsymbol{b}_{\\text {new }}$ what would you expect as compared to solving $H \\boldsymbol{x}=\\boldsymbol{b}$.\n",
    "e. Now let's actually make the change suggested in part (d). Use the code bnew $=\\mathrm{np}$.ones $((\\mathrm{n}, 1))$ and then bnew[0] $=$ bnew[0] $+1 \\mathrm{e}-15$ to build a new $\\boldsymbol{b}$ vector with this small change. Solve $H \\boldsymbol{x}=\\boldsymbol{b}$ and $H \\boldsymbol{x}_{\\text {new }}=\\boldsymbol{b}_{\\text {new }}$ and then compare the maximum absolute difference $n p \\cdot \\max (\\mathrm{np} . \\mathrm{abs}(\\mathrm{x}$ - xnew ) ). What do you notice? Make a plot with $n$ on the horizontal axis and the maximum absolute difference on the vertical axis. What does this plot tell you about the solution to the equation $H \\boldsymbol{x}=\\boldsymbol{b}$ ?\n",
    "f. We know that $H H^{-1}$ should be the identity matrix. As we'll see, however, Hilbert matrices are particularly poorly behaved! Write a loop over $n$ that (i) builds a Hilbert matrix of size $n$, (ii) calculates $H H^{-1}$ (using np.linalg.inv() to compute the inverse directly), (iii) calculates the norm of the difference between the identity matrix (np.identity (n)) and your calculated identity matrix from part (ii). Finally. Build a plot that shows $n$ on the horizontal axis and the normed difference on the vertical axis. What do you see? What does this mean about the matrix inversion of the Hilbert matrix.\n",
    "g. There are cautionary tales hiding in this problem. Write a paragraph explaining what you can learn by playing with pathological matrices like the Hilbert Matrix.\n",
    "\n",
    "Exercise 4.85. Now that you have $Q R$ and $L U$ code we're going to use both of them! The problem is as follows:\n",
    "\n",
    "We are going to find the polynomial of degree 4 that best fits the function $\\$$\n",
    "\n",
    "$$\n",
    "y=\\cos (4 t)+0.1 \\varepsilon(t)\n",
    "$$\n",
    "\n",
    "at 50 equally spaced points $t$ between 0 and 1 . Here we are using $\\varepsilon(t)$ as a function that outputs normally distributed random white noise. In Python you will build $y$ as $y=n p . \\cos (4 * \\mathrm{t})+0.1 * n p . r a n d o m . r a n d n(t . s h a p e[0])$\n",
    "\n",
    "Build the $t$ vector and the $y$ vector (these are your data). We need to set up the least squares problems $A \\boldsymbol{x}=\\boldsymbol{b}$ by setting up the matrix $A$ as we did in the other least squares curve fitting problems and by setting up the $\\boldsymbol{b}$ vector using the $y$ data you just built. Solve the problem of finding the coefficients of the best degree 4 polynomial that fits this data. Report the sum of squared error and show a plot of the data along with the best fit curve.\n",
    "\n",
    "Exercise 4.86. Find the largest eigenvalue and the associated eigenvector of the\n",
    "matrix $A$ WITHOUT using np.linalg.eig(). (Don't do this by hand either)\n",
    "\n",
    "$$\n",
    "A=\\left(\\begin{array}{llll}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 0 & 1 & 2 \\\\\n",
    "3 & 4 & 5 & 6\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Exercise 4.87. It is possible in a matrix that the eigenvalues $\\lambda_{1}$ and $\\lambda_{2}$ are equal but with the corresponding eigenvectors not equal. Before you experiment with matrices of this sort, write a conjecture about what will happen to the power method in this case (look back to our proof in Exercise 4.60 of how the power method works). Now build several specific matrices where this is the case and see what happens to the power method.\n",
    "\n",
    "Exercise 4.88. Will the power method fail, slow down, or be uneffected if one (or more) of the non-dominant eigenvalues is zero? Give sufficient mathematical evidence or show several numerical experiments to support your answer.\n",
    "\n",
    "Exercise 4.89. Find a cubic function that best fits the following data. you can download the data directly with the code below.\n",
    "\n",
    "| $x$ Data | $y$ Data |\n",
    "| :--- | :--- |\n",
    "| 0 | 1.0220 |\n",
    "| 0.0500 | 1.0174 |\n",
    "| 0.1000 | 1.0428 |\n",
    "| 0.1500 | 1.0690 |\n",
    "| 0.2000 | 1.0505 |\n",
    "| 0.2500 | 1.0631 |\n",
    "| 0.3000 | 1.0458 |\n",
    "| 0.3500 | 1.0513 |\n",
    "| 0.4000 | 1.0199 |\n",
    "| 0.4500 | 1.0180 |\n",
    "| 0.5000 | 1.0156 |\n",
    "| 0.5500 | 0.9817 |\n",
    "| 0.6000 | 0.9652 |\n",
    "| 0.6500 | 0.9429 |\n",
    "| 0.7000 | 0.9393 |\n",
    "| 0.7500 | 0.9266 |\n",
    "| 0.8000 | 0.8959 |\n",
    "| 0.8500 | 0.9014 |\n",
    "| 0.9000 | 0.8990 |\n",
    "| 0.9500 | 0.9038 |\n",
    "| 1.0000 | 0.8989 |\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "URL1 = 'https://raw.githubusercontent.com/NumericalMethodsSullivan'\n",
    "URL2 = '/NumericalMethodsSullivan.github.io/master/data/'\n",
    "URL = URL1+URL2\n",
    "data = np.array( pd.read_csv(URL+'Exercise4_89.csv') )\n",
    "# Exercise4_89.csv\n",
    "```\n",
    "\n",
    "Theorem 4.6. If $A$ is a symmetric matrix with eigenvalues $\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{n}$ then $\\left|\\lambda_{1}\\right|>\\left|\\lambda_{2}\\right|>\\cdots>\\left|\\lambda_{n}\\right|$. Furthermore, the eigenvectors will be orthogonal to each other.\n",
    "\n",
    "Exercise 4.90. (The Deflation Method) For symmetric matrices we can build an extension to the power method in order to find the second most dominant eigen-pair for a matrix $A$. Theorem 4.6 suggests the following method for finding the second dominant eigen-pair for a symmetric matrix. This method is called the deflation method.\n",
    "\n",
    "- Use the power method to find the dominant eigenvalue and eigenvector.\n",
    "- Start with a random unit vector of the correct shape.\n",
    "- Multiplying your vector by $A$ will pull it toward the dominant eigenvector. After you multiply, project your vector onto the dominant eigenvector and find the projection error.\n",
    "- Use the projection error as the new approximation for the eigenvector (Why should we do this? What are we really finding here?)\n",
    "\n",
    "Note that the deflation method is really exactly the same as the power method with the exception that we orthogonalize at every step. Hence, when you write your code expect to only change a few lines from your power method.\n",
    "\n",
    "Write a function to find the second largest eigenvalue and eigenvector pair by putting the deflation method into practice. Test your code on a matrix $A$ and compare against Python's np.linalg.eig() command. Your code needs to work on symmetric matrices of arbitrary size and you need to write test code that clearly shows the error between your calculated eigenvalue and Python's eigenvalue as well as your calculated eigenvector and 's eigenvector.\n",
    "\n",
    "To guarantee that you start with a symmetric matrix you can use the following code.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "N = 40\n",
    "A = np.random.randn(N,N)\n",
    "A = np.matrix(A)\n",
    "A = np.transpose(A) * A # why should this build a symmetric matrix\n",
    "```\n",
    "\n",
    "Exercise 4.91. (This concept for this problem is modified from [6]. The data is taken from NOAA and the National Weather Service with the specific values associated with La Crosse, WI.)\n",
    "\n",
    "Floods in the Mississippi River Valleys of the upper midwest have somewhat predictable day-to-day behavior in that the flood stage today has high predictive power for the flood stage tomorrow. Assume that the flood stages are:\n",
    "\n",
    "- Stage 0 (Normal): Average daily flow is below $90,000 \\mathrm{ft}^{3} / \\mathrm{sec}$ (cubic feet per second $=$ cfs). This is the normal river level.\n",
    "- Stage 1 (Action Level): Average daily flow is between 90,000 cfs and 124,000 cfs.\n",
    "- Stage 2 (Minor Flood): Average daily flow is between $124,000 \\mathrm{cfs}$ and 146,000 cfs.\n",
    "- Stage 3 (Moderate Flood): Average daily flow is between $146,000 \\mathrm{cfs}$ and 170,000 cfs.\n",
    "- Stage 4 (Extreme Flood): Average daily flow is above 170,000 cfs.\n",
    "\n",
    "The following table shows the probability of one stage transitioning into another stage from one day to the next.\n",
    "\n",
    "|  | 0 Today | 1 Today | 2 Today | 3 Today | 4 Today |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| 0 Tomorrow | 0.9 | 0.3 | 0 | 0 | 0 |\n",
    "| 1 Tomorrow | 0.05 | 0.7 | 0.4 | 0 | 0 |\n",
    "| 2 Tomorrow | 0.025 | 0 | 0.6 | 0.6 | 0 |\n",
    "| 3 Tomorrow | 0.015 | 0 | 0 | 0.4 | 0.8 |\n",
    "| 4 Tomorrow | 0.01 | 0 | 0 | 0 | 0.2 |\n",
    "\n",
    "Mathematically, if $s_{k}$ is the state at day $k$ and $A$ is the matrix given in the table above then the difference equation $s_{k+1}=A s_{k}$ shows how a state will transition from day to day. For example, if we are currently in Stage 0 then\n",
    "\n",
    "$$\n",
    "s_{0}=\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "We can interpret this as \"there is a probability of 1 that we are in Stage 0 today and there is a probability of 0 that we are in any other stage today.\"\n",
    "\n",
    "If we want to advance this model forward in time then we just need to iterate. In our example, the state tomorrow would be $s_{1}=A s_{0}$. The state two days from now would be $s_{2}=A s_{1}$, and if we use the expression for $s_{1}$ we can simplify to $s_{2}=A^{2} s_{0}$.\n",
    "a. Prove that the state at day $n$ is $\\boldsymbol{s}_{n}=A^{n} \\boldsymbol{s}_{0}$.\n",
    "b. If $n$ is large then the steady state solution to the difference equation in part (a) is given exactly by the power method iteration that we have studied in this chapter. Hence, as the iterations proceed they will be pulled toward the dominant eigenvector. Use the power method to find the dominant eigenvector of the matrix $A$.\n",
    "c. The vectors in this problem are called probability vectors in the sense that the vectors sum to 1 and every entry can be interpreted as a probability. Re-scale your answer from part (b) so that we can interpret the entries as probabilities. That is, ensure that the sum of the vector from part (b) is 1.\n",
    "d. Interpret your answer to part (c) in the context of the problem. Be sure that your interpretation could be well understood by someone that does not know the mathematics that you just did.\n",
    "\n",
    "Exercise 4.92. The $L U$ factorization as we have built it in this chapter is not smart about the way that it uses the memory on your computer. In the $L U$ factorization the 1's on the main diagonal don't actually need to be stored since we know that they will always be there. The zeros in the lower triangle of $U$ don't need to be stored either. If you store the upper triangle values in the $U$ matrix on top of the upper triangle of the $L$ matrix then we still store a full matrix for $L$ which contains both $L$ and $U$ simultaneously, but we don't have to store $U$ separately and hence save computer memory. The modifications to the existing code for an $L U$ solve is minimal - every time you call on an entry of the $U$ matrix it is stored in the upper triangle of $L$ instead. Write code to implement this new data storage idea and demonstrate your code on a few examples.\n",
    "\n",
    "Exercise 4.93. In the algorithm that we used to build the $Q R$ factorization we built the $R$ matrix as $R=Q^{T} A$ The trouble with this step is that it fills in a lot of redundant zeros into the $R$ matrix - some of which may not be exactly zero. First explain why this will be the case. Then rewrite your $Q R$ factorization code so that the top triangle of $R$ is filled with all of the projections (do this with a double for loop). Demonstrate that your code works on a few examples.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
