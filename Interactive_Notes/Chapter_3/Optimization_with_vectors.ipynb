{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to minimize $f(x,y) = (x-1)^2 + (y-2)^2$ using gradient descent.  We might set up a two variable version of gradient descent like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_descent(f, fx, fy, x0, y0, alpha, tol=1e-6, ftol=1e-6):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm for minimizing a function f(x, y).\n",
    "    \n",
    "    Parameters:\n",
    "    f: callable\n",
    "        The function to minimize.\n",
    "    fx: callable\n",
    "        Partial derivative of f with respect to x.\n",
    "    fy: callable\n",
    "        Partial derivative of f with respect to y.\n",
    "    x0: float\n",
    "        Initial value of x.\n",
    "    y0: float\n",
    "        Initial value of y.\n",
    "    alpha: float\n",
    "        Learning rate (step size).\n",
    "    tol: float\n",
    "        Tolerance for the gradient norm to stop the algorithm.\n",
    "    ftol: float\n",
    "        Tolerance for the change in function value to stop the algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    x, y: float\n",
    "        The coordinates of the minimum point.\n",
    "    fval: float\n",
    "        The value of the function at the minimum point.\n",
    "    \"\"\"\n",
    "    # Initialize x and y with the starting values x0 and y0\n",
    "    x = x0\n",
    "    y = y0\n",
    "    # Compute the initial value of the function\n",
    "    fval = f(x, y)\n",
    "    \n",
    "    while True:\n",
    "        # Compute the partial derivatives (gradients) at the current point\n",
    "        dfx = fx(x, y)\n",
    "        dfy = fy(x, y)\n",
    "        # Compute the norm of the gradient vector\n",
    "        norm = np.sqrt(dfx**2 + dfy**2)\n",
    "        \n",
    "        # If the gradient norm is smaller than the tolerance, stop the algorithm\n",
    "        if norm < tol:\n",
    "            break\n",
    "        \n",
    "        # Update x and y by moving in the direction opposite to the gradient\n",
    "        x -= alpha * dfx\n",
    "        y -= alpha * dfy\n",
    "        \n",
    "        # Compute the new value of the function after the update\n",
    "        fnew = f(x, y)\n",
    "        \n",
    "        # If the change in function value is smaller than the tolerance, stop the algorithm\n",
    "        if abs(fnew - fval) < ftol:\n",
    "            break\n",
    "        \n",
    "        # Update the function value for the next iteration\n",
    "        fval = fnew\n",
    "    \n",
    "    # Return the coordinates of the minimum point and the function value at that point\n",
    "    return x, y, fval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum at x = 0.9994929397599087, y = 1.9989858795198174, f(x,y) = 2.0086725553238508e-06\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: (x - 1)**2 + (y - 2)**2\n",
    "fx = lambda x, y: 2 * (x - 1)\n",
    "fy = lambda x, y: 2 * (y - 2)\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "alpha = 0.1\n",
    "x, y, fval = grad_descent(f, fx, fy, x0, y0, alpha)\n",
    "print(f\"Minimum at x = {x}, y = {y}, f(x,y) = {fval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine, but it's going to get to be a mess if we want to extend to input variables.  To make it easier to adapt to higher dimensions we'll think of our function as having a vector, $\\mathbf{x} = \\langle x_1, x_2 \\rangle$, as input.  So our function becomes $$f(\\mathbf{x}) = (x_1-1)^2 + (x_2-2)^2.$$\n",
    "\n",
    "If we want to take it a step further we could vectorize the output too.  Let $\\mathbf{x}_0 = \\langle 1, 2 \\rangle$ then we can rewrite $$f(\\mathbf{x}) = (\\mathbf{x}-\\mathbf{x}_0) \\cdot (\\mathbf{x}-\\mathbf{x}_0)$$ using the dot product.  Using vectors to represent the output takes some adjustment and practice so we'll stick to vectorizing the input for now.\n",
    "\n",
    "We can write our new version of $f$ in Python like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value at x = [0 0]: 5\n",
      "Gradient at x = [0 0]: [-2 -4]\n"
     ]
    }
   ],
   "source": [
    "def f_vec(x):\n",
    "    return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "def grad_f_vec(x):\n",
    "    return np.array([2 * (x[0] - 1), 2 * (x[1] - 2)])\n",
    "\n",
    "x_in = np.array([0, 0])\n",
    "\n",
    "print(f\"Function value at x = {x_in}: {f_vec(x_in)}\")\n",
    "print(f\"Gradient at x = {x_in}: {grad_f_vec(x_in)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the vectorized version of our function and its gradient we can write gradient descent like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_vec(f, gradf, x0, alpha, tol=1e-6, ftol=1e-6):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm for minimizing a function f(x).\n",
    "    \n",
    "    Parameters:\n",
    "    f: callable\n",
    "        The function to minimize.\n",
    "    gradf: callable\n",
    "        The gradient of the function f.\n",
    "    x0: numpy.ndarray\n",
    "        Initial guess for the minimum point.\n",
    "    alpha: float\n",
    "        Learning rate (step size).\n",
    "    tol: float\n",
    "        Tolerance for the gradient norm to stop the algorithm.\n",
    "    ftol: float\n",
    "        Tolerance for the change in function value to stop the algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    x: numpy.ndarray\n",
    "        The coordinates of the minimum point.\n",
    "    fval: float\n",
    "        The value of the function at the minimum point.\n",
    "    \"\"\"\n",
    "    # Initialize x with the starting value x0\n",
    "    x = x0\n",
    "    # Compute the initial value of the function\n",
    "    fval = f(x)\n",
    "    \n",
    "    while True:\n",
    "        # Compute the gradient of the function at the current point\n",
    "        dfx = gradf(x)\n",
    "        # Compute the norm (magnitude) of the gradient vector\n",
    "        norm = np.linalg.norm(dfx)\n",
    "        \n",
    "        # If the gradient norm is smaller than the tolerance, stop the algorithm\n",
    "        if norm < tol:\n",
    "            break\n",
    "        \n",
    "        # Update x by moving in the direction opposite to the gradient\n",
    "        # The step size is determined by the learning rate alpha\n",
    "        x -= alpha * dfx\n",
    "        \n",
    "        # Compute the new value of the function after the update\n",
    "        fnew = f(x)\n",
    "        \n",
    "        # If the change in function value is smaller than the tolerance, stop the algorithm\n",
    "        if abs(fnew - fval) < ftol:\n",
    "            break\n",
    "        \n",
    "        # Update the function value for the next iteration\n",
    "        fval = fnew\n",
    "    \n",
    "    # Return the coordinates of the minimum point and the function value at that point\n",
    "    return x, fval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've made a tradeoff.  This new approach is more abstract, but it's also completely general.  We could use this same code to minimize a function with ANY number of input variables from one to thousands!\n",
    "\n",
    "Let's see it action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum at x = [0.99949294 1.99898588], f(x) = 2.0086725553238508e-06\n"
     ]
    }
   ],
   "source": [
    "def f_vec(x):\n",
    "    return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "def grad_f_vec(x):\n",
    "    return np.array([2 * (x[0] - 1), 2 * (x[1] - 2)])\n",
    "\n",
    "x_start = np.array([0., 0.])\n",
    "\n",
    "xmin, fmin = grad_descent_vec(f_vec, grad_f_vec, x_start, alpha=0.1)\n",
    "print(f\"Minimum at x = {xmin}, f(x) = {fmin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo in Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_minimize_iterative(f, lower_bounds, upper_bounds, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo minimization of a function f by generating one new point per iteration.\n",
    "\n",
    "    Parameters:\n",
    "    f: callable\n",
    "        The function to minimize. It should take a numpy array as input.\n",
    "    lower_bounds: numpy.ndarray\n",
    "        An array specifying the lower bounds for each dimension.\n",
    "    upper_bounds: numpy.ndarray\n",
    "        An array specifying the upper bounds for each dimension.\n",
    "    max_iterations: int\n",
    "        The maximum number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "    xmin: numpy.ndarray\n",
    "        The coordinates of the point where f is minimized.\n",
    "    fmin: float\n",
    "        The minimum value of the function.\n",
    "    \"\"\"\n",
    "    # Initialize variables to track the minimum\n",
    "    fmin = float('inf')\n",
    "    xmin = None\n",
    "    \n",
    "    # Perform iterative sampling\n",
    "    for _ in range(max_iterations):\n",
    "        # Generate a single random sample within the bounds\n",
    "        sample = np.random.uniform(lower_bounds, upper_bounds)\n",
    "        \n",
    "        # Evaluate f at the sample point\n",
    "        fval = f(sample)\n",
    "        if fval < fmin:\n",
    "            fmin = fval\n",
    "            xmin = sample\n",
    "    \n",
    "    return xmin, fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's minimize our function above in the domain $-1 \\leq x \\leq 3$ a $0 \\leq x \\leq 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum at x = [0.99548816 1.99658483], f(x) = 3.20200629351652e-05\n"
     ]
    }
   ],
   "source": [
    "def f_vec(x):\n",
    "    return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "lower_bounds = np.array([-1,0])\n",
    "upper_bounds = np.array([3,4])\n",
    "xmin, fmin = monte_carlo_minimize_iterative(f_vec, lower_bounds, upper_bounds)\n",
    "print(f\"Minimum at x = {xmin}, f(x) = {fmin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n"
     ]
    }
   ],
   "source": [
    "x_every_other = x[::2]\n",
    "print(x_every_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  5  7  9 11 13 15 17 19]\n"
     ]
    }
   ],
   "source": [
    "x_every_other_from_second = x[1::2]\n",
    "print(x_every_other_from_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
