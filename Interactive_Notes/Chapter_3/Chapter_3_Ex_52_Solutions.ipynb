{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.52. \n",
    "\n",
    "Write code to implement the 1D derivative free optimization algorithm and use it to solve Exercise 3.48. Compare your answer to the analytic solution.\n",
    "\n",
    "Note: For 3.48 we need to find $0<x<10$ that minimizes $V(x) = x (20-2x)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_free_min(f, x0, dx):\n",
    "    \"\"\"\n",
    "    Find the approximate minimum of a function using derivative-free optimization.\n",
    "    This function performs a simple optimization by evaluating the function\n",
    "    at points to the left and right of the current position and moving in the\n",
    "    direction that decreases the function value. It stops when no further\n",
    "    improvement is found.\n",
    "    Parameters:\n",
    "    f (callable): The function to minimize. It should take a single argument (x) and return a scalar value.\n",
    "    x0 (float): The initial guess for the location of the minimum.\n",
    "    dx (float): The step size used to explore the function to the left and right of the current position.\n",
    "    Returns:\n",
    "    tuple: A tuple (current_x, current_f) where:\n",
    "        - current_x (float): The x-coordinate of the approximate minimum.\n",
    "        - current_f (float): The function value at the approximate minimum.\n",
    "\n",
    "    Notes:\n",
    "    - This method does not use derivatives, so it is suitable for functions\n",
    "        that are not differentiable or when derivatives are difficult to compute.\n",
    "    - The choice of `dx` affects the accuracy and speed of convergence.\n",
    "    - This method assumes the function has a single valley in the region of interest.\n",
    "    \"\"\"\n",
    "    current_x, current_f = x0, f(x0) # initial position and function value\n",
    "    fevals = 1\n",
    "    \n",
    "    while True:\n",
    "        # Calculate the new positions and function values\n",
    "        left_x, right_x = current_x - dx, current_x + dx\n",
    "        left_f, right_f = f(left_x), f(right_x)\n",
    "        fevals += 2\n",
    "        \n",
    "        if left_f < min(current_f, right_f): # f is smaller to the left so \"slide\" left\n",
    "            current_x, current_f = left_x, left_f\n",
    "        elif right_f < min(current_f, left_f): # f is smaller to the right so \"slide\" right\n",
    "            current_x, current_f = right_x, right_f\n",
    "        else: # neither left nor right is better, so we're done\n",
    "            break\n",
    "\n",
    "    print(f\"Approximate min at x = {current_x:.4f} with f = {current_f:.4f} after {fevals} evaluations.\")\n",
    "    \n",
    "    return current_x, current_f # return current x and f for the approximate minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate min at x = 3.3000 with f = -592.5480 after 29 evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Note we're flipping the sign of the function to find the maximum\n",
    "V = lambda x: -x*(20-2*x)**2\n",
    "x0 = 2\n",
    "dx = 0.1\n",
    "x, Vx = deriv_free_min(V, x0, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_free_min_v2(f, x0, dx):\n",
    "    \"\"\"\n",
    "    Find the approximate minimum of a function using derivative-free optimization.\n",
    "    This function performs a simple optimization by evaluating the function\n",
    "    at points to the left and right of the current position and moving in the\n",
    "    direction that decreases the function value. It stops when no further\n",
    "    improvement is found.\n",
    "    Parameters:\n",
    "    f (callable): The function to minimize. It should take a single argument (x) and return a scalar value.\n",
    "    x0 (float): The initial guess for the location of the minimum.\n",
    "    dx (float): The step size used to explore the function to the left and right of the current position.\n",
    "    Returns:\n",
    "    tuple: A tuple (current_x, current_f) where:\n",
    "        - current_x (float): The x-coordinate of the approximate minimum.\n",
    "        - current_f (float): The function value at the approximate minimum.\n",
    "\n",
    "    Notes:\n",
    "    - This method does not use derivatives, so it is suitable for functions\n",
    "        that are not differentiable or when derivatives are difficult to compute.\n",
    "    - The choice of `dx` affects the accuracy and speed of convergence.\n",
    "    - This method assumes the function has a single valley in the region of interest.\n",
    "    \"\"\"\n",
    "\n",
    "    left_x, current_x, right_x = x0 - dx, x0, x0 + dx # initial positions\n",
    "    left_f, current_f, right_f = f(left_x), f(current_x), f(right_x) # initial function values \n",
    "    fevals = 3\n",
    "    \n",
    "    while True:\n",
    "        if left_f < min(current_f, right_f): # f is smaller to the left so \"slide\" left\n",
    "            current_x, current_f, right_x, right_f = left_x, left_f, current_x, current_f\n",
    "            left_x = current_x - dx\n",
    "            left_f = f(left_x)\n",
    "            fevals += 1\n",
    "        elif right_f < min(current_f, left_f): # f is smaller to the right so \"slide\" right\n",
    "            current_x, current_f, left_x, left_f = right_x, right_f, current_x, current_f\n",
    "            right_x = current_x + dx\n",
    "            right_f = f(right_x)\n",
    "            fevals += 1\n",
    "        else: # neither left nor right is better, so we're done\n",
    "            break\n",
    "    \n",
    "    print(f\"Approximate min at x = {current_x:.4f} with f = {current_f:.4f} after {fevals} evaluations.\")\n",
    "    \n",
    "    return current_x, current_f # return current x and f for the approximate minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate min at x = 3.3000 with f = -592.5480 after 16 evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Note we're flipping the sign of the function to find the maximum\n",
    "V = lambda x: -x*(20-2*x)**2\n",
    "x0 = 2\n",
    "dx = 0.1\n",
    "x, Vx = deriv_free_min_v2(V, x0, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_free_min_adapt_dx(f, x0, dx, dx_min=1e-6):\n",
    "    \"\"\"\n",
    "    Find the approximate minimum of a function using derivative-free optimization with adaptive step size.\n",
    "    \n",
    "    This method evaluates the function at points to the left and right of the current position \n",
    "    and moves in the direction that decreases the function value. If no improvement is found, \n",
    "    the step size is halved until it reaches a minimum threshold (dx_min).\n",
    "    \n",
    "    Parameters:\n",
    "    f (callable): The function to minimize. It should take a single argument (x) and return a scalar value.\n",
    "    x0 (float): The initial guess for the location of the minimum.\n",
    "    dx (float): The initial step size used to explore the function to the left and right of the current position.\n",
    "    dx_min (float): The minimum allowable step size. Default is 1e-6.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple (current_x, current_f) where:\n",
    "        - current_x (float): The x-coordinate of the approximate minimum.\n",
    "        - current_f (float): The function value at the approximate minimum.\n",
    "    \n",
    "    Notes:\n",
    "    - This method does not use derivatives, so it is suitable for functions that are not differentiable \n",
    "      or when derivatives are difficult to compute.\n",
    "    - The choice of `dx` affects the accuracy and speed of convergence.\n",
    "    - This method assumes the function has a single valley in the region of interest.\n",
    "    \"\"\"\n",
    "\n",
    "    left_x, current_x, right_x = x0 - dx, x0, x0 + dx # initial positions\n",
    "    left_f, current_f, right_f = f(left_x), f(current_x), f(right_x) # initial function values \n",
    "    fevals = 3\n",
    "    \n",
    "    while True:\n",
    "        if left_f < min(current_f, right_f): # f is smaller to the left so \"slide\" left\n",
    "            current_x, current_f, right_x, right_f = left_x, left_f, current_x, current_f\n",
    "            left_x = current_x - dx\n",
    "            left_f = f(left_x)\n",
    "            fevals += 1\n",
    "        elif right_f < min(current_f, left_f): # f is smaller to the right so \"slide\" right\n",
    "            current_x, current_f, left_x, left_f = right_x, right_f, current_x, current_f\n",
    "            right_x = current_x + dx\n",
    "            right_f = f(right_x)\n",
    "            fevals += 1\n",
    "        else:\n",
    "            if dx < dx_min:  # Stop if the step size is below the minimum threshold\n",
    "                break\n",
    "            dx = dx / 2  # reduce the step size by half\n",
    "            left_x, right_x = current_x - dx, current_x + dx # update the new positions\n",
    "            left_f, right_f = f(left_x), f(right_x) # update the new function values\n",
    "            fevals += 2\n",
    "    \n",
    "    print(f\"Approximate min at x = {current_x:.4f} with f = {current_f:.4f} after {fevals} evaluations.\")\n",
    "\n",
    "    return current_x, current_f # return current x and f for the approximate minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_free_min_adapt_dx_v2(f, x0, dx, dx_min=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Find the approximate minimum of a function using derivative-free optimization with adaptive step size and iteration limit.\n",
    "    \n",
    "    This method evaluates the function at points to the left and right of the current position \n",
    "    and moves in the direction that decreases the function value. If no improvement is found, \n",
    "    the step size is halved until it reaches a minimum threshold (dx_min) or the maximum number of iterations is reached.\n",
    "    \n",
    "    Parameters:\n",
    "    f (callable): The function to minimize. It should take a single argument (x) and return a scalar value.\n",
    "    x0 (float): The initial guess for the location of the minimum.\n",
    "    dx (float): The initial step size used to explore the function to the left and right of the current position.\n",
    "    dx_min (float): The minimum allowable step size. Default is 1e-6.\n",
    "    max_iter (int): The maximum number of iterations. Default is 1000.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple (current_x, current_f) where:\n",
    "        - current_x (float): The x-coordinate of the approximate minimum.\n",
    "        - current_f (float): The function value at the approximate minimum.\n",
    "    \n",
    "    Notes:\n",
    "    - This method does not use derivatives, so it is suitable for functions that are not differentiable \n",
    "      or when derivatives are difficult to compute.\n",
    "    - The choice of `dx` affects the accuracy and speed of convergence.\n",
    "    - This method assumes the function has a single valley in the region of interest.\n",
    "    \"\"\"\n",
    "\n",
    "    left_x, current_x, right_x = x0 - dx, x0, x0 + dx # initial positions\n",
    "    left_f, current_f, right_f = f(left_x), f(current_x), f(right_x) # initial function values \n",
    "    fevals = 3\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        if left_f < min(current_f, right_f): # f is smaller to the left so \"slide\" left\n",
    "            current_x, current_f, right_x, right_f = left_x, left_f, current_x, current_f\n",
    "            left_x = current_x - dx\n",
    "            left_f = f(left_x)\n",
    "            fevals += 1\n",
    "        elif right_f < min(current_f, left_f): # f is smaller to the right so \"slide\" right\n",
    "            current_x, current_f, left_x, left_f = right_x, right_f, current_x, current_f\n",
    "            right_x = current_x + dx\n",
    "            right_f = f(right_x)\n",
    "            fevals += 1\n",
    "        else:\n",
    "            if dx < dx_min:  # Stop if the step size is below the minimum threshold\n",
    "                break\n",
    "            dx = dx / 2  # reduce the step size by half\n",
    "            left_x, right_x = current_x - dx, current_x + dx # update the new positions\n",
    "            left_f, right_f = f(left_x), f(right_x) # update the new function values\n",
    "            fevals += 2\n",
    "    \n",
    "    print(f\"Approximate min at x = {current_x:.4f} with f = {current_f:.4f} after {fevals} evaluations and {iteration + 1} iterations.\")\n",
    "\n",
    "    return current_x, current_f # return current x and f for the approximate minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate min at x = 3.3333 with f = -592.5926 after 46 evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Note we're flipping the sign of the function to find the maximum\n",
    "V = lambda x: -x*(20-2*x)**2\n",
    "x0 = 2\n",
    "dx = 0.1\n",
    "x, Vx = deriv_free_min_adapt_dx(V, x0, dx, dx_min= 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, x0, learning_rate, xtol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to find the minimum of a function.\n",
    "\n",
    "    Parameters:\n",
    "    f (callable): The function to minimize. It should take a single argument (x) and return a scalar value.\n",
    "    df (callable): The derivative of the function. It should take a single argument (x) and return the derivative at that point.\n",
    "    x0 (float): The initial guess for the location of the minimum.\n",
    "    learning_rate (float): The step size for each iteration.\n",
    "    xtol (float): The tolerance for changes in x. Default is 1e-6.\n",
    "    max_iter (int): The maximum number of iterations. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple (x_min, f_min) where:\n",
    "        - x_min (float): The x-coordinate of the approximate minimum.\n",
    "        - f_min (float): The function value at the approximate minimum.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    f_val = f(x)\n",
    "    fevals = 1\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        grad = df(x)\n",
    "        x_new = x - learning_rate * grad\n",
    "        f_new = f(x_new)\n",
    "        fevals += 1\n",
    "\n",
    "        if abs(x_new - x) < xtol:\n",
    "            break\n",
    "\n",
    "        x, f_val = x_new, f_new\n",
    "\n",
    "    print(f\"Approximate min at x = {x:.4f} with f = {f_val:.4f} after {fevals} evaluations.\")\n",
    "    return x, f_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(x) = x(20-2x)^2$$\n",
    "$$V'(x) = x \\cdot 2(20-2x)(-2) + (20-2x)^2 = -4x(20-2x)+(20-2x)^2 = (20-2x)(20-6x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably have to experiment with the learning rate to get this to work.  If the learning rate is too large the method may diverge.  If the learning rate is too small the method will take a long time to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate min at x = 3.3333 with f = -592.5926 after 10 evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Note we're flipping the sign of the function to find the maximum\n",
    "V = lambda x: -x*(20-2*x)**2\n",
    "Vprime = lambda x: -(20-2*x)*(20-6*x)\n",
    "x0 = 3\n",
    "x, Vx = gradient_descent(V, Vprime, x0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
