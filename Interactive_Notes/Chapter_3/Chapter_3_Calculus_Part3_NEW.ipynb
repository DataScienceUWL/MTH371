{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Optimization\n",
    "\n",
    "### 3.4.1 Single Variable Optimization\n",
    "\n",
    "You likely recall that one of the major applications of Calculus was to solve optimization problems - find the value of $x$ which makes some function as big or as small as possible. The process itself can sometimes be rather challenging due to either the modeling aspect of the problems and/or the fact that the differentiation might be quite cumbersome. In this section we will revisit those problems from Calculus, but our goal will be to build a numerical method for the Calculus step in hopes to avoid the messy algebra and differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.48. \n",
    "\n",
    "A piece of cardboard measuring 20 cm by 20 cm is to be cut so that it can be folded into a box without a lid (see Figure 3.8). We want to find the size of the cut, $x$, that maximizes the volume of the box.\n",
    "\n",
    "1. Write a function for the volume of the box resulting from a cut of size $x$. What is the domain of your function?\n",
    "2. We know that we want to maximize this function so go through the full Calculus exercise to find the maximum:\n",
    "\n",
    "- take the derivative\n",
    "- set it to zero\n",
    "- find the critical points\n",
    "- test the critical points and the boundaries of the domain using the extreme value theorem to determine the $x$ that gives the maximum.\n",
    "\n",
    "**Figure 3.8: Folds to make a cardboard box**\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2025_02_27_429587f441ab5f434461g-31.jpg?height=443&width=484&top_left_y=1622&top_left_x=731)\n",
    "\n",
    "\n",
    "The hard part of the single variable optimization process is often solving the equation $f^{\\prime}(x)=0$. We could use numerical root finding schemes to solve this\n",
    "equation, but we could also potentially do better without actually finding the derivative. In the following we propose a few numerical techniques that can approximate the solution to these types of problems. The basic ideas are simple!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.49. \n",
    "\n",
    "If you were blind folded and standing on a hill could you find the top of the hill? (assume no trees and no cliffs . . . this isn't supposed to be dangerous) How would you do it? Explain your technique clearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.50. \n",
    "\n",
    "If you were blind folded and standing on a crater on the moon could you find the lowest point? How would you do it? Remember that you can hop as far as you like ... because gravity ... but sometimes that's not a great thing because you could hop too far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The intuition of numerical optimization schemes is typically to visualize the function that you're trying to minimize or maximize and think about either climbing the hill to the top (maximization) or descending the hill to the bottom (minimization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.51. \n",
    "\n",
    "Let's turn your intuitions into algorithms. If $f(x)$ is the function that you are trying to maximize then turn your ideas from the previous problems into step-by-step algorithms which could be coded. Then try out your codes on the function\n",
    "\n",
    "$$\n",
    "f(x)=e^{-x^{2}}+\\sin \\left(x^{2}\\right)\n",
    "$$\n",
    "\n",
    "to see if your algorithms can find the local maximum near $x \\approx 1.14$. Try to generate several different algorithms.\n",
    "\n",
    "Some of the most common algorithms are listed below. Read through them and see which one(s) you ended up recreating? The intuition for these algorithms is pretty darn simple - travel uphill if you want to maximize - travel downhill if you want to minimize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Definition 3.5. (Derivative Free Optimization) \n",
    "\n",
    "Let $f(x)$ be the objective function which you are seeking to maximize (or minimize).\n",
    "\n",
    "- Pick a starting point, $x_{0}$, and find the value of your objective function at this point, $f\\left(x_{0}\\right)$.\n",
    "- Pick a small step size (say, $\\Delta x \\approx 0.01$ ).\n",
    "- Calculate the objective function one step to the left and one step to the right from your starting point. Which ever point is larger (if you're seeking a maximum) is the point that you keep for your next step.\n",
    "- Iterate (decide on a good stopping rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.52. \n",
    "\n",
    "Write code to implement the 1D derivative free optimization algorithm and use it to solve Exercise 3.48. Compare your answer to the analytic solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Definition 3.6. (Gradient Descent/Ascent) \n",
    "\n",
    "Let $f(x)$ be the objective function which you are seeking to maximize (or minimize).\n",
    "\n",
    "- Find the derivative of your objective function, $f^{\\prime}(x)$.\n",
    "- Pick a starting point, $x_{0}$.\n",
    "- Pick a small control parameter, $\\alpha$ (in machine learning this parameter is called the \"learning rate\" for the gradient descent algorithm).\n",
    "- Use the iteration $x_{n+1}=x_{n}+\\alpha f^{\\prime}\\left(x_{n}\\right)$ if you're maximizing. Use the iteration $x_{n+1}=x_{n}-\\alpha f^{\\prime}\\left(x_{n}\\right)$ if you're minimizing.\n",
    "- Iterate (decide on a good stopping rule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.53. \n",
    "\n",
    "Write code to implement the 1D gradient descent algorithm and use it to solve Exercise 3.48. Compare your answer to the analytic solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Definition 3.7. (Monte-Carlo Search) \n",
    "\n",
    "Let $f(x)$ be the objective function which you are seeking to maximize (or minimize).\n",
    "\n",
    "- Pick many (perhaps several thousand!) different $x$ values.\n",
    "- Find the value of the objective function at every one of these points (Hint: use lists, not loops)\n",
    "- Keep the $x$ value that has the largest (or smallest if you're minimizing) value of the objective function.\n",
    "- Iterate many times and compare the function value in each iteration to the previous best function value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.54. \n",
    "\n",
    "Write code to implement the 1D monte carlo search algorithm and use it to solve Exercise 3.48. Compare your answer to the analytic solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Definition 3.8. (Optimization via Numerical Root Finding) \n",
    "\n",
    "Let $f(x)$ be the objective function which you are seeking to maximize (or minimize).\n",
    "\n",
    "- Find the derivative of your objective function.\n",
    "- Set the derivative to zero and use a numerical root finding method (such as bisection or Newton) to find the critical point.\n",
    "- Use the extreme value theorem to determine if the critical point or one of the endpoints is the maximum (or minimum).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.55. \n",
    "\n",
    "Write code to implement the 1D numerical root finding optimization algorithm and use it to solve Exercise 3.48. Compare your answer to the analytic solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.56. \n",
    "\n",
    "In this problem we will compare an contrast the four methods proposed in the previous problem.\n",
    "\n",
    "1. What are the advantages to each of the methods proposed?\n",
    "\n",
    "2. What are the disadvantages to each of the methods proposed?\n",
    "\n",
    "3. Which method, do you suppose, will be faster in general? Why?\n",
    "\n",
    "4. Which method, do you suppose, will be slower in general? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.57. \n",
    "\n",
    "The Gradient Ascent/Descent algorithm is the most geometrically interesting of the four that we have proposed. The others are pretty brute force algorithms. What is the Gradient Ascent/Descent algorithm doing geometrically? Draw a picture and be prepared to explain to your peers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.58.\n",
    "\n",
    "A pig weighs 200 pounds and gains weight at a rate proportional to its current weight. Today the growth rate if 5 pounds per day. The pig costs 45 cents per day to keep due mostly to the price of food. The market price for pigs if 65 cents per pound but is falling at a rate of 1 cent per day. When should the pig be sold and how much profit do you make on the pig when you sell it? Write this situation as a single variable mathematical model and solve the problem analytically (by hand). Then solve the problem with all four methods outlined thus far in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.59.\n",
    "\n",
    "Reconsider the pig problem 3.58 but now suppose that the weight of the pig after $t$ days is\n",
    "\n",
    "$$\n",
    "w=\\frac{800}{1+3 e^{-t / 30}} \\text { pounds. }\n",
    "$$\n",
    "\n",
    "When should the pig be sold and how much profit do you make on the pig when you sell it? Write this situation as a single variable mathematical model. You\n",
    "should notice that the algebra and calculus for solving this problem is no longer really a desirable way to go. Use an appropriate numerical technique to solve this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.60. \n",
    "\n",
    "Numerical optimization is often seen as quite challenging since the algorithms that we have introduced here could all get \"stuck\" at local extrema. To illustrate this see the function shown in Figure 3.9. How will derivative free optimization methods have trouble finding the red point starting at the black point with this function? How will gradient descent/ascent methods have trouble? Why?\n",
    "\n",
    "**Figure 3.9: A challenging numerical optimization problem.**\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2025_02_27_429587f441ab5f434461g-35.jpg?height=475&width=597&top_left_y=931&top_left_x=669)\n",
    "\n",
    "If we start at the black point then how will any of our algorithms find the local minimum at the red point?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 3.4.2 Multivariable Optimization\n",
    "\n",
    "Now let's look at multivariable optimization. The analytic process for finding optimal solutions is essentially the same as for single variable.\n",
    "\n",
    "- Write a function that models a scenario in multiple variables,\n",
    "- find the gradient vector (presuming that the function is differentiable),\n",
    "- set the gradient vector equal to the zero vector and solve for the critical point(s), and\n",
    "- interpret your answer in the context of the problem.\n",
    "\n",
    "The trouble with unconstrained multivariable optimization is that finding the critical points is now equivalent to solving a system of nonlinear equations; a task that is likely impossible even with a computer algebra system.\n",
    "\n",
    "Let's see if you can extend your intuition from single variable to multivariable. This particular subsection is intentionally quite brief. If you want more details on multivariable optimization it would be wise to take a full course in optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.61. \n",
    "\n",
    "The derivative free optimization method discussed in the single variable optimization section just said that you should pick two points and pick the one that takes you furthest uphill.\n",
    "\n",
    "1. Why is it insufficient to choose just two points if we are dealing with a function of two variables? Hint: think about contour line.\n",
    "\n",
    "2. For a function of two variables, how many points should you use to compare and determine the direction of \"uphill?\"\n",
    "\n",
    "3. Extend your answer from part (b) to $n$ dimensions. How many points should we compare if we are in $n$ dimensions and need to determine which direction is \"uphill?\"\n",
    "\n",
    "4. Back in the case of a two-variable function, you should have decided that three points was best. Explain an algorithm for moving one point at a time so that your three points eventually converge to a nearby local maximum. It may be helpful to make a surface plot or a contour plot of a well-known function just as a visual.\n",
    "\n",
    "The code below will demonstrate how to make a contour plot.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "xdomain = np.linspace(-4,4,100)\n",
    "ydomain = np.linspace(-4,4,100)\n",
    "X, Y = np.meshgrid(xdomain,ydomain)\n",
    "f = lambda x, y: np.sin(x)*np.exp(-np.sqrt (x**2+y**2))\n",
    "plt.contour(X,Y,f(X,Y))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.62. \n",
    "\n",
    "Now let's tackle the gradient ascent/descent algorithm. You should recall that the gradient vector points in the direction of maximum change. How can you use this fact to modify the gradient ascent/descent algorithm given previously? Clearly write your algorithm so that a classmate could turn it into code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.63. \n",
    "\n",
    "How does the Monte Carlo algorithm extend to a two-variable optimization problem? Clearly write your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.64. \n",
    "\n",
    "Try out the gradient descent/ascent and Monte Carlo algorithms on the function $f(x, y)=\\sin (x) \\cos (y)+0.1 x^{2}$ which has many local extrema and no global maximum. We are not going to code the multidimensional derivative free optimization routine in this section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The derivative free, gradient ascent/descent, and monte carlo techniques still have good analogues in higher dimensions. We just need to be a bit careful since in higher dimensions there is much more room to move. Below we'll give the full description of the gradient ascent/descent algorithm. We don't give the full description of the derivative free or Monte Carlo algorithms since there are many ways to implement them. The interested reader should see a course in mathematical optimization or machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Definition 3.9. (The Gradient Descent Algorithm) \n",
    "\n",
    "We want to solve the problem\n",
    "\n",
    "$$\n",
    "\\text { minimize } f\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\text { subject to }\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\in S\n",
    "$$\n",
    "\n",
    "1. Choose an arbitrary starting point $\\boldsymbol{x}_{0}=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\in S$.\n",
    "\n",
    "2. We are going to define a difference equation that gives successive guesses for the optimal value:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{n+1}=\\boldsymbol{x}_{n}-\\alpha \\nabla f\\left(\\boldsymbol{x}_{n}\\right)\n",
    "$$\n",
    "\n",
    "The difference equation says to follow the negative gradient a certain distance from your present point (why are we doing this). Note that the value of $\\alpha$ is up to you so experiment with a few values (you should probably take $\\alpha \\leq 1 \\ldots$ why?).\n",
    "\n",
    "3. Repeat the iterative process in step b until two successive points are close enough to each other.\n",
    "\n",
    "**Take Note:** If you are looking to maximize your objective function then in the Monte-Carlo search you should examine if $z$ is greater than your current largest value. For gradient descent you should actually do a gradient ascent instead and follow the positive gradient instead of the negative gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.65. \n",
    "\n",
    "The functions like $f(x, y)=\\sin (x) \\cos (y)$ have many local extreme values which makes optimization challenging. Implement your Gradient Descent code on this function to find the local minimum $(-\\pi / 2,0)$. Start somewhere near $(-\\pi / 2,0)$ and show by way of example that your gradient descent code may not converge to this particular local minimum. Why is this important?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.5 Calculus with numpy and scipy\n",
    "\n",
    "In this section we will look at some highly versatile functions built into the `numpy` and `scipy` libraries in Python. These libraries allow us to lean on pre-built numerical routines for calculus and optimization and instead we can focus our energies on setting up the problems and interpreting solutions. The down side here is that we are going to treat some of the optimization routines in Python as black boxes, so part of the goal of this section is to partially unpack these black boxes so that we know what's going on under the hood. If you haven't done Exercise 2.65 yet you may want to do so now in order to get used to some of the syntax used by the Python `scipy` library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.5.1 Differentiation\n",
    "\n",
    "There are two main tools built into the `numpy` and `scipy` libraries that do numerical differentiation. In `numpy` there is the `np.diff()` command. In `scipy` there is the `scipy.misc.derivative()` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.66. \n",
    "\n",
    "In the following blocks of Python code we demonstrate what the `np.diff()` command does. Use these examples to give a thorough description for what np.diff() does to a Python list.\n",
    "\n",
    "1. First example of `np.diff()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "myList =  np.arange ( 0,10 )\n",
    "print(myList)\n",
    "print( np.diff(myList) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. Second example of `np.diff()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "myList = np.linspace(0,1,6)\n",
    "print(myList)\n",
    "print( np.diff(myList) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Third example of `np.diff()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(0,1,6)\n",
    "dx = x[1]-x[0]\n",
    "y = x**2\n",
    "dy = 2*x\n",
    "print(\"function values: \\n\",y)\n",
    "print(\"exact values of derivative: \\n\",dy)\n",
    "print(\"values from np.diff(): \\n\",np.diff(y))\n",
    "print(\"values from np.diff()/dx: \\n\",np.diff(y) / dx )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `np.diff()` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.67. \n",
    "\n",
    "Why does the `np.diff()` command produce a list that is one element shorter than the original list?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.68. \n",
    "\n",
    "If we have a list of $x$ values and a list of $y$ values for a function $y=f(x)$ then how do we use np. $\\operatorname{diff}()$ to approximate the first derivative of $f(x)$ ? What is the order of the error in the approximation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.69. \n",
    "\n",
    "What does the following block of Python code do?\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "x = np.linspace(0,1,6)\n",
    "dx = x[1]-x[0]\n",
    "y = x**2\n",
    "print( np.diff(y,2) / dx**2 )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.70. \n",
    "\n",
    "Use the `np.diff()` command to approximate the first and second derivatives of the function $f(x)=x \\sin (x)-\\ln (x)$ on the domain $[1,5]$. Then create a plot that shows $f(x)$ and the approximations of $f^{\\prime}(x)$ and $f^{\\prime \\prime}(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.71. \n",
    "\n",
    "Next we look into the `scipy.misc.derivative()` command from the `scipy` library. This will be another way to calculate the derivative of a function. One advantage will be that you can just send in a Python function (or a lambda function) without actually computing the lists of values. Examine the following Python code and fully describe what it does.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "f = lambda x: x**2\n",
    "x = np.linspace(1,5,5)\n",
    "df = scipy.misc.derivative(f,x,dx = 1e-10)\n",
    "print(df)\n",
    "import numpy as np\n",
    "x = np.linspace(0,1,6)\n",
    "dx = x[1]-x[0]\n",
    "y = x**2\n",
    "dy = 2*x\n",
    "print(\"function values: \\n\",y)\n",
    "```\n",
    "\n",
    "```\n",
    "print(\"exact values of derivative: \\n\",dy)\n",
    "print(\"values from np.diff(): \\n\",np.diff(y))\n",
    "print(\"values from np.diff()/dx: \\n\",np.diff(y) / dx )\n",
    "```\n",
    "\n",
    "One advantage to using scipy.misc.derivative() is that you get to dictate the error in the derivative computation, and that error is not tied to the list of values that you provide. In its simplest form you can provide just a single $x$ value just like in the next block of code.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "f = lambda x: x**2\n",
    "df = scipy.misc.derivative(f,1,dx = 1e-10) # derivative at x=1\n",
    "print(df)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.72. \n",
    "\n",
    "In the following code we find the first and second derivatives of $f(x)=x \\sin (x)-\\ln (x)$ using `scipy.misc.derivative()`. Notice that we've chosen to take $\\mathrm{dx}=1 \\mathrm{e}-6$ for each of the derivative computations. That may seem like an odd choice, but there is more going on here. Try successively smaller and smaller values for the dx parameter. What do you find? Why does it happen?\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "f = lambda x: np.sin(x)*x-np.log(x)\n",
    "x = np.linspace(1,5,100) # x domain: 100 points between 1 and 5\n",
    "df = scipy.misc.derivative(f,x,dx=1e-6)\n",
    "df2 = scipy.misc.derivative(f,x,dx=1e-6,n=2)\n",
    "plt.plot(x,f(x),'b',x,df,'r--',x,df2,'k--')\n",
    "plt.legend([\"f(x)\",\"f'(x)\",\"f''(x)\"])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.5.2 Integration\n",
    "\n",
    "In `numpy` there is a nice tool called `np.trapz()` that implements the trapezoidal rule. In the following problem you will find several examples of the `np.trapz()` command. Use these examples to determine how the command works to integrate functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.73. \n",
    "\n",
    "First we'll approximate the integral $\\int_{-2}^{2} x^{2} d x$. The exact answer\n",
    "\n",
    "**Figure 3.10: Derivatives with scipy**\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2025_02_27_429587f441ab5f434461g-42.jpg?height=595&width=898&top_left_y=429&top_left_x=706)\n",
    "\n",
    "is\n",
    "\n",
    "$$\n",
    "\\int_{-2}^{2} x^{2} d x=\\left.\\frac{x^{3}}{3}\\right|_{-2} ^{2}=\\frac{16}{3}=5.3333 \\ldots\n",
    "$$\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "x = np.linspace(-2, 2, 100)\n",
    "dx = x[1]-x[0]\n",
    "y = x**2\n",
    "print(\"Approximate integral is \",np.trapz(y)*dx)\n",
    "```\n",
    "\n",
    "Next we'll approximate $\\int_{0}^{2 \\pi} \\sin (x) d x$. We know that the exact value is 0 .\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "x = np.linspace(0,2*np.pi,100)\n",
    "dx = x[1]-x[0]\n",
    "y = np.sin(x)\n",
    "print(\"Approximate integral is \",np.trapz(y)*dx)\n",
    "```\n",
    "\n",
    "Pick a function and an interval for which you know the exact definite integral. Demonstrate how to use `np.trapz()` on your definite integral.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.74. \n",
    "\n",
    "Notice in the last examples that we multiplied the result of the `np.trapz()` command by $dx$. Why did we do this? What is the `np.trapz()` command doing without the $dx$ ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the `scipy` library there is a more general tool called `scipy.integrate.quad()`. The term \"quad\" is short for \"quadrature.\" In numerical analysis literature rules\n",
    "like Simpson's rule are called quadrature rules for integration. The function `scipy.integrate.quad()` accepts a Python function (or a lambda function) and the bounds of the definite integral. It outputs an approximation of the integral along with an approximation of the error in the integral calculation. See the Python code below.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import scipy.integrate\n",
    "f = lambda x: x**2\n",
    "I = scipy.integrate.quad(f,-2,2)\n",
    "print(I)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.75. \n",
    "\n",
    "What are the advantages and disadvantages to using the `scipy.integrate.quad()` command as compared to the `np.trapz()` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.76. \n",
    "\n",
    "If you have data for the hourly rate at which water is being drained from a dam and you want to find the total amount of water drained over the course of the time in the dataset, then which of the tools that we know would you use? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 3.5.3 Optimization\n",
    "\n",
    "As you've seen in this section there are many tools built into numpy and scipy that will do some of our basic numerical computations. The same is true for numerical optimization problems. Keep in mind throughout the remainder of this section that the whole topic of numerical optimization is still an active area of research and there is much more to the story that what we'll see here. However, the Python tools that we will use are highly optimized and tend to work quite well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.77. \n",
    "\n",
    "Let's solve a very simple function minimization problem to get started. Consider the function $f(x)=(x-3)^{2}-5$. A moment's thought reveals that the global minimum of this parabolic function occurs at $(3,-5)$. We can have `scipy.optimize.minimize()` find this value for us numerically. The routine is much like Newton's Method in that we give it a starting point near where we think the optimum will be and it will iterate through some algorithm (like a derivative free optimization routine) to approximate the minimum.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "f = lambda x: (x-3)**2 - 5\n",
    "minimize(f,2)\n",
    "```\n",
    "\n",
    "1. Implement the code above then spend some time playing around with the minimize command to minimize more challenging functions.\n",
    "\n",
    "2. Explain what all of the output information is from the `.minimize()` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.78. \n",
    "\n",
    "There is not a function called scipy.optimize.maximize(). Instead, Python expects you to rewrite every maximization problem as a minimization problem. How do you do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.79. \n",
    "\n",
    "Solve Exercise 3.48 using `scipy.optimize.minimize()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 3.6 Least Squares Curve Fitting\n",
    "\n",
    "In this section we'll change our focus a bit to look at a different question from algebra, and, in turn, reveal a hidden numerical optimization problem where the scipy.optimize.minimize() tool will come in quite handy.\n",
    "\n",
    "Here is the primary question of interest:\n",
    "If we have a few data points and a reasonable guess for the type of function fitting the points, how would we determine the actual function?\n",
    "\n",
    "You may recognize this as the basic question of regression from statistics. What we will do here is pose the statistical question of which curve best fits a data set as an optimization problem. Then we will use the tools that we've built so far to solve the optimization problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.80. \n",
    "\n",
    "Consider the function $f(x)$ that goes exactly through the points $(0,1),(1,4)$, and $(2,13)$.\n",
    "\n",
    "1. Find a function that goes through these points exactly. Be able to defend your work.\n",
    "\n",
    "2. Is your function unique? That is to say, is there another function out there that also goes exactly through these points?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.81. \n",
    "\n",
    "Now let's make a minor tweak to the previous problem. Let's say that we have the data points $(0,1.07),(1,3.9),(2,14.8)$, and $(3,26.8)$. Notice that these points are close to the points we had in the previous problem, but all of the $y$ values have a little noise in them and we have added a fourth point. If we suspect that a function $f(x)$ that best fits this data is quadratic then $f(x)=a x^{2}+b x+c$ for some constants $a, b$, and $c$.\n",
    "\n",
    "1. Plot the four points along with the function $f(x)$ for arbitrarily chosen values of $a, b$, and $c$.\n",
    "\n",
    "2. Work with your partner(s) to systematically change $a, b$, and $c$ so that you get a good visual match to the data. The Python code below will help you get started.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "xdata = np.array([0, 1, 2, 3])\n",
    "ydata = np.array([1.07, 3.9, 14.8, 26.8])\n",
    "a = # conjecture a value of a\n",
    "b = # conjecture a value of b\n",
    "c = # conjecture a value of c\n",
    "x = # build an x domain starting at 0 and going through 4\n",
    "guess = a*x**2 + b*x + c\n",
    "```\n",
    "\n",
    "```\n",
    "# make a plot of the data\n",
    "# make a plot of your function on top of the data\n",
    "```\n",
    "\n",
    "**Figure 3.11: Initial attempt at matching data with a quadratic.**\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2025_02_27_429587f441ab5f434461g-46.jpg?height=600&width=903&top_left_y=584&top_left_x=703)\n",
    "\n",
    "As an alternative to loading the data manually we could download the data from the book's github page. All datasets in the text can be loaded in this way. We will be using the pandas library (a Python data science library) to load the .csv files.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "URL1 = 'https://raw.githubusercontent.com/NumericalMethodsSullivan'\n",
    "URL2 = '/NumericalMethodsSullivan.github.io/master/data/'\n",
    "URL = URL1+URL2\n",
    "data = np.array( pd.read_csv(URL+'Exercise3_datafit1.cSv') )\n",
    "# Exercise3_datafit1.csv\n",
    "xdata = data[:,0]\n",
    "ydata = data[:,1]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.82. \n",
    "\n",
    "Now let's be a bit more systematic about things from the previous problem. Let's say that you have a pretty good guess that $b \\approx 2$ and $c \\approx 0.7$. We need to get a good estimate for $a$.\n",
    "\n",
    "1. Pick an arbitrary starting value for $a$ then for each of the four points find the error between the predicted $y$ value and the actual $y$ value. These errors are called the residuals.\n",
    "\n",
    "2. Square all four of your errors and add them up. (Pause, ponder, and discuss: why are we squaring the errors before we sum them?)\n",
    "\n",
    "3. Now change your value of $a$ to several different values and record the sum of the square errors for each of your values of $a$. It may be worth while to use a spreadsheet to keep track of your work here.\n",
    "\n",
    "4. Make a plot with the value of $a$ on the horizontal axis and the value of the sum of the square errors on the vertical axis. Use your plot to defend the optimal choice for $a$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.83. \n",
    "\n",
    "We're going to revisit part (c) of the previous problem. Write a loop that tries many values of $a$ in very small increments and calculates the sum of the squared errors. The following partial Python code should help you get started. In the resulting plot you should see a clear local minimum. What does that minimum tell you about solving this problem?\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "xdata = np.array([0, 1, 2, 3])\n",
    "ydata = np.array([1.07, 3.9, 14.8, 26.8])\n",
    "b = 2\n",
    "c = 0.75\n",
    "A = # give a numpy array of values for a\n",
    "SumSqRes = [] # this is storage for the sum of the sq. residuals\n",
    "for a in A:\n",
    "    guess = a*xdata**2 + b*xdata + c\n",
    "    residuals = # write code to calculate the residuals\n",
    "    SumSqRes.append( ??? ) # calculate the sum of the squ. residuals\n",
    "plt.plot(A,SumSqRes,'r*')\n",
    "plt.grid()\n",
    "plt.xlabel('Value of a')\n",
    "plt.ylabel('Sum of squared residuals')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Now let's formalize the process that we've described in the previous problems.\n",
    "Definition 3.10. (Least Squares Regression) Let\n",
    "\n",
    "$$\n",
    "S=\\left\\{\\left(x_{0}, y_{0}\\right),\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "be a set of $n+1$ ordered pairs in $\\mathbb{R}^{2}$. If we guess that a function $f(x)$ is a best choice to fit the data and if $f(x)$ depends on parameters $a_{0}, a_{n}, \\ldots, a_{n}$ then\n",
    "\n",
    "1. Pick initial values for the parameters $a_{0}, a_{1}, \\ldots, a_{n}$ so that the function $f(x)$ looks like it is close to the data (this is strictly a visual step ... take care that it may take some playing around to guess the initial values of the parameters)\n",
    "\n",
    "2. Calculate the square error between the data point and the prediction from the function $f(x)$\n",
    "\n",
    "$$\n",
    "\\text { error for the point } x_{i}: e_{i}=\\left(y_{i}-f\\left(x_{i}\\right)\\right)^{2}\n",
    "$$\n",
    "\n",
    "Note that squaring the error has the advantages of removing the sign, accentuating errors larger than 1, and decreasing errors that are less than 1.\n",
    "\n",
    "3. As a measure of the total error between the function and the data, sum the squared errors\n",
    "\n",
    "$$\n",
    "\\text { sum of square errors }=\\sum_{i=1}^{n}\\left(y_{i}-f\\left(x_{i}\\right)\\right)^{2}\n",
    "$$\n",
    "\n",
    "(Take note that if there were a continuum of points instead of a discrete set then we would integrate the square errors instead of taking a sum.)\n",
    "\n",
    "4. Change the parameters $a_{0}, a_{1}, \\ldots$ so as to minimize the sum of the square errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.84. \n",
    "\n",
    "In 3.10 the last step is a bit vague. That was purposeful since there are many techniques that could be used to minimize the sum of the square errors. However, if we just think about the sum of the squared residuals as a function then we can apply scipy.optimize.minimize() to that function in order to return the values of the parameters that best minimize the sum of the squared residuals. The following blocks of Python code implement the idea in a very streamlined way. Go through the code and comment each line to describe exactly what it does.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "xdata = np.array([0, 1, 2, 3])\n",
    "ydata = np.array([1.07, 3.9, 14.8, 26.8])\n",
    "def SSRes(parameters):\n",
    "    # In the next line of code we want to build our\n",
    "    # quadratic approximation y = ax^2 + bx + c\n",
    "    # We are sending in a list of parameters so\n",
    "    # a = parameters[0], b = parameters [1], and c = parameters[2]\n",
    "    yapprox = parameters[0]*xdata**2 + \\\n",
    "                        parameters[1]*xdata + \\\n",
    "                            parameters[2]\n",
    "    residuals = np.abs(ydata-yapprox)\n",
    "    return np.sum(residuals**2)\n",
    "```\n",
    "\n",
    "```\n",
    "BestParameters = minimize(SSRes, [2, 2,0.75])\n",
    "print(\"The best values of a, b, and c are: \\n\",BestParameters.x)\n",
    "# If you want to print the diagnositc then use the line below:\n",
    "# print(\"The minimization diagnostics are: \\n\",BestParameters)\n",
    "plt.plot(xdata,ydata,'bo',markersize=5)\n",
    "x = np.linspace(0,4,100)\n",
    "y = BestParameters.x[0]*x**2 + \\\n",
    "    BestParameters.x[1]*x + \\\n",
    "    BestParameters.x[2]\n",
    "plt.plot(x,y,'r--')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Best Fit Quadratic')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Figure 3.12: Best fit quadratic function.**\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2025_02_27_429587f441ab5f434461g-49.jpg?height=630&width=903&top_left_y=1165&top_left_x=516)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3.85. \n",
    "With a partner choose a function and then choose 10 points on that function. Add a small bit of error into the $y$-values of your points. Give your 10 points to another group. Upon receiving your new points:\n",
    "\n",
    "- Plot your points.\n",
    "- Make a guess about the basic form of the function that might best fit the data. Your general form will likely have several parameters (just like the quadratic had the parameters $a, b$, and $c$ ).\n",
    "- Modify the code from above to find the best collection of parameters minimize the sum of the squares of the residuals between your function and the data.\n",
    "- Plot the data along with your best fit function. If you are not satisfied with how it fit then make another guess on the type of function and repeat the process.\n",
    "- Finally, go back to the group who gave you your points and check your work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.86. \n",
    "For each dataset associated with this exercise give a functional form that might be a good model for the data. Be sure to choose the most general form of your guess. For example, if you choose \"quadratic\" then your functional guess is $f(x)=a x^{2}+b x+c$, if you choose \"exponential\" then your functional guess should be something like $f(x)=a e^{b(x-c)}+d$, or if you choose \"sinusoidal\" then your guess should be something like $f(x)=a \\sin (b x)+c \\cos (d x)+e$. Once you have a guess of the function type create a plot showing your data along with your guess for a reasonable set of parameters. Then write a function that leverages scipy.optimize.minimize() to find the best set of parameters so that your function best fits the data. Note that if scipy.optimize.minimize() does not converge then try the alternative scipy function scipy.optimize.fmin(). Also note that you likely need to be very close to the optimal parameters to get the optimizer to work properly.\n",
    "\n",
    "You can load the data with the following script.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "URL1 = 'https://raw.githubusercontent.com/NumericalMethodsSullivan'\n",
    "URL2 = '/NumericalMethodsSullivan.github.io/master/data/'\n",
    "URL = URL1+URL2\n",
    "datasetA = np.array( pd.read_csv(URL+'Exercise3_datafit2.csv') )\n",
    "datasetB = np.array( pd.read_csv(URL+'Exercise3_datafit3.csv') )\n",
    "datasetC = np.array( pd.read_csv(URL+'Exercise3_datafit4.csv') )\n",
    "# Exercise3_datafit1.csv,\n",
    "# Exercise3_datafit2.csv,\n",
    "# Exercise3_datafit3.csv\n",
    "```\n",
    "\n",
    "As a nudge in the right direction, in the left-hand pane of Figure 3.13 the function appears to be exponential. Hence we should choose a function of the form $f(x)=a e^{b(x-c)}+d$. Moreover, we need to pick good approximations of the parameters to start the optimization process. In the left-hand pane of Figure 3.13 the data appears to start near $x=1970$ so our initial guess for $c$ might be $c \\approx 1970$. To get initial guesses for $a, b$, and $d$ we can observe that the expected best fit curve will approximately go through the points $(1970,15000)$, $(1990,40000)$, and $(2000,75000)$. With this information we get the equations\n",
    "$a+d \\approx 15000, a e^{20 b}+d \\approx 40000$ and $a e^{30 b}+d \\approx 75000$ and work to get reasonable approximations for $a, b$, and $d$ to feed into the scipy.optimize.minimize() command.\n",
    "\n",
    "**Figure 3.13: Raw data for least squares function matching problems.**\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2025_02_27_429587f441ab5f434461g-51.jpg?height=630&width=1161&top_left_y=577&top_left_x=387)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
