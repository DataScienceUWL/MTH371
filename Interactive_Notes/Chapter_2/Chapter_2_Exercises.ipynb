{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Exercises\n",
    "\n",
    "## 2.6.1 Algorithm Summaries\n",
    "\n",
    "The following four problems are meant to have you re-build each of the algorithms that we developed in this chapter. Write all of the mathematical details completely and clearly. Don't just write \"how\" the method works, but give all of the mathematical details for \"why\" it works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.51. \n",
    "\n",
    "Let $f(x)$ be a continuous function on the interval $[a, b]$ where $f(a) \\cdot f(b)<0$. Clearly give all of the mathematical details for how the Bisection Method approximates the root of the function $f(x)$ in the interval $[a, b]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.52. \n",
    "\n",
    "Let $f(x)$ be a continuous function on the interval $[a, b]$ where $f(a) \\cdot f(b)<0$. Clearly give all of the mathematical details for how the Regula Falsi Method approximates the root of the function $f(x)$ in the interval $[a, b]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.53. \n",
    "\n",
    "Let $f(x)$ be a differentiable function with a root near $x=$ $x_{0}$. Clearly give all of the mathematical details for how Newton's Method approximates the root of the function $f(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.54. \n",
    "\n",
    "Let $f(x)$ be a continuous function with a root near $x=$ $x_{0}$. Clearly give all of the mathematical details for how the Secant Method approximates the root of the function $f(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2.6.2 Applying What You've Learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.55. \n",
    "\n",
    "How many iterations of the bisection method are necessary to approximate $\\sqrt{3}$ to within $10^{-3}, 10^{-4}, \\ldots, 10^{-15}$ using the initial interval $[a, b]=[0,2]$ ? See Theorem 2.2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.56. \n",
    "\n",
    "Refer back to Example 2.1 and demonstrate that you get the same results by solving the problem $x^{3}-3=0$. Generate versions of all of the plots from the Example and give thorough descriptions of what you learn from each plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.57. \n",
    "\n",
    "In this problem you will demonstrate that all of your root finding codes work. At the beginning of this chapter we proposed the equation solving problem\n",
    "\n",
    "$$\n",
    "3 \\sin (x)+9=x^{2}-\\cos (x)\n",
    "$$\n",
    "\n",
    "Write a script that calls upon your Bisection, Regula Falsi, Newton, and Secant methods one at a time to find the positive solution to this equation. Your script needs to output the solutions in a clear and readable way so you can tell which answer can from which root finding algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.58. \n",
    "\n",
    "A root-finding method has a convergence rate of order $M$ if there is a constant $C$ such that\n",
    "\n",
    "$$\n",
    "\\left|x_{k+1}-x_{*}\\right| \\leq C\\left|x_{k}-x_{*}\\right|^{M}\n",
    "$$\n",
    "\n",
    "Here, $x_{*}$ is the exact root, $x_{k}$ is the $k^{t h}$ iteration of the root finding technique, and $x_{k+1}$ is the $(k+1)^{s t}$ iteration of the root finding technique.\n",
    "\n",
    "1. If we consider the equation\n",
    "\n",
    "$$\n",
    "\\left|x_{k+1}-x_{*}\\right| \\leq C\\left|x_{k}-x_{*}\\right|^{M}\n",
    "$$\n",
    "\n",
    "and take the logarithm (base 10) of both sides then we get\n",
    "\n",
    "$$\n",
    "\\log \\left(\\left|x_{k+1}-x_{*}\\right|\\right) \\leq \\underline{\\hspace{1in}} + \\leq \\underline{\\hspace{1in}}\n",
    "$$\n",
    "\n",
    "2. In part 1. you should have found that the $\\log$ of new error is a linear function of the log of the old error. What is the slope of this linear function on a log-log plot?\n",
    "\n",
    "3. In the plots below you will see six different log-log plots of the new error to the old error for different root finding techniques. What is the order of the approximate convergence rate for each of these methods?\n",
    "\n",
    "4. In your own words, what does it mean for a root finding method to have a \"first order \n",
    "convergence rate?\" \"Second order convergence rate?\" etc.\n",
    "\n",
    "<img src=\"https://datascienceuwl.github.io/MTH371/figures/Chapter2/Figure_2_12.png\" alt=\"Figure 2.12: Six Error Plots\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.59. \n",
    "\n",
    "Shelby started using Newton's method to solve a root-finding problem. To test her code she was using an equation for which she new the solution. Given the starting point the absolute error after one step of Newton's method was $\\left|x_{1}-x_{*}\\right|=0.2$. What is the approximate expected error at step 2 ? What about at step 3? Step 4? Defend your answers by fully describing your thought process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.60. \n",
    "\n",
    "There are MANY other root finding techniques beyond the four that we have studied thus far. We can build these methods using Taylor Series as follows:\n",
    "\n",
    "Near $x=x_{0}$ the function $f(x)$ is approximated by the Taylor Series\n",
    "\n",
    "$$\n",
    "f(x) \\approx y=f\\left(x_{0}\\right)+\\sum_{n=1}^{N} \\frac{f^{(n)}\\left(x_{0}\\right)}{n!}\\left(x-x_{0}\\right)^{n}\n",
    "$$\n",
    "\n",
    "where $N$ is a positive integer. In a root-finding algorithm we set $y$ to zero to find the root of the approximation function. The root of this function should be close to the actual root that we're looking for. Therefore, to find the next iterate we solve the equation\n",
    "\n",
    "$$\n",
    "0=f\\left(x_{0}\\right)+\\sum_{n=1}^{N} \\frac{f^{(n)}\\left(x_{0}\\right)}{n!}\\left(x-x_{0}\\right)^{n}\n",
    "$$\n",
    "\n",
    "for $x$. For example, if $N=1$ then we need to solve $0=f\\left(x_{0}\\right)+f^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)$ for $x$. In doing so we get $x=x_{0}-f\\left(x_{0}\\right) / f^{\\prime}\\left(x_{0}\\right)$. This is exactly Newton's method. If $N=2$ then we need to solve\n",
    "\n",
    "$$\n",
    "0=f\\left(x_{0}\\right)+f^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{f^{\\prime \\prime}\\left(x_{0}\\right)}{2!}\\left(x-x_{0}\\right)^{2}\n",
    "$$\n",
    "\n",
    "for $x$.\n",
    "\n",
    "1. Solve for $x$ in the case that $N=2$. Then write a Python function that implements this root-finding method.\n",
    "\n",
    "2. Demonstrate that your code from part 1. is indeed working by solving several problems where you know the exact solution.\n",
    "\n",
    "3. Show several plots that estimates the order of the method from part 1. That is, create a log-log plot of the successive errors for several different equation-solving problems.\n",
    "\n",
    "4. What are the pro's and con's to using this new method?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.61. \n",
    "\n",
    "An object falling vertically through the air is subject to friction due to air resistance as well as gravity. The function describing the position of such a function is\n",
    "\n",
    "$$\n",
    "s(t)=s_{0}-\\frac{m g}{k} t+\\frac{m^{2} g}{k^{2}}\\left(1-e^{-k t / m}\\right)\n",
    "$$\n",
    "\n",
    "where $m$ is the mass measured in $\\mathrm{kg}, g$ is gravity measured in meters per second per second, $s_{0}$ is the initial position measured in meters, and $k$ is the coefficient of air resistance.\n",
    "\n",
    "1. What are the units of the parameter $k$ ?\n",
    "\n",
    "2. If $m=1 \\mathrm{~kg}, g=9.8 \\mathrm{~m} / \\mathrm{s}^{2}, k=0.1$, and $s_{0}=100 \\mathrm{~m}$ how long will it take for the object to hit the ground? Find your answer to within 0.01 seconds.\n",
    "\n",
    "3. The value of $k$ depends on the aerodynamics of the object and might be challenging to measure. We want to perform a sensitivity analysis on your answer to part (b) subject to small measurement errors in $k$. If the value of $k$ is only known to within $10 \\%$ then what are your estimates of when the object will hit the ground?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.62. \n",
    "\n",
    "Can the Bisection Method, Regula Falsi Method, or Newton's Method be used to find the roots of the function $f(x)=\\cos (x)+1$ ? Explain why or why not for each technique?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.63. \n",
    "\n",
    "In Single Variable Calculus you studied methods for finding local and global extrema of functions. You likely recall that part of the process is to set the first derivative to zero and to solve for the independent variable (remind yourself why you're doing this). The trouble with this process is that it may be very very challenging to solve by hand. This is a perfect place for Newton's method or any other root finding techinque!\n",
    "Find the local extrema for the function $f(x)=x^{3}(x-3)(x-6)^{4}$ using numerical techniques where appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.64. \n",
    "\n",
    "A fixed point of a function $f(x)$ is a point that solves the equation $f(x)=x$. Fixed points are interesting in iterative processes since fixed points don't change under repeated application of the function $f$.\n",
    "\n",
    "For example, consider the function $f(x)=x^{2}-6$. The fixed points of $f(x)$ can be found by solving the equation $x^{2}-6=x$ which, when simplified algebraically, is $x^{2}-x-6=0$. Factoring the left-hand side gives $(x-3)(x+2)=0$ which implies that $x=3$ and $x=-2$ are fixed points for this function. That is, $f(3)=3$ and $f(-2)=-2$. Notice, however, that finding fixed points is identical to a root finding problem.\n",
    "\n",
    "1. Use a numerical root-finding algorithm to find the fixed points of the function $f(x)=x^{2}-6$ on the interval $[0, \\infty)$.\n",
    "\n",
    "2. Find the fixed points of the function $f(x)=\\sqrt{\\frac{8}{x+6}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.65. (`scipy.optimize.fsolve()`) \n",
    "\n",
    "The scipy library in Python has many built-in numerical analysis routines much like the ones that we have built in this chapter. Of particular interest to the task of root finding is the fsolve command in the scipy.optimize library.\n",
    "\n",
    "1. Go to the [help documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html) for `scipy.optimize.fsolve` and make yourself familiar with how to use the tool.\n",
    "\n",
    "2. First solve the equation $x \\sin (x)-\\ln (x)=0$ for $x$ starting at $x_{0}=3$.\n",
    "\n",
    "    * Make a plot of the function on the domain $[0,5]$ so you can eyeball the root before using the tool.\n",
    "    * Use the `scipy.optimize.fsolve()` command to approximate the root.\n",
    "    * Fully explain each of the outputs from the `scipy.optimize.fsolve()` command. You should use the `fsolve()` command with `full_output=1` so you can see all of the solver diagnostics.\n",
    "\n",
    "3. Demonstrate how to use `fsolve()` using any non-trivial nonlinear equation solving problem. Demonstrate what some of the options of `fsolve()` do.\n",
    "\n",
    "4. The `scipy.optimize.fsolve()` command can also solve systems of equations (something we have not built algorithms for in this chapter). Consider the system of equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& x_{0} \\cos \\left(x_{1}\\right)=4 \\\\\n",
    "& x_{0} x_{1}-x_{1}=5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The following Python code allows you to use `scipy.optimize.fsolve()` so solve this system of nonlinear equations in much the same way as we did in part (b) of this problem. However, be aware that we need to think of x as a vector of $x$-values. Go through the code below and be sure that you understand every line of code.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "def F(x):\n",
    "    Output = [ x[0]*np.cos(x[1])-4 ]\n",
    "    Output.append( x[0]*x[1] - x[1] - 5 )\n",
    "    return Output\n",
    "# Or alternately we could define the system as a lambda function\n",
    "# with F = lambda x: [ x[0]*np.cos(x[1])-4 , x[0]*x[1]-x[1]-5 ]\n",
    "fsolve(F,[6,1],full_output=1)\n",
    "# Note: full_output gives the solver diagnostics\n",
    "```\n",
    "\n",
    "5. Solve the system of nonlinear equations below using `fsolve()`.\n",
    "\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "x^{2}-x y^{2}=2 \\\\\n",
    "x y=2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
