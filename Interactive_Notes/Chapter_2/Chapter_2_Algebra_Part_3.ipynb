{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Analysis\n",
    "\n",
    "There are several ways in which Newton's Method will behave unexpectedly - or downright fail. Some of these issues can be foreseen by examining the Newton iteration formula\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}\n",
    "$$\n",
    "\n",
    "Some of the failures that we'll see are a little more surprising. Also in this section we will look at the convergence rate of Newton's Method and we will show that we can greatly outperform the Bisection and Regula-Falsi methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.40. \n",
    "\n",
    "There are several reasons why Newton's method could fail. Work with your partners to come up with a list of reasons. Support each of your reasons with a sketch or an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.41. \n",
    "\n",
    "One of the failures of Newton's Method is that it requires a division by $f^{\\prime}\\left(x_{n}\\right)$. If $f^{\\prime}\\left(x_{n}\\right)$ is zero then the algorithm completely fails. Go back to your Python function and put an if statement in the function that catches instances when Newton's Method fails in this way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.42. \n",
    "\n",
    "An interesting failure can occur with Newton's Method that you might not initially expect. Consider the function $f(x)=x^{3}-2 x+2$. This function has a root near $x=-1.77$. Fill in the table below and draw the tangent lines on the figure for approximating the solution to $f(x)=0$ with a starting point of $x=0$.\n",
    "\n",
    "| $n$ | $x_{n}$ | $f\\left(x_{n}\\right)$ |\n",
    "| :--- | :--- | :--- |\n",
    "| 0 | $x_{0}=0$ | $f\\left(x_{0}\\right)=2$ |\n",
    "| 1 | $x_{1}=0-\\frac{f\\left(x_{0}\\right)}{f^{\\prime}\\left(x_{0}\\right)}=1$ | $f\\left(x_{1}\\right)=1$ |\n",
    "| 2 | $x_{2}=1-\\frac{f\\left(x_{1}\\right)}{f^{\\prime}\\left(x_{1}\\right)}=$ | $f\\left(x_{2}\\right)=$ |\n",
    "| 3 | $x_{3}=$ | $f\\left(x_{3}\\right)=$ |\n",
    "| 4 | $x_{4}=$ | $f\\left(x_{4}\\right)=$ |\n",
    "| $\\vdots$ | $\\vdots$ | $\\vdots$ |\n",
    "\n",
    "**Figure 2.9: An interesting Newton's Method failure.**\n",
    "\n",
    "<img src=\"https://datascienceuwl.github.io/MTH371/figures/Chapter2/Figure_2_9.png\" alt=\"Figure 2.9\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.43. \n",
    "\n",
    "Now let's consider the function $f(x)=\\sqrt[3]{x}$. This function has a root $x=0$. Furthermore, it is differentiable everywhere except at $x=0$ since\n",
    "\n",
    "$$\n",
    "f^{\\prime}(x)=\\frac{1}{3} x^{-2 / 3}=\\frac{1}{3 x^{2 / 3}} .\n",
    "$$\n",
    "\n",
    "The point of this problem is to show what can happen when the point of non-differentiability is precisely the point that you're looking for.\n",
    "a. Fill in the table of iterations starting at $x=-1$, draw the tangent lines on the plot, and make a general observation of what is happening with the Newton iterations.\n",
    "\n",
    "| $n$ | $x_{n}$ | $f\\left(x_{n}\\right)$ |\n",
    "| :--- | :--- | :--- |\n",
    "| 0 | $x_{0}=-1$ | $f\\left(x_{0}\\right)=-1$ |\n",
    "| 1 | $x_{1}=-1-\\frac{f(-1)}{f^{\\prime}(-1)}=$ | $f\\left(x_{1}\\right)=$ |\n",
    "| 2 |  |  |\n",
    "| 3 |  |  |\n",
    "| 4 |  | $\\vdots$ |\n",
    "| $\\vdots$ | $\\vdots$ |  |\n",
    "\n",
    "b. Now let's look at the Newton iteration in a bit more detail. Since $f(x)=$ $x^{1 / 3}$ and $f^{\\prime}(x)=\\frac{1}{3} x^{-2 / 3}$ the Newton iteration can be simplified as\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_{n}-\\frac{x^{1 / 3}}{\\left(\\frac{1}{3} x^{-2 / 3}\\right)}=x_{n}-3 \\frac{x^{1 / 3}}{x^{-2 / 3}}=x_{n}-3 x_{n}=-2 x_{n} .\n",
    "$$\n",
    "\n",
    "What does this tell us about the Newton iterations?\n",
    "Hint: You should have found the exact same thing in the numerical experiment in part (a).\n",
    "c. Was there anything special about the starting point $x_{0}=-1$ ? Will this problem exist for every starting point?\n",
    "\n",
    "Figure 2.10: Another surprising Newton's Method failure.\n",
    "\n",
    "<img src=\"https://datascienceuwl.github.io/MTH371/figures/Chapter2/Figure_2_10.png\" alt=\"Figure 2.10\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.44. \n",
    "\n",
    "Repeat the previous exercise with the function $f(x)=x^{3}-5 x$ with the starting point $x_{0}=-1$.\n",
    "\n",
    "**Figure 2.11: Another surprising Newton's Method failure.**\n",
    "\n",
    "<img src=\"https://datascienceuwl.github.io/MTH371/figures/Chapter2/Figure_2_11.png\" alt=\"Figure 2.11\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.45. \n",
    "\n",
    "Newton's Method is known to have a quadratic convergence rate. This means that there is some constant $C$ such that\n",
    "\n",
    "$$\n",
    "\\left|x_{k+1}-x_{*}\\right| \\leq C\\left|x_{k}-x_{*}\\right|^{2}\n",
    "$$\n",
    "\n",
    "where $x_{*}$ is the root that we're hunting for.\n",
    "The quadratic convergence implies that if we plot the error in the new iterate on the $y$-axis and the error in the old iterate on the $x$ axis of a log-log plot then we will see a constant slope of 2 . To see this we can take the $\\log$ (base 10) of both sides of the previous equation to get\n",
    "\n",
    "$$\n",
    "\\log \\left(\\left|x_{k+1}-x_{*}\\right|\\right)=\\log (C)+2 \\log \\left(\\left|x_{k}-x_{*}\\right|\\right)\n",
    "$$\n",
    "\n",
    "and we see that this is a linear function (on a log-log plot) and the slope is 2. We created plots like this back in Example 2.1.\n",
    "\n",
    "We are going to create an error plot just like what we just described.\n",
    "\n",
    "1. Pick an equation where you know the solution.\n",
    "2. Create the error plot with $\\left|x_{k}-x_{*}\\right|$ on the horizontal axis and $\\left|x_{k+1}-x_{*}\\right|$ on the vertical axis\n",
    "3. Demonstrate that this plot has a slope of 2 .\n",
    "4. Give a thorough explanation for how to interpret the plot that you just made.\n",
    "5. When solving an equation with Newton's method Joe found that the absolute error at iteration 1 of the process was 0.15 . Based on the fact that Newton's method is a second order method this means that the absolute error at step 2 will be less than or equal to some constant times $0.15^{2}=0.0225$. Similarly, the error at step 3 will be less than or equal to some scalar multiple of $0.0025^{2}=0.00050625$. What would Joe's expected error be bounded by for the fourth iteration, fifth iteration, etc?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 2.5 The Secant Method\n",
    "\n",
    "### 2.5.1 Intuition and Implementation\n",
    "\n",
    "Newton's Method has second-order (quadratic) convergence and, as such, will perform faster than the Bisection and Regula-Falsi methods. However, Newton's Method requires that you have a function and a derivative of that function. The conundrum here is that sometimes the derivative is cumbersome or impossible to obtain but you still want to have the great quadratic convergence exhibited by Newton's method.\n",
    "Recall that Newton's method is\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}\n",
    "$$\n",
    "\n",
    "If we replace $f^{\\prime}\\left(x_{n}\\right)$ with an approximation of the derivative then we may have a method that is close to Newton's method in terms of convergence rate but is less troublesome to compute. Any method that replaces the derivative in Newton's method with an approximation is called a Quasi-Newton Method. The first, and most obvious, way to approximate the derivative is just to use the slope of a secant line instead of the slope a tangent line in the Newton iteration. If we choose two starting points that are quite close to each other then the slope of the secant line through those points will be approximately the same as the slope of the tangent line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.46. (The Secant Method) \n",
    "\n",
    "Assume that $f(x)$ is continuous and we wish to solve $f(x)=0$ for $x$.\n",
    "\n",
    "1. Determine if there is a root near an arbitrary starting point $x_{0}$. How might you do that?\n",
    "2. Pick a second starting point near $x_{0}$. Call this second starting point $x_{1}$. Note well that the points $x_{0}$ and $x_{1}$ should be close to each other. Why? (The choice here is different than for the Bisection and Regula Falsi methods. We are not choosing the left- and right- sides of an interval surrounding the root.)\n",
    "3. Use the backward difference\n",
    "\n",
    "$$\n",
    "f^{\\prime}\\left(x_{n}\\right) \\approx \\frac{f\\left(x_{n}\\right)-f\\left(x_{n-1}\\right)}{x_{n}-x_{n-1}}\n",
    "$$\n",
    "\n",
    "to approximate the derivative of $f$ at $x_{n}$. Discuss why this approximates the derivative.\n",
    "4. Perform the Newton-type iteration\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{\\left(\\frac{f\\left(x_{n}\\right)-f\\left(x_{n-1}\\right)}{x_{n}-x_{n-1}}\\right)}\n",
    "$$\n",
    "\n",
    "until $f\\left(x_{n}\\right)$ is close enough to zero. Notice that the new iteration simplifies to\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)\\left(x_{n}-x_{n-1}\\right)}{f\\left(x_{n}\\right)-f\\left(x_{n-1}\\right)} .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.47. \n",
    "\n",
    "Draw several pictures showing what the Secant method does pictorially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.48. \n",
    "\n",
    "Write pseudo-code to outline how you will implement the Secant Method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.49. \n",
    "\n",
    "Write Python code for solving equations of the form $f(x)=0$ with the Secant method. Your function should accept a Python function, two starting points, and an optional error tolerance. Also write a test script that clearly shows that your code is working.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Analysis\n",
    "\n",
    "Up to this point we have done analysis work on the Bisection Method, the Regula-Falsi Method, and Newton's Method. We have found that the methods are first order, first order, and second order respectively. We end this chapter by doing the same for the Secant Method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.50. \n",
    "\n",
    "Choose a non-trivial equation for which you know the solution and write a script to empirically determine the convergence rate of the Secant method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
